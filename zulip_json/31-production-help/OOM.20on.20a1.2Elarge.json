[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span>  hi reaching out about this <a href=\"https://github.com/zulip/zulip/issues/21195#issuecomment-1048071212\">https://github.com/zulip/zulip/issues/21195#issuecomment-1048071212</a></p>",
        "id": 1332654,
        "sender_full_name": "Stpn",
        "timestamp": 1645554048
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"23254\">@Stpn</span> hi!  Moved this to a more appropriate topic.</p>",
        "id": 1332655,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1645554104
    },
    {
        "content": "<p>(And then it looks like you moved this to an old one).</p>",
        "id": 1332656,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1645554122
    },
    {
        "content": "<p>Have you tried restarting <code>rabbitmq</code> itself?</p>",
        "id": 1332661,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1645554160
    },
    {
        "content": "<p>yes, but it did hang up in ~24 hours</p>",
        "id": 1332663,
        "sender_full_name": "Stpn",
        "timestamp": 1645554202
    },
    {
        "content": "<p>please feel free to move this to any other topic</p>",
        "id": 1332667,
        "sender_full_name": "Stpn",
        "timestamp": 1645554268
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span>  is there any test I could run? b/c the service status returns nothing suspicious (to me at least)</p>",
        "id": 1332672,
        "sender_full_name": "Stpn",
        "timestamp": 1645554372
    },
    {
        "content": "<p>When it's hung, can you show the output of:</p>\n<div class=\"codehilite\"><pre><span></span><code>service rabbitmq-server status\nss -ltpn | grep 5672\n</code></pre></div>",
        "id": 1332674,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645554378
    },
    {
        "content": "<p>when it's hung the server is unresponsive, can't even SSH into it, have to restart (or force-restart it even)</p>",
        "id": 1332676,
        "sender_full_name": "Stpn",
        "timestamp": 1645554415
    },
    {
        "content": "<p>OK.  I guess what makes you point the finger at rabbitmq, then?</p>",
        "id": 1332678,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645554464
    },
    {
        "content": "<p>it's the only error in the error.log ?</p>",
        "id": 1332681,
        "sender_full_name": "Stpn",
        "timestamp": 1645554476
    },
    {
        "content": "<p>It could just be a symptom of whatever is causing the CPU to lock up.</p>\n<p>What's the instance type of the EC2 host?</p>",
        "id": 1332683,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645554553
    },
    {
        "content": "<p>a1.large</p>",
        "id": 1332686,
        "sender_full_name": "Stpn",
        "timestamp": 1645554625
    },
    {
        "content": "<p>any chance you could advise how to debug the CPU lock?</p>",
        "id": 1332692,
        "sender_full_name": "Stpn",
        "timestamp": 1645554802
    },
    {
        "content": "<p>OK; we don't have a lot of reports (positive or negative) about running on ARM64 hosts in production -- support was only recently added for them.  I might suggest trying an AMD64 instance type and seeing if that resolves it.</p>",
        "id": 1332694,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645554821
    },
    {
        "content": "<p>I see</p>",
        "id": 1332699,
        "sender_full_name": "Stpn",
        "timestamp": 1645554843
    },
    {
        "content": "<p>any advice on how to migrate Zulip seamlessly to another instance? or it'd be a pure AWS problem</p>",
        "id": 1332701,
        "sender_full_name": "Stpn",
        "timestamp": 1645554876
    },
    {
        "content": "<p>We're very interested in debugging the failure if it <em>is</em> ARM64-based, but that's just my first guess at the issue, not a certain diagnosis.</p>",
        "id": 1332702,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645554880
    },
    {
        "content": "<p>I am down to provide any logs, just idk which <br>\nwe had maybe 5 of these failures in the last 1 week</p>",
        "id": 1332705,
        "sender_full_name": "Stpn",
        "timestamp": 1645554913
    },
    {
        "content": "<p>You'll need to install Zulip from scratch on the new instance, but you can use our export and import tooling to make the data transfer pretty painless:<br>\n<a href=\"https://zulip.readthedocs.io/en/latest/production/export-and-import.html\">https://zulip.readthedocs.io/en/latest/production/export-and-import.html</a></p>",
        "id": 1332708,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645554930
    },
    {
        "content": "<p>it was happening on 4.9 and on 5.0 too</p>",
        "id": 1332709,
        "sender_full_name": "Stpn",
        "timestamp": 1645554935
    },
    {
        "content": "<p>I'd check kernel logs to see what those have to say</p>",
        "id": 1332711,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645554941
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code> oom_reaper: reaped process 2781 (python3), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB\n</code></pre></div>\n<p>I guess that answers it? ugh</p>",
        "id": 1332720,
        "sender_full_name": "Stpn",
        "timestamp": 1645555066
    },
    {
        "content": "<p>4G should be enough for a Zulip instance.</p>\n<p>It's also surprising to me that an OOM makes the whole host unreachable.</p>",
        "id": 1332724,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645555121
    },
    {
        "content": "<p>So likely the rabbitmq failures are because rabbitmq got taken down by the oom-killer, not any thing actually wrong with rabbitmq</p>",
        "id": 1332727,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645555166
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>Feb 22 05:46:43 ip-172-31-11-73 kernel: [144000.577084] oom-kill:constraint=CONSTRAINT_NONE,nodemask=(null),cpuset=/,mems_allowed=0,global_oom,task_memcg=/system.slice/supervisor.service,task=python3,pid=2781,uid=1001\nFeb 22 05:46:43 ip-172-31-11-73 kernel: [144000.577109] Out of memory: Killed process 2781 (python3) total-vm:248340kB, anon-rss:153864kB, file-rss:1244kB, shmem-rss:0kB, UID:1001 pgtables:456kB oom_score_adj:0\nFeb 22 05:46:43 ip-172-31-11-73 kernel: [144000.599199] oom_reaper: reaped process 2781 (python3), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB\n</code></pre></div>\n<p>this is full 3 lines before the hang-up</p>",
        "id": 1332728,
        "sender_full_name": "Stpn",
        "timestamp": 1645555201
    },
    {
        "content": "<p>The total-vm on that is only 1/4 of a GB.</p>",
        "id": 1332732,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645555286
    },
    {
        "content": "<p>You can try switching ton a1.xlarge, on the hopes that that will give you enough memory overhead that when the memory bloats you can go looking around for the cause, rather than having an unresponsive host.</p>",
        "id": 1332733,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645555358
    },
    {
        "content": "<p>Renamed the topic now that we have a better sense of the causes.</p>",
        "id": 1332737,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645555436
    },
    {
        "content": "<p>Upgrading from a1.large to a1.xlarge should be a straightforward change, since it's the same arch -- see <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html</a></p>",
        "id": 1332741,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645555509
    },
    {
        "content": "<p>ok migrated<br>\nlet's see I guess :)</p>",
        "id": 1332747,
        "sender_full_name": "Stpn",
        "timestamp": 1645555859
    },
    {
        "content": "<p>Sounds good.  If you do see, via memory-size monitoring of the processes, a large step function increase in zulip's memory footprint, or a slow leak, we'd like to help diagnose that.</p>",
        "id": 1332749,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645555949
    },
    {
        "content": "<p>thank you, will let you know</p>",
        "id": 1332754,
        "sender_full_name": "Stpn",
        "timestamp": 1645556164
    },
    {
        "content": "<p>if you want I can log the memory footprint over the next few days, but I'd ask for a command to do that then</p>",
        "id": 1332756,
        "sender_full_name": "Stpn",
        "timestamp": 1645556211
    },
    {
        "content": "<p>You can use something like:</p>\n<div class=\"codehilite\"><pre><span></span><code>top -cbd 60 -o +%MEM | grep &quot;[l]oad average&quot; -A 16 &gt; memory-usage.log\n</code></pre></div>\n<p>...which will log the top-10 memory processes every 60s.</p>",
        "id": 1332782,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1645557223
    }
]