[
    {
        "content": "<p>We have a really interesting problem right now. <br>\n4 clients (desktop and webapp) can't connect properly to our zulip server. The read message at the top props up and disappears again on a constant basis. The notifications don't work at all. When clicking on private messages they can click on another use and then see the last messages sent to them. But it won't update the narrow if messages are sent after click on a person. You have to click the person again and then it will update. Neither does it mark messages as read properly. <br>\nall the other clients are working fine. A zulip server reboot didn't help it. <br>\nHow can a partial connection problem appear on only a few computers and not for everyone and then so out of the blue?</p>",
        "id": 487865,
        "sender_full_name": "Matthias",
        "timestamp": 1518081894
    },
    {
        "content": "<p>ok, more insight now (by now 5 people)<br>\nI had one affected person login on a different PC. Same problem. Then I loggged in with my account and I had no problem at all. So it's not antivirus, network connection, workstation etc. related but somehow Zulip itself that can't connect to some user account. <br>\nI hope this helps</p>",
        "id": 487937,
        "sender_full_name": "Matthias",
        "timestamp": 1518085179
    },
    {
        "content": "<p>If I don't hear back from anyone I will run an update from git master in 40-50 mintues to see if this has already been fixed.</p>",
        "id": 487943,
        "sender_full_name": "Matthias",
        "timestamp": 1518085286
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-email=\"matthiasmueller@protonmail.com\" data-user-id=\"1646\">@Matthias</span> Can you wait until Tim, Greg, Rishi or Steve shows up. I don't think it's a known issue so It would be helpful for them to have access to the server in the current state to debug this.</p>",
        "id": 487951,
        "sender_full_name": "Vishnu KS",
        "timestamp": 1518085547
    },
    {
        "content": "<p>My bet is that this can probably fixed without an upgrade.</p>",
        "id": 487953,
        "sender_full_name": "Vishnu KS",
        "timestamp": 1518085615
    },
    {
        "content": "<p>the only problem is that the server is only accessible through the LAN...</p>",
        "id": 487956,
        "sender_full_name": "Matthias",
        "timestamp": 1518086183
    },
    {
        "content": "<p>the only thing I can see is maybe teamviewer</p>",
        "id": 487957,
        "sender_full_name": "Matthias",
        "timestamp": 1518086221
    },
    {
        "content": "<p>Let me correct my statement. I didn't mean direct access. They can probably ask you to run some commands to debug the issue.</p>",
        "id": 487958,
        "sender_full_name": "Vishnu KS",
        "timestamp": 1518086280
    },
    {
        "content": "<p>I see</p>",
        "id": 487960,
        "sender_full_name": "Matthias",
        "timestamp": 1518086404
    },
    {
        "content": "<p>ok then, I'll wait with the update but 5 people can't really use Zulip now</p>",
        "id": 487961,
        "sender_full_name": "Matthias",
        "timestamp": 1518086483
    },
    {
        "content": "<p>update: 6 people now disconnected...</p>",
        "id": 487964,
        "sender_full_name": "Matthias",
        "timestamp": 1518086958
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-email=\"greg@zulipchat.com\" data-user-id=\"2187\">@Greg Price</span> <span class=\"user-mention\" data-user-email=\"showell@zulipchat.com\" data-user-id=\"58\">@Steve Howell</span> <span class=\"user-mention\" data-user-email=\"tabbott@zulipchat.com\" data-user-id=\"7\">@Tim Abbott</span> <span class=\"user-mention\" data-user-email=\"rishig@zulipchat.com\" data-user-id=\"131\">@Rishi Gupta</span>  FYI</p>",
        "id": 487972,
        "sender_full_name": "Vishnu KS",
        "timestamp": 1518087210
    },
    {
        "content": "<p>Currently 3AM in our timezone. Will have to wait a couple hours (~4)  at least.</p>",
        "id": 487982,
        "sender_full_name": "Joshua Pan",
        "timestamp": 1518087576
    },
    {
        "content": "<p>Can you check the web browser JS console for errors? I saw something like this on testing in 1.7.1. There were JS errors in the client about an invalid regex.</p>",
        "id": 488089,
        "sender_full_name": "Scott Russell",
        "timestamp": 1518092473
    },
    {
        "content": "<p>will be offline for an hour or so...</p>",
        "id": 488499,
        "sender_full_name": "Matthias",
        "timestamp": 1518108168
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-email=\"tabbott@zulipchat.com\" data-user-id=\"7\">@Tim Abbott</span> Hi, pinging you again as it's kinda a blocker for those affected.</p>",
        "id": 488765,
        "sender_full_name": "Matthias",
        "timestamp": 1518116836
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-email=\"matthiasmueller@protonmail.com\" data-user-id=\"1646\">@Matthias</span> Would you paste a copy of your <code>/var/log/zulip/errors.log</code>? That will have any errors the server is hitting with those users.</p>",
        "id": 488822,
        "sender_full_name": "Greg Price",
        "timestamp": 1518118094
    },
    {
        "content": "<p>Also</p>\n<blockquote>\n<p>The read message at the top props up and disappears again on a constant basis.</p>\n</blockquote>\n<p>What message exactly are you/they getting?</p>",
        "id": 488825,
        "sender_full_name": "Greg Price",
        "timestamp": 1518118127
    },
    {
        "content": "<p>while looking for the logs I can say this: when the server is offline there is the read pop down message at the top of the web app. That's what appears.</p>",
        "id": 488829,
        "sender_full_name": "Matthias",
        "timestamp": 1518118210
    },
    {
        "content": "<p>notifications don't work at all but if the user manually selects a PM or stream they will see the latest messages. The red warning also sporadically disappears.</p>",
        "id": 488833,
        "sender_full_name": "Matthias",
        "timestamp": 1518118246
    },
    {
        "content": "<p>Sorry, I'm still not sure which message you mean -- is it the \"couldn't connect to server\" message, or something else?</p>",
        "id": 488845,
        "sender_full_name": "Greg Price",
        "timestamp": 1518118380
    },
    {
        "content": "<p>i.e. what is the message text?</p>",
        "id": 488848,
        "sender_full_name": "Greg Price",
        "timestamp": 1518118385
    },
    {
        "content": "<p><a href=\"/user_uploads/2/2f/Xry3qfZyOf43UwppSucceupk/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/2f/Xry3qfZyOf43UwppSucceupk/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/2/2f/Xry3qfZyOf43UwppSucceupk/pasted_image.png\"></a></div>",
        "id": 488855,
        "sender_full_name": "Matthias",
        "timestamp": 1518118490
    },
    {
        "content": "<p>oh, error log is 89MB... need to trucate a bit first.</p>",
        "id": 488867,
        "sender_full_name": "Matthias",
        "timestamp": 1518118669
    },
    {
        "content": "<p>Sure</p>\n<p>And thanks for the screenshot!</p>",
        "id": 488873,
        "sender_full_name": "Greg Price",
        "timestamp": 1518118700
    },
    {
        "content": "<p>oh man, the error log has &gt; 1M lines and 99% of them are from the last two days</p>",
        "id": 488879,
        "sender_full_name": "Matthias",
        "timestamp": 1518118748
    },
    {
        "content": "<p>the below seems to be repeating every few seconds. This one is one of the first. </p>\n<div class=\"codehilite\"><pre><span></span>2018-02-07 14:24:44.593 ERR  [] Traceback (most recent call last):\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zulip-py3-venv/lib/python3.5/site-packages/django/core/handlers/base.py&quot;, line 185, in _get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zulip-py3-venv/lib/python3.5/site-packages/django/views/decorators/csrf.py&quot;, line 58, in wrapped_view\n    return view_func(*args, **kwargs)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/lib/rest.py&quot;, line 124, in rest_dispatch\n    return target_function(request, **kwargs)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/decorator.py&quot;, line 507, in _wrapped_view_func\n    return view_func(request, *args, **kwargs)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zulip-py3-venv/lib/python3.5/site-packages/django/utils/decorators.py&quot;, line 149, in _wrapped_view\n    response = view_func(request, *args, **kwargs)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/decorator.py&quot;, line 543, in _wrapped_view_func\n    return authenticate_log_and_execute_json(request, view_func, *args, **kwargs)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/decorator.py&quot;, line 525, in authenticate_log_and_execute_json\n    return rate_limit()(view_func)(request, user_profile, *args, **kwargs)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/decorator.py&quot;, line 686, in wrapped_func\n    return func(request, *args, **kwargs)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/lib/request.py&quot;, line 170, in _wrapped_view_func\n    return view_func(request, *args, **kwargs)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/views/messages.py&quot;, line 1017, in send_message_backend\n    local_id=local_id, sender_queue_id=queue_id)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/lib/actions.py&quot;, line 1692, in check_send_message\n    return do_send_messages([message])[0]\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/lib/actions.py&quot;, line 1184, in do_send_messages\n    wide_message_dict = MessageDict.wide_dict(message[&#39;message&#39;])\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/lib/message.py&quot;, line 138, in wide_dict\n    json = message_to_dict_json(message)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/lib/cache.py&quot;, line 163, in func_with_caching\n    cache_set(key, val, cache_name=cache_name, timeout=timeout)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zerver/lib/cache.py&quot;, line 174, in cache_set\n    cache_backend.set(KEY_PREFIX + key, (val,), timeout=timeout)\n  File &quot;/home/zulip/deployments/2018-01-22-22-48-38/zulip-py3-venv/lib/python3.5/site-packages/django/core/cache/backends/memcached.py&quot;, line 86, in set\n    if not self._cache.set(key, value, self.get_backend_timeout(timeout)):\npylibmc.TooBig\n</pre></div>",
        "id": 488893,
        "sender_full_name": "Matthias",
        "timestamp": 1518118972
    },
    {
        "content": "<p>Huh!</p>",
        "id": 488909,
        "sender_full_name": "Greg Price",
        "timestamp": 1518119419
    },
    {
        "content": "<p>Well, that's an error from memcached, then</p>",
        "id": 488911,
        "sender_full_name": "Greg Price",
        "timestamp": 1518119435
    },
    {
        "content": "<p>Sounds like it's saying that we're trying to store a value that is bigger than the memcache server can accommodate</p>",
        "id": 488912,
        "sender_full_name": "Greg Price",
        "timestamp": 1518119452
    },
    {
        "content": "<p>Maybe a giant message? Looks like it's trying to fetch a message</p>",
        "id": 488915,
        "sender_full_name": "Greg Price",
        "timestamp": 1518119483
    },
    {
        "content": "<p>Can that be deleted? How can I find that message? The problem spread from 3 or so to 6 people.</p>",
        "id": 488936,
        "sender_full_name": "Matthias",
        "timestamp": 1518119870
    },
    {
        "content": "<p>Also if we find the message, you guys and program something that prevents the problem from happening again.</p>",
        "id": 488937,
        "sender_full_name": "Matthias",
        "timestamp": 1518119919
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-email=\"matthiasmueller@protonmail.com\" data-user-id=\"1646\">@Matthias</span> looking into this now.</p>",
        "id": 489002,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518121857
    },
    {
        "content": "<p>So, this trace is super interesting.</p>",
        "id": 489064,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518122844
    },
    {
        "content": "<p>It's getting this error message in the <code>send_message</code> code path, when trying to store the message in the cache.</p>",
        "id": 489066,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518122875
    },
    {
        "content": "<p>It seems pretty unlikely that the message object itself is over the 1MB limit for message size (given that we have a 10KB limit on how big the input could be), though I could imagine some crazy corner case where the rendered markdown was 100x bigger than the original content.</p>",
        "id": 489069,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518122921
    },
    {
        "content": "<p>I wouldn't expect this to be the traceback if memcached were full and somehow not handling it well, but it's probably worth trying that possiblity first -- <span class=\"user-mention\" data-user-email=\"matthiasmueller@protonmail.com\" data-user-id=\"1646\">@Matthias</span> you should restart memcached and then the Zulip server and see if the problem goes away.</p>",
        "id": 489071,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518122962
    },
    {
        "content": "<p>What's the command to restart memcached?<br>\nBtw earlier today I rebooted the entire VM... Didn't help</p>",
        "id": 489076,
        "sender_full_name": "Matthias",
        "timestamp": 1518123053
    },
    {
        "content": "<p>Rebooting the VM would definitely achieve what restarting memcached would.</p>",
        "id": 489123,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518123465
    },
    {
        "content": "<p>If you run <code>top</code> to look at memory usage, does it appear the machine is starved for memory?</p>",
        "id": 489124,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518123496
    },
    {
        "content": "<p>i've given it 10GB of RAM... (but checking anyway)</p>",
        "id": 489125,
        "sender_full_name": "Matthias",
        "timestamp": 1518123515
    },
    {
        "content": "<p><a href=\"/user_uploads/2/c7/vNIoRMfS2Rhmv8rYpqOfjwRY/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">top</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/c7/vNIoRMfS2Rhmv8rYpqOfjwRY/pasted_image.png\" target=\"_blank\" title=\"top\"><img src=\"/user_uploads/2/c7/vNIoRMfS2Rhmv8rYpqOfjwRY/pasted_image.png\"></a></div>",
        "id": 489145,
        "sender_full_name": "Matthias",
        "timestamp": 1518123865
    },
    {
        "content": "<p>Definitely not a RAM shortage.</p>",
        "id": 489147,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518123881
    },
    {
        "content": "<p>to keep others interested in the loop: Tim did some remote debugging yesterday his guess so far is that it might be a message that was inflated by markdown to go way beyond the 10Kbyte limit. <br>\nI asked the affected people whether they share a stream and it looks like it. <br>\nThis means the number of people affected hasn't increased. It's just that I learnt about them one by one. It's 7 people but in the end 9 might be affected by it (private stream).</p>",
        "id": 489847,
        "sender_full_name": "Matthias",
        "timestamp": 1518163286
    },
    {
        "content": "<p><em>Can anyone help me create a postgres user that I can use to access the db from another client?</em> I'd prefer a GUI to browse the messages table to find that culprid message and then delete it with the manage.py dbshell</p>",
        "id": 489849,
        "sender_full_name": "Matthias",
        "timestamp": 1518163367
    },
    {
        "content": "<p>i was able to fix it.</p>",
        "id": 489921,
        "sender_full_name": "Matthias",
        "timestamp": 1518168631
    },
    {
        "content": "<p><span class=\"emoji emoji-1f389\" title=\"tada\">:tada:</span></p>",
        "id": 489922,
        "sender_full_name": "Matthias",
        "timestamp": 1518168636
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-email=\"matthiasmueller@protonmail.com\" data-user-id=\"1646\">@Matthias</span> Did the issue got fixed by deleting the message?</p>",
        "id": 489926,
        "sender_full_name": "Vishnu KS",
        "timestamp": 1518168844
    },
    {
        "content": "<p>the message in question didn't contain much in the unrendered column of the database. It was a .pdf link on the web. But the rendered content was 3,500,000 characters that blocked memcached and subsequently the stream for all 9 subscribers.</p>",
        "id": 489927,
        "sender_full_name": "Matthias",
        "timestamp": 1518168844
    },
    {
        "content": "<p>yes, I deleted the message with <code>/home/zulip/deployments/current/manage.py shell</code><br>\nthen <br>\n<code>Message.objects.get(1234).delete()</code></p>",
        "id": 489929,
        "sender_full_name": "Matthias",
        "timestamp": 1518168927
    },
    {
        "content": "<p>but first I had to create a prosgres user and enable external access to use a db manager (DBeaver) from my windows workstation (couldn't face doing this thorugh the CLI)<br>\nthen queried the <code>zerver_message</code> table with <code>length(rendered_content) &gt; 100000</code></p>",
        "id": 489930,
        "sender_full_name": "Matthias",
        "timestamp": 1518169016
    },
    {
        "content": "<p>immediately when I deleted the message the users affected got 'free'</p>",
        "id": 489931,
        "sender_full_name": "Matthias",
        "timestamp": 1518169148
    },
    {
        "content": "<p>Are URL previews enabled for this realm? If so would disabling url previews have prevented this? </p>\n<p>Not a fix. More of me trying to understand how the rendered content became so large.</p>",
        "id": 489957,
        "sender_full_name": "Scott Russell",
        "timestamp": 1518172622
    },
    {
        "content": "<p>I think it's almost certainly from the URL previews feature.</p>",
        "id": 490454,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518194822
    },
    {
        "content": "<p>But we'll be able to confirm after trying to reproduce in a development environment.</p>",
        "id": 490455,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518194839
    },
    {
        "content": "<p>Fascinating. Thanks <span class=\"user-mention\" data-user-email=\"matthiasmueller@protonmail.com\" data-user-id=\"1646\">@Matthias</span> for the report and the debugging!</p>",
        "id": 490842,
        "sender_full_name": "Greg Price",
        "timestamp": 1518205953
    },
    {
        "content": "<p>We should impose a limit on the size of rendered messages -- if it's going to go over say 2x the limit on the unrendered size, truncate. (With a bit of care to avoid unclosed HTML tags; probably just throw away any element that wants to span the boundary, i.e. throw away any unclosed tags at the limit and everything after them.)</p>",
        "id": 490852,
        "sender_full_name": "Greg Price",
        "timestamp": 1518206118
    },
    {
        "content": "<p>There's probably also something we can improve in the URL previews feature to make it less likely to hit that limit; if the limit comes into effect, you're probably not going to have a super great user experience on that message anyway, it just bounds how bad it can get.</p>",
        "id": 490856,
        "sender_full_name": "Greg Price",
        "timestamp": 1518206198
    },
    {
        "content": "<p>At Dropbox I ran the team that among other things was responsible for our Phabricator install (a code-review tool), and there were a number of edge-case bugs where if you sent a gigantic diff (for some enormous generated network-config files, say) the page wouldn't load; or if you sent a small diff one way but then a big diff at a different stage, it'd get past that but there'd be a perennially-failing task trying to render a notification email, or store something in the database, etc., etc. This bug is a lot like those, where the message came in under a limit we checked for but then ballooned later.</p>",
        "id": 490863,
        "sender_full_name": "Greg Price",
        "timestamp": 1518206413
    },
    {
        "content": "<p>Part of fixing that class of bug is to limit the size of an item again at every step where something nontrivial has changed or been done to it -- like our markdown rendering :)</p>",
        "id": 490869,
        "sender_full_name": "Greg Price",
        "timestamp": 1518206489
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-email=\"greg@zulipchat.com\" data-user-id=\"2187\">@Greg Price</span> I had the same idea: already resolved via <a href=\"https://github.com/zulip/zulip/commit/77addc5456c3e61062bd3615d1ec0d33bb105a90\" target=\"_blank\" title=\"https://github.com/zulip/zulip/commit/77addc5456c3e61062bd3615d1ec0d33bb105a90\">77addc5456c3e61062bd3615d1ec0d33bb105a90</a></p>",
        "id": 490889,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518206918
    },
    {
        "content": "<p>Great <span class=\"emoji emoji-1f600\" title=\"grinning\">:grinning:</span></p>",
        "id": 490890,
        "sender_full_name": "Greg Price",
        "timestamp": 1518206961
    },
    {
        "content": "<p>There's a second bug here (with the URL previews feature; it appears with certain PDF files it seems to end up putting the entire PDF file into the message body)</p>",
        "id": 490891,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1518206963
    },
    {
        "content": "<p>Yep -- this basically had to mean an issue in the previews feature too</p>",
        "id": 490892,
        "sender_full_name": "Greg Price",
        "timestamp": 1518206987
    },
    {
        "content": "<p>Ha, you even picked the same threshold (2x raw-length limit) I suggested.</p>",
        "id": 490894,
        "sender_full_name": "Greg Price",
        "timestamp": 1518207027
    }
]