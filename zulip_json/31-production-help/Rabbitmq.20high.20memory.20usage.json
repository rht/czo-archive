[
    {
        "content": "<p>Hi, since upgrade from 4.9 to 5.1 on Debian 11, I have a high memory alert in my monitoring tool.<br>\nThe ram seems to be full and I suspect rabbitmq to consume all the ram (many rabbitmq process spawned).</p>\n<p>Is there any tweak to reduce this ?</p>\n<p>Thx</p>",
        "id": 1376805,
        "sender_full_name": "Unam",
        "timestamp": 1651480910
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"12755\">@Unam</span>: We're not aware of any specific changes to RabbitMQ in 5.1 that would cause this.</p>\n<p>Are you running all of the server on one host?  In most such configs, PostgreSQL and memcached are the high memory processes.</p>\n<p>Be aware that RabbitMQ is highly multi-threaded, and depending on how you're looking, those may show up as processes -- and they share memory, so if you're parsing that as each line being independent memory usage, that's not the case.  What in particular makes you point at RabbitMQ?</p>\n<p>Can you show <code>rabbitmqctl list-queues</code>?</p>",
        "id": 1376993,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1651517626
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> , thx for your reply.<br>\nYou are probably right, rabbitmq is not the reason.</p>\n<div class=\"codehilite\"><pre><span></span><code>root@zulip  ~  rabbitmqctl list_queues\nTimeout: 60.0 seconds ...\nListing queues for vhost / ...\nname    messages\ndigest_emails   0\nembed_links 0\noutgoing_webhooks   0\nemail_senders   0\ninvites 0\nuser_activity   2\nsignups 0\nembedded_bots   0\nuser_activity_interval  0\nerror_reports   0\nuser_presence   0\nmissedmessage_mobile_notifications  0\nmissedmessage_emails    0\nnotify_tornado  0\nemail_mirror    0\ndeferred_work   0\n</code></pre></div>",
        "id": 1377199,
        "sender_full_name": "Unam",
        "timestamp": 1651579958
    },
    {
        "content": "<p>When I sort by percent_mem in htop, I have many manage.py process (attachement)</p>",
        "id": 1377200,
        "sender_full_name": "Unam",
        "timestamp": 1651580029
    },
    {
        "content": "<p><a href=\"/user_uploads/2/46/x-Mv9QMHeXYSL77-Npq-iSP7/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/46/x-Mv9QMHeXYSL77-Npq-iSP7/image.png\" title=\"image.png\"><img src=\"/user_uploads/2/46/x-Mv9QMHeXYSL77-Npq-iSP7/image.png\"></a></div>",
        "id": 1377201,
        "sender_full_name": "Unam",
        "timestamp": 1651580056
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"12755\">@Unam</span>: It looks like your server is using 3GB out of the 3.84GB it has -- which in general <em>is a good thing</em>.  Memory which isn't in use is going to waste.</p>\n<p>None of the memory sizes there jump out to me as being egregiously wrong.</p>",
        "id": 1378001,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1651692149
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> , I made the upgrade on April 28th.<br>\nAs you can see below, memory usage was lowest before.</p>\n<p><a href=\"/user_uploads/2/fa/lxvSWD8ow7Qi1yOuAROibj3t/image.png\">image.png</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/fa/lxvSWD8ow7Qi1yOuAROibj3t/image.png\" title=\"image.png\"><img src=\"/user_uploads/2/fa/lxvSWD8ow7Qi1yOuAROibj3t/image.png\"></a></div><p>Anyway, it's not a big problem from my point of view, my monitoring system just popped up an alert and I was wondering why.<br>\nAlso, the server is swapping more than before, don't know why yet.</p>",
        "id": 1378301,
        "sender_full_name": "Unam",
        "timestamp": 1651735902
    }
]