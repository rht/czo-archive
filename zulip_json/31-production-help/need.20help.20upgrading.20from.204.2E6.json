[
    {
        "content": "<p>Hi all, and thank you for your time!</p>\n<p>I've been managing a Zulip server for my small org of ~10 people for the past 5 years. I want to move us to the hosted Zulip Cloud version, but first I need to upgrade to the latest version before we can hop over.</p>\n<p>I am currently on Zulip version 4.6 (running on an AWS EC2 instance running Ubuntu 18.04).</p>\n<p>Given the outdated-ness I believe I would have to:</p>\n<p>1) Migrate my z4.6 on ub18.04, to a z4.6 on ub20.04.<br>\n2) Upgrade all the way to stable (probably first to 5.0, then to stable?)</p>",
        "id": 1499386,
        "sender_full_name": "Will Monge",
        "timestamp": 1675113863
    },
    {
        "content": "<p>In trying this migration from step 1. I am encountering the following error when trying to restore the backup file. See error below:</p>\n<div class=\"codehilite\"><pre><span></span><code>+ /home/zulip/deployments/2023-01-12-00-52-04/scripts/zulip-puppet-apply -f\nNotice: Compiled catalog for ip-10-0-0-106.ec2.internal in environment production in 4.20 seconds\nNotice: /Stage[main]/Zulip::Apt_repository/Exec[setup_apt_repo]/returns: executed successfully\nNotice: /Stage[main]/Zulip::Nginx/Package[certbot]/ensure: created\nNotice: /Stage[main]/Zulip::Tornado_sharding/File[/etc/zulip/nginx_sharding.conf]/owner: owner changed &#39;zulip&#39; to &#39;root&#39;\nNotice: /Stage[main]/Zulip::Tornado_sharding/File[/etc/zulip/nginx_sharding.conf]/group: group changed &#39;zulip&#39; to &#39;root&#39;\nNotice: /Stage[main]/Zulip::App_frontend_base/File[/etc/zulip/uwsgi.ini]/owner: owner changed &#39;zulip&#39; to &#39;root&#39;\nNotice: /Stage[main]/Zulip::App_frontend_base/File[/etc/zulip/uwsgi.ini]/group: group changed &#39;zulip&#39; to &#39;root&#39;\nNotice: /Stage[main]/Zulip::Profile::Redis/File[/etc/redis/zulip-redis.conf]/content: content changed &#39;{md5}85e216320ecde291dc81cf1b44c345de&#39; to &#39;{md5}4b60a9ef0f276c7a2c9270c7b87241a1&#39;\nNotice: /Stage[main]/Zulip::Profile::Redis/Service[redis-server]: Triggered &#39;refresh&#39; from 1 event\nNotice: /Stage[main]/Zulip::Profile::Memcached/File[/etc/sasl2/memcached-zulip-password]/content: content changed &#39;{md5}27449f1417db1df9f8cf1fb64231c659&#39; to &#39;{md5}dc253ac5737b9f7f6e66b87eeac01c53&#39;\nNotice: /Stage[main]/Zulip::Profile::Memcached/Exec[generate_memcached_sasldb2]: Triggered &#39;refresh&#39; from 1 event\nNotice: /Stage[main]/Zulip::Profile::Memcached/File[/etc/sasl2/memcached-sasldb2]/owner: owner changed &#39;root&#39; to &#39;memcache&#39;\nNotice: /Stage[main]/Zulip::Profile::Memcached/File[/etc/sasl2/memcached-sasldb2]/group: group changed &#39;zulip&#39; to &#39;memcache&#39;\nNotice: /Stage[main]/Zulip::Profile::Memcached/File[/etc/sasl2/memcached-sasldb2]/mode: mode changed &#39;0640&#39; to &#39;0600&#39;\nNotice: /Stage[main]/Zulip::Profile::Rabbitmq/File[/etc/rabbitmq]/owner: owner changed &#39;rabbitmq&#39; to &#39;root&#39;\nNotice: /Stage[main]/Zulip::Profile::Rabbitmq/File[/etc/rabbitmq]/group: group changed &#39;rabbitmq&#39; to &#39;root&#39;\nNotice: /Stage[main]/Zulip::Camo/File[/etc/default/camo]/content: content changed &#39;{md5}57d177275b8b724948afd2cb35861560&#39; to &#39;{md5}ace22afb9a5c3eb177a3f2b38f846b48&#39;\nNotice: /Stage[main]/Zulip::Camo/Service[camo]: Triggered &#39;refresh&#39; from 1 event\nNotice: /Stage[main]/Zulip::Nginx/Service[nginx]: Triggered &#39;refresh&#39; from 2 events\nNotice: /Stage[main]/Zulip::Postgresql_common/Zulip::Safepackage[postgresql-10]/Package[postgresql-10]/ensure: created\nNotice: /Stage[main]/Zulip::Postgresql_base/File[/usr/share/postgresql/10/tsearch_data/zulip_english.stop]/ensure: defined content as &#39;{md5}34e771a8f53b4ae2e8b43affd3f820b2&#39;\nNotice: /Stage[main]/Zulip::Profile::Postgresql/File[/etc/postgresql/10/main/postgresql.conf]/content: content changed &#39;{md5}ab3b2b1f919468fa526e64d383c365cf&#39; to &#39;{md5}efc78dc8f0de6348e5a7bc9ec1d295a0&#39;\nNotice: /Stage[main]/Zulip::Profile::Postgresql/Exec[pg_ctlcluster 10 main restart]/returns: Job for postgresql@10-main.service failed because the service did not take the steps required by its unit configuration.\nNotice: /Stage[main]/Zulip::Profile::Postgresql/Exec[pg_ctlcluster 10 main restart]/returns: See &quot;systemctl status postgresql@10-main.service&quot; and &quot;journalctl -xe&quot; for details.\nError: /Stage[main]/Zulip::Profile::Postgresql/Exec[pg_ctlcluster 10 main restart]: Failed to call refresh: &#39;pg_ctlcluster 10 main restart&#39; returned 1 instead of one of [0]\nError: /Stage[main]/Zulip::Profile::Postgresql/Exec[pg_ctlcluster 10 main restart]: &#39;pg_ctlcluster 10 main restart&#39; returned 1 instead of one of [0]\nNotice: /Stage[main]/Zulip::Supervisor/Service[supervisor]: Triggered &#39;refresh&#39; from 2 events\nNotice: Applied catalog in 42.68 seconds\n\nError running a subcommand of /home/zulip/deployments/current/scripts/setup/restore-backup: /home/zulip/deployments/2023-01-12-00-52-04/scripts/zulip-puppet-apply -f\nActual error output for the subcommand is just above this.\n</code></pre></div>",
        "id": 1499387,
        "sender_full_name": "Will Monge",
        "timestamp": 1675113970
    },
    {
        "content": "<p>I made sure that the postgres version was the same on both the ub18 and ub20, and in fact I confirmed that both servers show the same db migrations. On both the last migration was <code>0325_alter_realmplayground_unique_together</code>.</p>",
        "id": 1499388,
        "sender_full_name": "Will Monge",
        "timestamp": 1675114067
    },
    {
        "content": "<p>Is this a new 20.04 host you installed onto, and you're trying to move the data onto?  That's not something we test, but <em>should</em> woirk if you have the same <code>/etc/zulip/</code> on both hosts.</p>\n<p>Our suggestion is generally to upgrade the host in place: <a href=\"https://zulip.readthedocs.io/en/latest/production/upgrade.html#upgrading-the-operating-system\">https://zulip.readthedocs.io/en/latest/production/upgrade.html#upgrading-the-operating-system</a></p>",
        "id": 1499400,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1675115170
    },
    {
        "content": "<blockquote>\n<p>2) Upgrade all the way to stable (probably first to 5.0, then to stable?)</p>\n</blockquote>\n<p>You can go direct from 4.6 to 6.1.</p>",
        "id": 1499402,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1675115216
    },
    {
        "content": "<p>If you want to debug your duplicate-host, I'd start with showing <code>apt-cache policy postgresql-10</code> and <code>systemctl status postgresql@10-main.service</code></p>",
        "id": 1499403,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1675115256
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/need.20help.20upgrading.20from.204.2E6/near/1499400\">said</a>:</p>\n<blockquote>\n<p>Is this a new 20.04 host you installed onto, and you're trying to move the data onto?  That's not something we test, but <em>should</em> woirk if you have the same <code>/etc/zulip/</code> on both hosts.</p>\n<p>Our suggestion is generally to upgrade the host in place: <a href=\"https://zulip.readthedocs.io/en/latest/production/upgrade.html#upgrading-the-operating-system\">https://zulip.readthedocs.io/en/latest/production/upgrade.html#upgrading-the-operating-system</a></p>\n</blockquote>\n<p>yup, this error is trying to move the data to a separate host running the same version of zulip on ub20.04.</p>",
        "id": 1499407,
        "sender_full_name": "Will Monge",
        "timestamp": 1675115923
    },
    {
        "content": "<p>Originally, I tried upgrading the AWS instance from 18.04 to 20.04 directly, but that didn't go well. And after going through some AWS help forums I saw the general advice was to simply create a new EC2 instance and port the apps/data (not zulip-specific).</p>",
        "id": 1499414,
        "sender_full_name": "Will Monge",
        "timestamp": 1675116012
    },
    {
        "content": "<p>If anybody has experience upgrading an AWS EC2 instance, I'm happy to follow along.</p>",
        "id": 1499417,
        "sender_full_name": "Will Monge",
        "timestamp": 1675116059
    },
    {
        "content": "<p>I don't have any of the problems I encountered available, so I'll try it again tonight, and post any errors I get. Thank you <span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span>!</p>",
        "id": 1499420,
        "sender_full_name": "Will Monge",
        "timestamp": 1675116225
    },
    {
        "content": "<p>We've done <a href=\"https://zulip.readthedocs.io/en/latest/production/upgrade.html#upgrading-the-operating-system\">https://zulip.readthedocs.io/en/latest/production/upgrade.html#upgrading-the-operating-system</a> on EC2 AWS hosts without issue.</p>\n<p>The \"just make a new one\" response comes from the \"cattle not pets\" mentality when folks have large deploys, and it makes a lot of sense to have a robust \"start from scratch\" deploy technique.  When you have only a couple hosts, there's no reason to not do an in-place upgrade.  In the bad old Xen hypervisor days, you had to get your kernel versions to agree, which could be a hassle, but  ther's no such limitation now.</p>",
        "id": 1499423,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1675116326
    },
    {
        "content": "<p>yeah, that's a good point. I would prefer to just upgrade the working instance. I'll attempt as people don't need the active server, thanks!</p>",
        "id": 1499426,
        "sender_full_name": "Will Monge",
        "timestamp": 1675116525
    }
]