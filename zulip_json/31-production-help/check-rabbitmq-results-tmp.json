[
    {
        "content": "<p>Hi, I occasionally get emails with content like this: <code>mv: cannot stat '/var/lib/nagios_state/check-rabbitmq-results-tmp': No such file or directory</code>. This is from a small install, on debian, pretty standard. I don't really know where to start looking here, maybe someone knows about this? Thanks in advance!</p>",
        "id": 887972,
        "sender_full_name": "tostr",
        "timestamp": 1590606610
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"14326\">@tostr</span> interesting.  We do have a cron job that runs <code>/home/zulip/deployments/current/scripts/nagios/check-rabbitmq-queue &amp;&gt; /var/lib/nagios_state/check-rabbitmq-results-tmp; mv /var/lib/nagios_state/check-rabbitmq-results-tmp /var/lib/nagios_state/check-rabbitmq-results</code>, but I've never seen that error.</p>",
        "id": 887986,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1590606933
    },
    {
        "content": "<p>Is it possible you don't have a <code>/var/lib/nagios_state</code> directory?</p>",
        "id": 887987,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1590606949
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>$ ll /var/lib/nagios_state/\ntotal 100K\n-rw-r--r-- 1 zulip zulip 38 May 27 20:30 check-analytics-state\n-rw-r--r-- 1 root  root  61 May 27 21:16 check-rabbitmq-consumers-deferred_work\n-rw-r--r-- 1 root  root  80 May 18 23:39 check-rabbitmq-consumers-deferred_workdigest_emails\n-rw-r--r-- 1 root  root  61 May 27 21:16 check-rabbitmq-consumers-digest_emails\n-rw-r--r-- 1 root  root  60 May 27 21:16 check-rabbitmq-consumers-email_mirror\n-rw-r--r-- 1 root  root  61 May 27 21:16 check-rabbitmq-consumers-email_senders\n-rw-r--r-- 1 root  root  61 May 27 21:16 check-rabbitmq-consumers-embedded_bots\n-rw-r--r-- 1 root  root  59 May 27 21:16 check-rabbitmq-consumers-embed_links\n-rw-r--r-- 1 root  root  61 May 27 21:16 check-rabbitmq-consumers-error_reports\n-rw-r--r-- 1 root  root  71 May 18 23:39 check-rabbitmq-consumers-feedback_messages\n-rw-r--r-- 1 root  root  55 May 27 21:16 check-rabbitmq-consumers-invites\n-rw-r--r-- 1 root  root  62 May 27 21:16 check-rabbitmq-consumers-message_sender\n-rw-r--r-- 1 root  root  68 May 27 21:16 check-rabbitmq-consumers-missedmessage_emails\n-rw-r--r-- 1 root  root  75 May 27 21:16 check-rabbitmq-consumers-missedmessage_email_senders\n-rw-r--r-- 1 root  root  82 May 27 21:16 check-rabbitmq-consumers-missedmessage_mobile_notifications\n-rw-r--r-- 1 root  root  62 May 27 21:16 check-rabbitmq-consumers-notify_tornado\n-rw-r--r-- 1 root  root  65 May 27 21:16 check-rabbitmq-consumers-outgoing_webhooks\n-rw-r--r-- 1 root  root  55 May 27 21:16 check-rabbitmq-consumers-signups\n-rw-r--r-- 1 root  root  66 May 27 21:16 check-rabbitmq-consumers-slow_queries\n-rw-r--r-- 1 root  root  62 May 27 21:16 check-rabbitmq-consumers-tornado_return\n-rw-r--r-- 1 root  root  67 May 27 21:16 check-rabbitmq-consumers-user_activity\n-rw-r--r-- 1 root  root  70 May 27 21:16 check-rabbitmq-consumers-user_activity_interval\n-rw-r--r-- 1 root  root  89 May 18 23:39 check-rabbitmq-consumers-user_activityuser_activity_interval\n-rw-r--r-- 1 root  root  61 May 27 21:16 check-rabbitmq-consumers-user_presence\n-rw-r--r-- 1 root  root  43 May 27 21:16 check-rabbitmq-results\n</code></pre></div>",
        "id": 887991,
        "sender_full_name": "tostr",
        "timestamp": 1590606983
    },
    {
        "content": "<p>no this exists</p>",
        "id": 887993,
        "sender_full_name": "tostr",
        "timestamp": 1590606988
    },
    {
        "content": "<p>hmm, I was assuming that it was something common because our setup is really boring. Could this be memory related? The server has 2G <span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span> (about 15 users in total)</p>",
        "id": 887996,
        "sender_full_name": "tostr",
        "timestamp": 1590607111
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"14326\">@tostr</span> it could be memory related; check <code>dmesg</code> for evidence of OOM kills?</p>",
        "id": 888000,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1590607253
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span> I will try to correlate that the next time I get this email (it happens somewhat irregularly)</p>",
        "id": 888152,
        "sender_full_name": "tostr",
        "timestamp": 1590613316
    },
    {
        "content": "<p>Thanks!</p>",
        "id": 888153,
        "sender_full_name": "tostr",
        "timestamp": 1590613321
    },
    {
        "content": "<p>About this problem that I described some time ago, I upped the memory on the server to 4G but tonight I got a lot of mails with this:</p>\n<div class=\"codehilite\"><pre><span></span><code>Error: unable to perform an operation on node &#39;zulip@localhost&#39;. Please see diagnostics information and suggestions below.\n\nMost common reasons for this are:\n\n * Target node is unreachable (e.g. due to hostname resolution, TCP connection or firewall issues)\n * CLI tool fails to authenticate with the server (e.g. due to CLI tool&#39;s Erlang cookie not matching that of the server)\n * Target node is not running\n\nIn addition to the diagnostics info below:\n\n * See the CLI, clustering and networking guides on http://rabbitmq.com/documentation.html to learn more\n * Consult server logs on node zulip@localhost\n\nDIAGNOSTICS\n===========\n\nattempted to contact: [zulip@localhost]\n\nzulip@localhost:\n  * connected to epmd (port 4369) on localhost\n  * epmd reports node &#39;zulip&#39; uses port 25672 for inter-node and CLI tool traffic\n  * can&#39;t establish TCP connection to the target node, reason: econnrefused (connection refused)\n  * suggestion: check if host &#39;localhost&#39; resolves, is reachable and ports 25672, 4369 are not blocked by firewall\n\nCurrent node details:\n * node name: rabbitmqcli74@higanbana\n * effective user&#39;s home directory: /var/lib/rabbitmq\n * Erlang cookie hash: oqtA3/YgeKTLL8dk4ycWNg==\n\nTraceback (most recent call last):\n  File &quot;/home/zulip/deployments/current/scripts/nagios/check-rabbitmq-consumers&quot;, line 44, in &lt;module&gt;\n    universal_newlines=True)\n  File &quot;/usr/lib/python3.7/subprocess.py&quot;, line 395, in check_output\n    **kwargs).stdout\n  File &quot;/usr/lib/python3.7/subprocess.py&quot;, line 487, in run\n    output=stdout, stderr=stderr)\nsubprocess.CalledProcessError: Command &#39;[&#39;/usr/sbin/rabbitmqctl&#39;, &#39;list_consumers&#39;]&#39; returned non-zero exit status 69.\n</code></pre></div>\n\n\n<p>Looking at <code>dmesg</code> I see python3 getting oom-reaped by the system:</p>\n<div class=\"codehilite\"><pre><span></span><code>[819935.521384] Out of memory: Kill process 21321 (python3) score 24 or sacrifice child\n[819935.522234] Killed process 21321 (python3) total-vm:224260kB, anon-rss:99104kB, file-rss:0kB, shmem-rss:0kB\n[819935.531411] oom_reaper: reaped process 21321 (python3), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB\n</code></pre></div>\n\n\n<p><code>/var/lib/nagios_state</code> does not exists, I have rebooted the machine for now.</p>",
        "id": 912488,
        "sender_full_name": "tostr",
        "timestamp": 1592986837
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"14326\">@tostr</span> well, OOM kill is pretty clearly the issue here.  I suspect that you'll find the issue resolved when you upgrade to the upcoming major release, since we reduce steady-state memory requirements somewhat.</p>",
        "id": 912827,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1593018839
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span>  ok, then I will just do a zulip restart every night for the time being, but might there be a leak somewhere? 4G ram with just 14 users chatting less than 100msg/day seems a lot, plus the fact that it does only happen if the server runs for some time? I don't have a clear expectation of memory usage for zulip, but the behaviour seems to suggest that something is off.</p>\n<p>Just a thought, thanks a lot for getting back to me!</p>",
        "id": 913015,
        "sender_full_name": "tostr",
        "timestamp": 1593024168
    },
    {
        "content": "<p>I don't think it's likely a memory leak; more likely just the cron jobs for that Nagios work starting up is putting you over the edge.</p>",
        "id": 913018,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1593024222
    },
    {
        "content": "<p>(We have monitoring for memory leaks on the servers we run, and haven't seen a significant leak in years with much more load than that)</p>",
        "id": 913020,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1593024243
    }
]