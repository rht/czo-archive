[
    {
        "content": "<p>I tried to upgrade from Zulip Server 6.1 to 7.0 beta1... it failed. I don't know why. I tried twice, same result. I'm running version 6.1 on Proxmox LXC Debian container. Glad I took a snapshot before attempting the upgrade.</p>",
        "id": 1563118,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683422059
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"26472\">@Louis-Marie Gendron</span> can you post the error that was printed when the upgrade failed?</p>",
        "id": 1563128,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1683429844
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span> I kept the upgrade.log:</p>\n<p><a href=\"/user_uploads/2/92/sCbPZ2TNOe1uJdGslop5dInO/upgrade.log\">upgrade.log</a></p>",
        "id": 1563216,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683473242
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"26472\">@Louis-Marie Gendron</span>: It looks like your upgrades to previous versions of Zulip have also failed:</p>\n<div class=\"codehilite\"><pre><span></span><code>2022-03-02 05:19:53,626 upgrade-zulip-stage-2: Applying Puppet changes...\nNotice: Compiled catalog for chat.hs-mittweida.de in environment production in 1.68 seconds\nNotice: /Stage[main]/Zulip::Smokescreen/Tidy[/usr/local/bin/smokescreen-*]: Tidying 1 files\nNotice: /Stage[main]/Zulip::Golang/Zulip::External_dep[golang]/Tidy[/srv/zulip-golang-*]: Tidying 1 files\nNotice: /Stage[main]/Zulip::Smokescreen/Zulip::External_dep[smokescreen-src]/Tidy[/srv/zulip-smokescreen-src-*]: Tidying 1 files\nNotice: /Stage[main]/Zulip::Camo/Zulip::External_dep[go-camo]/Tidy[/srv/zulip-go-camo-*]: Tidying 1 files\nNotice: /Stage[main]/Zulip::Apt_repository/Exec[setup_apt_repo]/returns: executed successfully\nNotice: /Stage[main]/Zulip::Smokescreen/File[/usr/local/bin/smokescreen-dc403015f563eadc556a61870c6ad327688abe88-go-1.17.3]/ensure: removed\nNotice: /Stage[main]/Zulip::Profile::App_frontend/File[/etc/nginx/sites-available/zulip-enterprise]/content: content changed &#39;{md5}75c790b7bb2704adc21dba01fe87245d&#39; to &#39;{md5}85f086aa6977456cc5f25612ecb199b9&#39;\nNotice: /Stage[main]/Zulip::Profile::App_frontend/File[/etc/letsencrypt/renewal-hooks/deploy/001-nginx.sh]/ensure: defined content as &#39;{md5}980fe968383378cccb8fb8806611ea20&#39;\nNotice: /Stage[main]/Zulip::Profile::App_frontend/Exec[fix-standalone-certbot]/returns: Saving debug log to /var/log/letsencrypt/letsencrypt.log\nNotice: /Stage[main]/Zulip::Profile::App_frontend/Exec[fix-standalone-certbot]/returns: Plugins selected: Authenticator webroot, Installer None\nNotice: /Stage[main]/Zulip::Profile::App_frontend/Exec[fix-standalone-certbot]/returns: Enter email address (used for urgent renewal and security notices) (Enter &#39;c&#39; to\nNotice: /Stage[main]/Zulip::Profile::App_frontend/Exec[fix-standalone-certbot]/returns: cancel): An unexpected error occurred:\nNotice: /Stage[main]/Zulip::Profile::App_frontend/Exec[fix-standalone-certbot]/returns: EOFError\nNotice: /Stage[main]/Zulip::Profile::App_frontend/Exec[fix-standalone-certbot]/returns: Please see the logfiles in /var/log/letsencrypt for more details.\nError: &#39;/home/zulip/deployments/2022-03-02-06-19-17/scripts/lib/fix-standalone-certbot&#39; returned 1 instead of one of [0]\nError: /Stage[main]/Zulip::Profile::App_frontend/Exec[fix-standalone-certbot]/returns: change from &#39;notrun&#39; to [&#39;0&#39;] failed: &#39;/home/zulip/deployments/2022-03-02-06-19-17/scripts/lib/fix-standalone-certbot&#39; returned 1 instead of one of [0]\nError: /Stage[main]/Zulip::Nginx/Service[nginx]: Failed to call refresh: Systemd restart for nginx failed!\njournalctl log for nginx:\n-- Logs begin at Sun 2022-02-27 17:40:24 CET, end at Wed 2022-03-02 06:20:07 CET. --\nMar 02 06:20:07 chat systemd[1]: Stopping A high performance web server and a reverse proxy server...\nMar 02 06:20:07 chat systemd[1]: nginx.service: Succeeded.\nMar 02 06:20:07 chat systemd[1]: Stopped A high performance web server and a reverse proxy server.\nMar 02 06:20:07 chat systemd[1]: Starting A high performance web server and a reverse proxy server...\nMar 02 06:20:07 chat nginx[48267]: nginx: [emerg] BIO_new_file(&quot;/etc/ssl/certs/zulip.combined-chain.crt&quot;) failed (SSL: error:02001002:system library:fopen:No such file or directory:fopen(&#39;/etc/ssl/certs/zulip.combined-chain.crt&#39;,&#39;r&#39;) error:2006D080:BIO routines:BIO_new_file:no such file)\nMar 02 06:20:07 chat nginx[48267]: nginx: configuration file /etc/nginx/nginx.conf test failed\nMar 02 06:20:07 chat systemd[1]: nginx.service: Control process exited, code=exited, status=1/FAILURE\nMar 02 06:20:07 chat systemd[1]: nginx.service: Failed with result &#39;exit-code&#39;.\nMar 02 06:20:07 chat systemd[1]: Failed to start A high performance web server and a reverse proxy server.\n\nError: /Stage[main]/Zulip::Nginx/Service[nginx]: Systemd restart for nginx failed!\n</code></pre></div>\n<p>It looks like, from the cert presented by <a href=\"https://chat.hs-mittweida.de/\">https://chat.hs-mittweida.de/</a>, that you're not using certbot.  Can you show (in a <a href=\"https://zulip.com/help/code-blocks\">code block</a>) the output of running, as root:</p>\n<div class=\"codehilite\"><pre><span></span><code>ls /etc/letsencrypt/renewal/*.conf\n\ncertbot certificates\n</code></pre></div>",
        "id": 1563463,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683552033
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> doesn't look like my upgrade.log, I did not attempt to upgrade in march 2022.</p>\n<div class=\"codehilite\"><pre><span></span><code>2023-05-04 01:02:33,026 upgrade-zulip-stage-2: Applying Puppet changes...\n</code></pre></div>",
        "id": 1563588,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683561800
    },
    {
        "content": "<p>Grr.  Sorry, too many upgrade logs.   Let me look again.</p>",
        "id": 1563590,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683561834
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>2023-05-04 01:03:53,230 upgrade-zulip-stage-2: Stopping Zulip...\nTraceback (most recent call last):\n  File &quot;/home/zulip/deployments/2023-05-03-21-01-18/./scripts/stop-server&quot;, line 53, in &lt;module&gt;\n    services = list_supervisor_processes(services, only_running=True)\n  File &quot;/home/zulip/deployments/2023-05-03-21-01-18/./scripts/../scripts/lib/supervisor.py&quot;, line 34, in list_supervisor_processes\n    processes = rpc().supervisor.getAllProcessInfo()\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1116, in __call__\n    return self.__send(self.__name, args)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1456, in __request\n    response = self.__transport.request(\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1160, in request\n    return self.single_request(host, handler, request_body, verbose)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1172, in single_request\n    http_conn = self.send_request(host, handler, request_body, verbose)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1285, in send_request\n    self.send_content(connection, request_body)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1315, in send_content\n    connection.endheaders(request_body)\n  File &quot;/usr/lib/python3.9/http/client.py&quot;, line 1250, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File &quot;/usr/lib/python3.9/http/client.py&quot;, line 1010, in _send_output\n    self.send(msg)\n  File &quot;/usr/lib/python3.9/http/client.py&quot;, line 950, in send\n    self.connect()\n  File &quot;/home/zulip/deployments/2023-05-03-21-01-18/./scripts/../scripts/lib/supervisor.py&quot;, line 10, in connect\n    self.sock.connect(self.host)\nFileNotFoundError: [Errno 2] No such file or directory\nTraceback (most recent call last):\n  File &quot;/home/zulip/deployments/2023-05-03-21-01-18/scripts/lib/upgrade-zulip-stage-2&quot;, line 528, in &lt;module&gt;\n    shutdown_server()\n  File &quot;/home/zulip/deployments/2023-05-03-21-01-18/scripts/lib/upgrade-zulip-stage-2&quot;, line 202, in shutdown_server\n    subprocess.check_call([&quot;./scripts/stop-server&quot;], preexec_fn=su_to_zulip)\n  File &quot;/usr/lib/python3.9/subprocess.py&quot;, line 373, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command &#39;[&#39;./scripts/stop-server&#39;]&#39; returned non-zero exit status 1.\n</code></pre></div>\n<p>OK, so this is a failure talking to supervisor's socket.</p>",
        "id": 1563594,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683561912
    },
    {
        "content": "<p>I assume <code>supervisorctl status</code> looks reasonable?</p>",
        "id": 1563597,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683561954
    },
    {
        "content": "<p>What OS is this?  Can you show:</p>\n<div class=\"codehilite\"><pre><span></span><code>ls -ld /var/run /run /var/run/supervisor.sock\n</code></pre></div>",
        "id": 1563599,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683562051
    },
    {
        "content": "<p><code>supervisorctl status</code></p>\n<blockquote>\n<p>root@ct-zulip:~# supervisorctl status<br>\ngo-camo                                                         RUNNING   pid 600, uptime 2 days, 1:43:14<br>\nprocess-fts-updates                                             RUNNING   pid 83643, uptime 1 day, 6:53:39<br>\nsmokescreen                                                     RUNNING   pid 601, uptime 2 days, 1:43:14<br>\nzulip-django                                                    RUNNING   pid 83654, uptime 1 day, 6:53:35<br>\nzulip-tornado                                                   RUNNING   pid 83646, uptime 1 day, 6:53:38<br>\nzulip-workers:zulip_events_deferred_work                        RUNNING   pid 83500, uptime 1 day, 6:54:00<br>\nzulip-workers:zulip_events_digest_emails                        RUNNING   pid 83505, uptime 1 day, 6:53:59<br>\nzulip-workers:zulip_events_email_mirror                         RUNNING   pid 83510, uptime 1 day, 6:53:57<br>\nzulip-workers:zulip_events_email_senders                        RUNNING   pid 83515, uptime 1 day, 6:53:56<br>\nzulip-workers:zulip_events_embed_links                          RUNNING   pid 83520, uptime 1 day, 6:53:55<br>\nzulip-workers:zulip_events_embedded_bots                        RUNNING   pid 83527, uptime 1 day, 6:53:53<br>\nzulip-workers:zulip_events_error_reports                        RUNNING   pid 83532, uptime 1 day, 6:53:52<br>\nzulip-workers:zulip_events_invites                              RUNNING   pid 83537, uptime 1 day, 6:53:51<br>\nzulip-workers:zulip_events_missedmessage_emails                 RUNNING   pid 83542, uptime 1 day, 6:53:49<br>\nzulip-workers:zulip_events_missedmessage_mobile_notifications   RUNNING   pid 83605, uptime 1 day, 6:53:48<br>\nzulip-workers:zulip_events_outgoing_webhooks                    RUNNING   pid 83611, uptime 1 day, 6:53:47<br>\nzulip-workers:zulip_events_user_activity                        RUNNING   pid 83616, uptime 1 day, 6:53:45<br>\nzulip-workers:zulip_events_user_activity_interval               RUNNING   pid 83623, uptime 1 day, 6:53:44<br>\nzulip-workers:zulip_events_user_presence                        RUNNING   pid 83628, uptime 1 day, 6:53:43<br>\nzulip_deliver_scheduled_emails                                  RUNNING   pid 83633, uptime 1 day, 6:53:41<br>\nzulip_deliver_scheduled_messages                                RUNNING   pid 83638, uptime 1 day, 6:53:40</p>\n</blockquote>\n<p><code>ls -ld /var/run /run /var/run/supervisor.sock</code></p>\n<blockquote>\n<p>drwxr-xr-x 18 root  root  580 May  8 12:52 /run<br>\nlrwxrwxrwx  1 root  root    4 Dec 19 14:43 /var/run -&gt; /run<br>\nsrwx------  1 zulip zulip   0 May  6 11:10 /var/run/supervisor.sock</p>\n</blockquote>\n<p>It's running in a Proxmox LXC container : <br>\n<code>OS: Debian GNU/Linux 11 (bullseye) x86_64</code> <code>Kernel: 5.15.107-1-pve</code></p>",
        "id": 1563670,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683565129
    },
    {
        "content": "<p>Hm.  It seems like the LXC property is likely being the problematic part, but I don't think there's anything new in the 7.0 codepath vs 6.x which would be notable here.  And the UNIX socket exists as expected, and we're running as the <code>zulip</code> user.</p>",
        "id": 1563857,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683572760
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/2-general/topic/Zulip.207.2E0.20upgrades/near/1563857\">said</a>:</p>\n<blockquote>\n<p>Hm.  It seems like the LXC property is likely being the problematic part, but I don't think there's anything new in the 7.0 codepath vs 6.x which would be notable here.  And the UNIX socket exists as expected, and we're running as the <code>zulip</code> user.</p>\n</blockquote>\n<p>Does this means I'm stuck with 6.1? I won't be able to upgrade to 7.0 when final release is available?</p>",
        "id": 1563970,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683576775
    },
    {
        "content": "<p>No, it means we're not sure what the problem is on your system yet.</p>",
        "id": 1564041,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1683580719
    },
    {
        "content": "<p>Yeah, we do want to get to the bottom of this, but we've been distracted wit a few other issues today.  I may have more things to try tomorrow.</p>",
        "id": 1564042,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683580793
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"7\">Tim Abbott</span> <a href=\"#narrow/stream/2-general/topic/Zulip.207.2E0.20upgrades/near/1564041\">said</a>:</p>\n<blockquote>\n<p>No, it means we're not sure what the problem is on your system yet.</p>\n</blockquote>\n<p>Ok. No pressure then. Zulip Server 6.1 is working fine. I might be missing a package on my Debian LXC container? This is strange. Initial Zulip install was done with :</p>\n<div class=\"codehilite\"><pre><span></span><code>cd $(mktemp -d)\ncurl -fLO https://download.zulip.com/server/zulip-server-latest.tar.gz\ntar -xf zulip-server-latest.tar.gz\n</code></pre></div>\n<p>then</p>\n<div class=\"codehilite\"><pre><span></span><code>./zulip-server-*/scripts/setup/install --certbot \\\n    --email=YOUR_EMAIL --hostname=YOUR_HOSTNAME\n</code></pre></div>\n<p>Users and messages were imported from RocketChat.</p>",
        "id": 1564063,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683584339
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/2-general/topic/Zulip.207.2E0.20upgrades/near/1564042\">said</a>:</p>\n<blockquote>\n<p>Yeah, we do want to get to the bottom of this, but we've been distracted wit a few other issues today.  I may have more things to try tomorrow.</p>\n</blockquote>\n<p>If I can help, let me know. I can always do a snapshot, try anything, and rollback if anything goes bad. My timezone is EDT and I'm working during the day, might not react instantly.</p>",
        "id": 1564064,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683584569
    },
    {
        "content": "<p>Hi! Did you find time to investigate my failed attempt to upgrade to 7.0beta1?</p>",
        "id": 1567936,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683894343
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"26472\">@Louis-Marie Gendron</span>: It's a stumper!</p>\n<p>Let's see -- what's the history of this instance?  You installed it at 6.1, and imported RocketChat data into it -- so it's never been upgraded before?</p>\n<p>Can you try showing the output of:</p>\n<div class=\"codehilite\"><pre><span></span><code>su - zulip &#39;supervisorctl status&#39;\n</code></pre></div>",
        "id": 1567997,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683901063
    },
    {
        "content": "<p>And is this an <em>unprivileged</em> container?</p>",
        "id": 1567998,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683901333
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/2-general/topic/Zulip.207.2E0.20upgrades/near/1567997\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"26472\">Louis-Marie Gendron</span>: It's a stumper!</p>\n<p>Let's see -- what's the history of this instance?  You installed it at 6.1, and imported RocketChat data into it -- so it's never been upgraded before?</p>\n<p>Can you try showing the output of:</p>\n<p><div class=\"codehilite\"><pre><span></span><code>su - zulip &#39;supervisorctl status&#39;\n</code></pre></div><br>\n</p>\n</blockquote>\n<p>Exactly. First install ever was 6.1 and imported RocketChat. Never been upgraded.</p>",
        "id": 1568038,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683907099
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/2-general/topic/Zulip.207.2E0.20upgrades/near/1567998\">said</a>:</p>\n<blockquote>\n<p>And is this an <em>unprivileged</em> container?</p>\n</blockquote>\n<p>Yes, unprivileged.</p>\n<p><a href=\"/user_uploads/2/27/2YSQXg_XQI9JpEIt2lZcNs7c/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/27/2YSQXg_XQI9JpEIt2lZcNs7c/image.png\" title=\"image.png\"><img src=\"/user_uploads/2/27/2YSQXg_XQI9JpEIt2lZcNs7c/image.png\"></a></div>",
        "id": 1568041,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683907316
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/2-general/topic/Zulip.207.2E0.20upgrades/near/1567997\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"26472\">Louis-Marie Gendron</span>: It's a stumper!</p>\n<p>Let's see -- what's the history of this instance?  You installed it at 6.1, and imported RocketChat data into it -- so it's never been upgraded before?</p>\n<p>Can you try showing the output of:</p>\n<p><div class=\"codehilite\"><pre><span></span><code>su - zulip &#39;supervisorctl status&#39;\n</code></pre></div><br>\n</p>\n</blockquote>\n<p>Are you sure about the command? without single quote :</p>\n<div class=\"codehilite\"><pre><span></span><code>root@ct-zulip:~# su - zulip supervisorctl status\n/usr/bin/supervisorctl: line 3: import: command not found\n/usr/bin/supervisorctl: line 4: import: command not found\n/usr/bin/supervisorctl: line 7: __requires__: command not found\n/usr/bin/supervisorctl: line 9: try:: command not found\n/usr/bin/supervisorctl: line 10: from: command not found\n/usr/bin/supervisorctl: line 11: except: command not found\n/usr/bin/supervisorctl: line 12: try:: command not found\n/usr/bin/supervisorctl: line 13: from: command not found\n/usr/bin/supervisorctl: line 14: except: command not found\n/usr/bin/supervisorctl: line 15: from: command not found\n/usr/bin/supervisorctl: supervisorctl: line 18: syntax error near unexpected token `(&#39;\n/usr/bin/supervisorctl: supervisorctl: line 18: `def importlib_load_entry_point(spec, group, name):&#39;\n</code></pre></div>",
        "id": 1568043,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683907425
    },
    {
        "content": "<p>with single quote:</p>\n<div class=\"codehilite\"><pre><span></span><code>root@ct-zulip:~# su - zulip &#39;supervisorctl status&#39;\n-bash: supervisorctl status: No such file or directory\n</code></pre></div>",
        "id": 1568044,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683907491
    },
    {
        "content": "<p>Ah, sorry:</p>\n<div class=\"codehilite\"><pre><span></span><code>su - zulip bash -c &#39;supervisorctl status&#39;\n</code></pre></div>",
        "id": 1568045,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683907498
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/2-general/topic/Zulip.207.2E0.20upgrades/near/1568045\">said</a>:</p>\n<blockquote>\n<p>Ah, sorry:</p>\n<div class=\"codehilite\"><pre><span></span><code>su - zulip bash -c &#39;supervisorctl status&#39;\n````\n</code></pre></div>\n<p>root@ct-zulip:~# su - zulip bash -c 'supervisorctl status'<br>\ngo-camo                                                         RUNNING   pid 603, uptime 23:20:36<br>\nprocess-fts-updates                                             RUNNING   pid 609, uptime 23:20:36<br>\nsmokescreen                                                     RUNNING   pid 604, uptime 23:20:36<br>\nzulip-django                                                    RUNNING   pid 605, uptime 23:20:36<br>\nzulip-tornado                                                   RUNNING   pid 606, uptime 23:20:36<br>\nzulip-workers:zulip_events_deferred_work                        RUNNING   pid 610, uptime 23:20:36<br>\nzulip-workers:zulip_events_digest_emails                        RUNNING   pid 611, uptime 23:20:36<br>\nzulip-workers:zulip_events_email_mirror                         RUNNING   pid 612, uptime 23:20:36<br>\nzulip-workers:zulip_events_email_senders                        RUNNING   pid 617, uptime 23:20:36<br>\nzulip-workers:zulip_events_embed_links                          RUNNING   pid 613, uptime 23:20:36<br>\nzulip-workers:zulip_events_embedded_bots                        RUNNING   pid 614, uptime 23:20:36<br>\nzulip-workers:zulip_events_error_reports                        RUNNING   pid 615, uptime 23:20:36<br>\nzulip-workers:zulip_events_invites                              RUNNING   pid 616, uptime 23:20:36<br>\nzulip-workers:zulip_events_missedmessage_emails                 RUNNING   pid 618, uptime 23:20:36<br>\nzulip-workers:zulip_events_missedmessage_mobile_notifications   RUNNING   pid 619, uptime 23:20:36<br>\nzulip-workers:zulip_events_outgoing_webhooks                    RUNNING   pid 620, uptime 23:20:36<br>\nzulip-workers:zulip_events_user_activity                        RUNNING   pid 621, uptime 23:20:36<br>\nzulip-workers:zulip_events_user_activity_interval               RUNNING   pid 622, uptime 23:20:36<br>\nzulip-workers:zulip_events_user_presence                        RUNNING   pid 623, uptime 23:20:36<br>\nzulip_deliver_scheduled_emails                                  RUNNING   pid 607, uptime 23:20:36<br>\nzulip_deliver_scheduled_messages                                RUNNING   pid 608, uptime 23:20:36</p>\n<p><div class=\"codehilite\"><pre><span></span><code>\n</code></pre></div><br>\n</p>\n</blockquote>",
        "id": 1568046,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683907540
    },
    {
        "content": "<p>Hm.  And this fails?</p>\n<div class=\"codehilite\"><pre><span></span><code>sudo -i -u zulip\ncd ~/deployments/current\n./scripts/restart-server\n</code></pre></div>\n<p>(given your above report, I expect this will fail immediately and do nothing -- but if it works, it will result in a very short (seconds) interruption of service of Zulip)</p>",
        "id": 1568047,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683907698
    },
    {
        "content": "<p>sudo is not installed on this LXC Debian container... the default user is root in this container.</p>",
        "id": 1568048,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683907786
    },
    {
        "content": "<p>OK, so:</p>\n<div class=\"codehilite\"><pre><span></span><code>su - zulip\ncd ~/deployments/current\n./scripts/restart-server\n</code></pre></div>",
        "id": 1568049,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683907815
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>zulip@ct-zulip:~/deployments/current$ ./scripts/restart-server\n2023-05-12 16:11:43,047 restart-server: Running syntax and database checks\nSystem check identified no issues (15 silenced).\n2023-05-12 16:11:44,334 restart-server: Restarting zulip-workers:zulip_events_deferred_work\nzulip-workers:zulip_events_deferred_work: stopped\nzulip-workers:zulip_events_deferred_work: started\n2023-05-12 16:11:45,689 restart-server: Restarting zulip-workers:zulip_events_digest_emails\nzulip-workers:zulip_events_digest_emails: stopped\nzulip-workers:zulip_events_digest_emails: started\n2023-05-12 16:11:47,045 restart-server: Restarting zulip-workers:zulip_events_email_mirror\nzulip-workers:zulip_events_email_mirror: stopped\nzulip-workers:zulip_events_email_mirror: started\n2023-05-12 16:11:48,408 restart-server: Restarting zulip-workers:zulip_events_email_senders\nzulip-workers:zulip_events_email_senders: stopped\nzulip-workers:zulip_events_email_senders: started\n2023-05-12 16:11:49,770 restart-server: Restarting zulip-workers:zulip_events_embed_links\nzulip-workers:zulip_events_embed_links: stopped\nzulip-workers:zulip_events_embed_links: started\n2023-05-12 16:11:51,134 restart-server: Restarting zulip-workers:zulip_events_embedded_bots\nzulip-workers:zulip_events_embedded_bots: stopped\nzulip-workers:zulip_events_embedded_bots: started\n2023-05-12 16:11:52,503 restart-server: Restarting zulip-workers:zulip_events_error_reports\nzulip-workers:zulip_events_error_reports: stopped\nzulip-workers:zulip_events_error_reports: started\n2023-05-12 16:11:53,871 restart-server: Restarting zulip-workers:zulip_events_invites\nzulip-workers:zulip_events_invites: stopped\nzulip-workers:zulip_events_invites: started\n2023-05-12 16:11:55,244 restart-server: Restarting zulip-workers:zulip_events_missedmessage_emails\nzulip-workers:zulip_events_missedmessage_emails: stopped\nzulip-workers:zulip_events_missedmessage_emails: started\n2023-05-12 16:11:56,631 restart-server: Restarting zulip-workers:zulip_events_missedmessage_mobile_notifications\nzulip-workers:zulip_events_missedmessage_mobile_notifications: stopped\nzulip-workers:zulip_events_missedmessage_mobile_notifications: started\n2023-05-12 16:11:58,015 restart-server: Restarting zulip-workers:zulip_events_outgoing_webhooks\nzulip-workers:zulip_events_outgoing_webhooks: stopped\nzulip-workers:zulip_events_outgoing_webhooks: started\n2023-05-12 16:11:59,386 restart-server: Restarting zulip-workers:zulip_events_user_activity\nzulip-workers:zulip_events_user_activity: stopped\nzulip-workers:zulip_events_user_activity: started\n2023-05-12 16:12:00,755 restart-server: Restarting zulip-workers:zulip_events_user_activity_interval\nzulip-workers:zulip_events_user_activity_interval: stopped\nzulip-workers:zulip_events_user_activity_interval: started\n2023-05-12 16:12:02,130 restart-server: Restarting zulip-workers:zulip_events_user_presence\nzulip-workers:zulip_events_user_presence: stopped\nzulip-workers:zulip_events_user_presence: started\n2023-05-12 16:12:03,489 restart-server: Restarting zulip_deliver_scheduled_emails\nzulip_deliver_scheduled_emails: stopped\nzulip_deliver_scheduled_emails: started\n2023-05-12 16:12:05,581 restart-server: Restarting zulip_deliver_scheduled_messages\nzulip_deliver_scheduled_messages: stopped\nzulip_deliver_scheduled_messages: started\n2023-05-12 16:12:06,707 restart-server: Restarting process-fts-updates\nprocess-fts-updates: stopped\nprocess-fts-updates: started\n2023-05-12 16:12:08,005 restart-server: Restarting Tornado process\nzulip-tornado: stopped\nzulip-tornado: started\n2023-05-12 16:12:09,370 restart-server: Restarting django server\nzulip-django: stopped\nzulip-django: started\n2023-05-12 16:12:11,792 restart-server: Done!\nZulip restarted successfully!\n</code></pre></div>",
        "id": 1568054,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683907961
    },
    {
        "content": "<p>Oho!  That's fascinating.</p>",
        "id": 1568056,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683908014
    },
    {
        "content": "<p>The story of my life!</p>",
        "id": 1568057,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683908043
    },
    {
        "content": "<p>So this has to do with how we're attempting to drop privileges to the zulip user, from root, in the upgrade script.  The uid-mapping shenanigans of unprivileged containers are at fault here somehow.</p>",
        "id": 1568058,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683908092
    },
    {
        "content": "<p>I saw beta2 is out,  should I try?</p>",
        "id": 1568059,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683908141
    },
    {
        "content": "<p>There's nothing relevant to this that's changed in beta2.  You can give it a try, but I fully expect you'll see the exact same message.</p>\n<p>The way we drop privileges has certainly not changed recently (or in years), so this isn't a regression (and thus probably not a release blocker).  But we're certainly still interested in solving it, to be clear!</p>\n<p>Let me get you some python to try running.</p>",
        "id": 1568060,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683908199
    },
    {
        "content": "<p>Ok! It's lunch time here... I'll be back in a few minutes.</p>",
        "id": 1568061,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683908264
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"2\" href=\"/#narrow/stream/2-general/topic/Zulip.207.2E0.20upgrades\">#general &gt; Zulip 7.0 upgrades</a> by <span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span>.</p>",
        "id": 1568065,
        "sender_full_name": "Notification Bot",
        "timestamp": 1683908603
    },
    {
        "content": "<p>OK, try, as root, running <code>python3</code> and then pasting in:</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pwd</span>\n\n<span class=\"n\">uid</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">stat</span><span class=\"p\">(</span><span class=\"s2\">\"/home/zulip/deployments/current\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">st_uid</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">uid</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">pwd</span><span class=\"o\">.</span><span class=\"n\">getpwuid</span><span class=\"p\">(</span><span class=\"n\">uid</span><span class=\"p\">))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">pwd</span><span class=\"o\">.</span><span class=\"n\">getpwnam</span><span class=\"p\">(</span><span class=\"s2\">\"zulip\"</span><span class=\"p\">))</span>\n\n<span class=\"n\">pwent</span> <span class=\"o\">=</span> <span class=\"n\">pwd</span><span class=\"o\">.</span><span class=\"n\">getpwuid</span><span class=\"p\">(</span><span class=\"n\">uid</span><span class=\"p\">)</span>\n<span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">setgid</span><span class=\"p\">(</span><span class=\"n\">pwent</span><span class=\"o\">.</span><span class=\"n\">pw_gid</span><span class=\"p\">)</span>\n<span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">setuid</span><span class=\"p\">(</span><span class=\"n\">pwent</span><span class=\"o\">.</span><span class=\"n\">pw_uid</span><span class=\"p\">)</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">socket</span>\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">socket</span><span class=\"o\">.</span><span class=\"n\">socket</span><span class=\"p\">(</span><span class=\"n\">socket</span><span class=\"o\">.</span><span class=\"n\">AF_UNIX</span><span class=\"p\">,</span> <span class=\"n\">socket</span><span class=\"o\">.</span><span class=\"n\">SOCK_STREAM</span><span class=\"p\">)</span>\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span><span class=\"s2\">\"/var/run/supervisor.sock\"</span><span class=\"p\">)</span>\n</code></pre></div>",
        "id": 1568069,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683908697
    },
    {
        "content": "<p>os.setgid(<strong>pewnt</strong>.pw_gid) ? a typo?</p>",
        "id": 1568083,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683909486
    },
    {
        "content": "<p>Yup, sorry! (fixed above)</p>",
        "id": 1568084,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683909542
    },
    {
        "content": "<p><a href=\"/user_uploads/2/ae/4CkVu9G8xNjMeq9trNBFXx7I/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/ae/4CkVu9G8xNjMeq9trNBFXx7I/image.png\" title=\"image.png\"><img src=\"/user_uploads/2/ae/4CkVu9G8xNjMeq9trNBFXx7I/image.png\"></a></div>",
        "id": 1568088,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683909616
    },
    {
        "content": "<p>...huh.  That all looks fine.</p>",
        "id": 1568089,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683909634
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/Upgrades.20in.20unprivileged.20LXC.20container/near/1568084\">said</a>:</p>\n<blockquote>\n<p>Yup, sorry! (fixed above)</p>\n</blockquote>\n<p>wasn't sure if it was intentional.. ;)</p>",
        "id": 1568090,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683909656
    },
    {
        "content": "<p>Either I'm doing something wrong, or that's exactly what was failing before, and isn't now.</p>",
        "id": 1568092,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683909693
    },
    {
        "content": "<p>I guess try upgrading to beta2, and we'll see if it fails in the same way?</p>",
        "id": 1568094,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683909715
    },
    {
        "content": "<p>Ok. I have still time before my first meeting in the afternoon...</p>",
        "id": 1568095,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683909766
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>Reading package lists...\nBuilding dependency tree...\nReading state information...\nCalculating upgrade...\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n2023-05-12 16:50:26,938 upgrade-zulip-stage-2: Stopping Zulip...\nTraceback (most recent call last):\n  File &quot;/home/zulip/deployments/2023-05-12-12-47-56/./scripts/stop-server&quot;, line 53, in &lt;module&gt;\n    services = list_supervisor_processes(services, only_running=True)\n  File &quot;/home/zulip/deployments/2023-05-12-12-47-56/./scripts/../scripts/lib/supervisor.py&quot;, line 34, in list_supervisor_processes\n    processes = rpc().supervisor.getAllProcessInfo()\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1116, in __call__\n    return self.__send(self.__name, args)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1456, in __request\n    response = self.__transport.request(\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1160, in request\n    return self.single_request(host, handler, request_body, verbose)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1172, in single_request\n    http_conn = self.send_request(host, handler, request_body, verbose)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1285, in send_request\n    self.send_content(connection, request_body)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1315, in send_content\n    connection.endheaders(request_body)\n  File &quot;/usr/lib/python3.9/http/client.py&quot;, line 1250, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File &quot;/usr/lib/python3.9/http/client.py&quot;, line 1010, in _send_output\n    self.send(msg)\n  File &quot;/usr/lib/python3.9/http/client.py&quot;, line 950, in send\n    self.connect()\n  File &quot;/home/zulip/deployments/2023-05-12-12-47-56/./scripts/../scripts/lib/supervisor.py&quot;, line 10, in connect\n    self.sock.connect(self.host)\nFileNotFoundError: [Errno 2] No such file or directory\nTraceback (most recent call last):\n  File &quot;/home/zulip/deployments/2023-05-12-12-47-56/scripts/lib/upgrade-zulip-stage-2&quot;, line 528, in &lt;module&gt;\n    shutdown_server()\n  File &quot;/home/zulip/deployments/2023-05-12-12-47-56/scripts/lib/upgrade-zulip-stage-2&quot;, line 202, in shutdown_server\n    subprocess.check_call([&quot;./scripts/stop-server&quot;], preexec_fn=su_to_zulip)\n  File &quot;/usr/lib/python3.9/subprocess.py&quot;, line 373, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command &#39;[&#39;./scripts/stop-server&#39;]&#39; returned non-zero exit status 1.\n\nZulip upgrade failed (exit code 1)!\n\nThe upgrade process is designed to be idempotent, so you can retry after resolving whatever issue caused the failure (there should be a traceback above). A log of this installation is available in /var/log/zulip/upgrade.log\n</code></pre></div>",
        "id": 1568099,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683910295
    },
    {
        "content": "<p>I was under the impression that I was further in the upgrade process, but it failed</p>",
        "id": 1568101,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683910377
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>Notice: Applied catalog in 0.11 seconds\n2023-05-12 16:49:03,855 upgrade-zulip-stage-2: Pre-checking for puppet changes...\n2023-05-12 16:49:06,735 upgrade-zulip-stage-2: Stopping Zulip...\n\nZulip stopped successfully!\n2023-05-12 16:49:06,956 upgrade-zulip-stage-2: Applying Puppet changes...\n</code></pre></div>",
        "id": 1568102,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683910406
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>Notice: /Stage[main]/Zulip::Camo/File[/etc/supervisor/conf.d/zulip/go-camo.conf]/content: content changed &#39;{md5}17eb93344d6ff117c06d25eaa1f18aba&#39; to &#39;{md5}ab8cff1263fb1ada6fef43b945cd7d9d&#39;\nNotice: /Stage[main]/Zulip::Supervisor/Service[supervisor]: Triggered &#39;refresh&#39; from 3 events\nNotice: /Stage[main]/Zulip::Supervisor/Exec[supervisor-restart]: Triggered &#39;refresh&#39; from 1 event\nNotice: Applied catalog in 76.93 seconds\nReading package lists...\nBuilding dependency tree...\nReading state information...\nCalculating upgrade...\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n2023-05-12 16:50:26,938 upgrade-zulip-stage-2: Stopping Zulip...\nTraceback (most recent call last):\n</code></pre></div>",
        "id": 1568103,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683910468
    },
    {
        "content": "<p>why is it trying to stop zulip server at this step? it is fully stopped. and the stop-server script in current is working, but at this step it's using this version : /home/zulip/deployments/2023-05-12-12-47-56/./scripts/stop-server</p>",
        "id": 1568106,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683910703
    },
    {
        "content": "<p>I'm sorry, I'm a little confused with the multiple pastes in a row.  Can you upload <code>upgrade.log</code> so I can see everything next o each other?</p>",
        "id": 1568109,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683910794
    },
    {
        "content": "<p><a href=\"/user_uploads/2/c8/DwF8DBnttW8b6aOw84Tr5FuZ/upgrade.log\">upgrade.log</a></p>",
        "id": 1568111,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683910918
    },
    {
        "content": "<p>I'm rolling back my snapshot.</p>",
        "id": 1568112,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683910943
    },
    {
        "content": "<p>If you need me to try something. Just list the steps here and I will try later. Thanks for your help.</p>",
        "id": 1568113,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683911001
    },
    {
        "content": "<p>now back on a working 6.1 server.</p>",
        "id": 1568117,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683911174
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>2023-05-12 16:49:06,735 upgrade-zulip-stage-2: Stopping Zulip...\n\nZulip stopped successfully!\n</code></pre></div>\n<p>That's real weird, since I'd have expected (a) that to have the same failure mode as below, and (b) it show have some lines of output in between, for each of the services it's stopping.</p>\n<p>I suppose one theory is that at the end of puppet:</p>\n<div class=\"codehilite\"><pre><span></span><code>Notice: /Stage[main]/Zulip::Supervisor/Service[supervisor]: Triggered &#39;refresh&#39; from 3 events\nNotice: /Stage[main]/Zulip::Supervisor/Exec[supervisor-restart]: Triggered &#39;refresh&#39; from 1 event\nNotice: Applied catalog in 76.93 seconds\nReading package lists...\nBuilding dependency tree...\nReading state information...\nCalculating upgrade...\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n2023-05-12 16:50:26,938 upgrade-zulip-stage-2: Stopping Zulip...\nTraceback (most recent call last):\n</code></pre></div>\n<p>we're just restarting supervisor for config changes and it's not up yet?  It being a race would explain the inability to replicate, and the LXC stuff is all a red herring.</p>",
        "id": 1568122,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683911476
    },
    {
        "content": "<p>And that restart is why we need to stop everything again.  We need to restart <code>supervisord</code> to pick up the config change, but when it comes back up it automatically starts back up all of its services -- so we need to stop them again. (<a href=\"https://github.com/zulip/zulip/commit/0af00a3233fc8964b1ba7c32e672b74d80889b89\">0af00a3233fc8964b1ba7c32e672b74d80889b89</a>)</p>",
        "id": 1568125,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683911633
    },
    {
        "content": "<p>One way to test this race condition theory is to run the upgrade twice, back to back.  The second time, puppet won't need to apply any changes, so it won't kick supervisor, and we can see if it is able to carry on.</p>",
        "id": 1568130,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683911842
    },
    {
        "content": "<p>And the fix would be to have shutdown_server wait for the socket to be available.</p>",
        "id": 1568134,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683911936
    },
    {
        "content": "<p>Yeah, OK, this replicates:</p>\n<div class=\"codehilite\"><pre><span></span><code>root@alexmv-prod:~# service supervisor stop\nroot@alexmv-prod:~# python3\nPython 3.8.10 (default, Mar 13 2023, 10:26:41)\n[GCC 9.4.0] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import socket\n&gt;&gt;&gt; client = socket.socket( socket.AF_UNIX, socket.SOCK_STREAM )\n&gt;&gt;&gt; client.connect(&quot;/var/run/supervisor.sock&quot;)\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nFileNotFoundError: [Errno 2] No such file or directory\n</code></pre></div>",
        "id": 1568135,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683911943
    },
    {
        "content": "<p>Pushed <a href=\"https://github.com/zulip/zulip/pull/25571\">#25571</a>.</p>",
        "id": 1568195,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683915959
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"26472\">@Louis-Marie Gendron</span>: OK, this fix is now in <code>main</code>.  If you want to test it, you can <a href=\"https://zulip.readthedocs.io/en/latest/production/upgrade.html#upgrading-from-a-git-repository\">upgrade to the latest <code>main</code></a>:</p>\n<div class=\"codehilite\"><pre><span></span><code>/home/zulip/deployments/current/scripts/upgrade-zulip-from-git main\n</code></pre></div>\n<p>That should fix your issue, and you can later upgrade from there to any very small set of changes between that and the official 7.0, expected in a week or so.</p>",
        "id": 1568316,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683923239
    },
    {
        "content": "<p>Thank you for reporting this bug and helping us track it down!</p>",
        "id": 1568317,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683923264
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/Upgrades.20in.20unprivileged.20LXC.20container/near/1568316\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"26472\">Louis-Marie Gendron</span>: OK, this fix is now in <code>main</code>.  If you want to test it, you can <a href=\"https://zulip.readthedocs.io/en/latest/production/upgrade.html#upgrading-from-a-git-repository\">upgrade to the latest <code>main</code></a>:</p>\n<div class=\"codehilite\"><pre><span></span><code>/home/zulip/deployments/current/scripts/upgrade-zulip-from-git main\n</code></pre></div>\n<p>That should fix your issue, and you can later upgrade from there to any very small set of changes between that and the official 7.0, expected in a week or so.</p>\n</blockquote>\n<p>I tried just now to upgrade from git main, sadly it failed again.</p>",
        "id": 1568431,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683928819
    },
    {
        "content": "<p>I'm trying a second time in a row to test your other theory.</p>",
        "id": 1568432,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683928898
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>Done in 2.8s\n+ ./tools/setup/emoji/build_emoji\nbuild_emoji: Using cached emojis from /srv/zulip-emoji-cache/d366bfece8df5a9a5683ca99386d943ae2b86097/emoji\n+ ./tools/setup/generate_zulip_bots_static_files.py\n+ ./tools/setup/build_pygments_data\n+ ./tools/setup/build_timezone_values\n+ ./tools/webpack --quiet\n+ ./manage.py collectstatic -v0 --noinput\n+ ./manage.py compilemessages -v0 &#39;--ignore=*&#39;\nSystem check identified no issues (18 silenced).\n2023-05-12 22:01:25,580 upgrade-zulip-stage-2: Checking for needed migrations\n2023-05-12 22:01:26,244 upgrade-zulip-stage-2: Installing hooks\nNotice: Compiled catalog for ct-zulip.desgends.com in environment production in 1.58 seconds\nNotice: Applied catalog in 0.11 seconds\n2023-05-12 22:01:29,001 upgrade-zulip-stage-2: Pre-checking for puppet changes...\n2023-05-12 22:01:31,923 upgrade-zulip-stage-2: Stopping Zulip...\nTraceback (most recent call last):\n  File &quot;/home/zulip/deployments/2023-05-12-18-00-38/./scripts/stop-server&quot;, line 53, in &lt;module&gt;\n    services = list_supervisor_processes(services, only_running=True)\n  File &quot;/home/zulip/deployments/2023-05-12-18-00-38/./scripts/../scripts/lib/supervisor.py&quot;, line 47, in list_supervisor_processes\n    processes = rpc().supervisor.getAllProcessInfo()\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1116, in __call__\n    return self.__send(self.__name, args)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1456, in __request\n    response = self.__transport.request(\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1160, in request\n    return self.single_request(host, handler, request_body, verbose)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1172, in single_request\n    http_conn = self.send_request(host, handler, request_body, verbose)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1285, in send_request\n    self.send_content(connection, request_body)\n  File &quot;/usr/lib/python3.9/xmlrpc/client.py&quot;, line 1315, in send_content\n    connection.endheaders(request_body)\n  File &quot;/usr/lib/python3.9/http/client.py&quot;, line 1250, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File &quot;/usr/lib/python3.9/http/client.py&quot;, line 1010, in _send_output\n    self.send(msg)\n  File &quot;/usr/lib/python3.9/http/client.py&quot;, line 950, in send\n    self.connect()\n  File &quot;/home/zulip/deployments/2023-05-12-18-00-38/./scripts/../scripts/lib/supervisor.py&quot;, line 21, in connect\n    raise Exception(\nException: Failed to connect to supervisor -- check that it is running, by running &#39;service supervisor status&#39;\nTraceback (most recent call last):\n  File &quot;/home/zulip/deployments/2023-05-12-18-00-38/scripts/lib/upgrade-zulip-stage-2&quot;, line 517, in &lt;module&gt;\n    shutdown_server()\n  File &quot;/home/zulip/deployments/2023-05-12-18-00-38/scripts/lib/upgrade-zulip-stage-2&quot;, line 202, in shutdown_server\n    subprocess.check_call([&quot;./scripts/stop-server&quot;], preexec_fn=su_to_zulip)\n  File &quot;/usr/lib/python3.9/subprocess.py&quot;, line 373, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command &#39;[&#39;./scripts/stop-server&#39;]&#39; returned non-zero exit status 1.\n\nZulip upgrade failed (exit code 1)!\n\nThe upgrade process is designed to be idempotent, so you can retry after resolving whatever issue caused the failure (there should be a traceback above). A log of this installation is available in /var/log/zulip/upgrade.log\n</code></pre></div>",
        "id": 1568433,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683928933
    },
    {
        "content": "<p><a href=\"/user_uploads/2/f4/6UFtjQu1WTzg2WZ9oYiaV6rQ/upgrade_git_main.log\">upgrade_git_main.log</a></p>",
        "id": 1568441,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683929054
    },
    {
        "content": "<p>my two tries are in the log file.</p>",
        "id": 1568443,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1683929107
    },
    {
        "content": "<p>Grrr.  That theory seemed so promising!</p>\n<p>I'll take a look at the log files on Monday - thanks for providing those.</p>",
        "id": 1568451,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1683929770
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"26472\">@Louis-Marie Gendron</span>: Yeah, I'm pretty stumped.  I don't have a theory for this part of the output, which should have more lines between the first two non-empty lines:</p>\n<div class=\"codehilite\"><pre><span></span><code>2023-05-12 21:57:22,867 upgrade-zulip-stage-2: Stopping Zulip...\n\nZulip stopped successfully!\n2023-05-12 21:57:23,092 upgrade-zulip-stage-2: Applying Puppet changes...\n</code></pre></div>\n<p>And if this isn't a race, that means that supervisor is just failing to come up cleanly at the end of the puppet run.</p>\n<p>If you're game to try another go at it, can you (after it fails) show the output of:</p>\n<div class=\"codehilite\"><pre><span></span><code>tail -n100 /var/log/supervisor/supervisord.log\n\nservice supervisor status\n</code></pre></div>",
        "id": 1570468,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684267209
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/Upgrades.20in.20unprivileged.20LXC.20container/near/1570468\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"26472\">Louis-Marie Gendron</span>: Yeah, I'm pretty stumped.  I don't have a theory for this part of the output, which should have more lines between the first two non-empty lines:</p>\n<div class=\"codehilite\"><pre><span></span><code>2023-05-12 21:57:22,867 upgrade-zulip-stage-2: Stopping Zulip...\n\nZulip stopped successfully!\n2023-05-12 21:57:23,092 upgrade-zulip-stage-2: Applying Puppet changes...\n</code></pre></div>\n<p>And if this isn't a race, that means that supervisor is just failing to come up cleanly at the end of the puppet run.</p>\n<p>If you're game to try another go at it, can you (after it fails) show the output of:</p>\n<p><div class=\"codehilite\"><pre><span></span><code>tail -n100 /var/log/supervisor/supervisord.log\n\nservice supervisor status\n</code></pre></div><br>\n</p>\n</blockquote>\n<p><a href=\"/user_uploads/2/5a/qeQQiHh3jUjfYWi5FWl-Bw0z/upgrade_git_main_2.log\">upgrade_git_main_2.log</a><br>\n<a href=\"/user_uploads/2/26/W2_1aau9LvPb_Kli_Xfp2Q0v/tail_n100_supervisord_right_after_fail.log\">tail_n100_supervisord_right_after_fail.log</a></p>",
        "id": 1570842,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684289405
    },
    {
        "content": "<p>It failed again, no surprise. After a quick snapshot rollback, Zulip Server 6.1 is running again.</p>\n<p><a href=\"/user_uploads/2/2b/ka_8N5rLOHJQQ3M8WeH7MWji/healthy-zulip-6.1-service-supervisor-status.log\">healthy-zulip-6.1-service-supervisor-status.log</a></p>",
        "id": 1570844,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684289654
    },
    {
        "content": "<p>According to that <code>supervisord.log</code>, all of the supervisor processes (including ones not normally stopped by <code>./scripts/stop-server</code>) were stopped at <code>2023-05-16 21:56:26,378</code> through <code>2023-05-16 22:01:33,527</code>, several hours before you started the upgrade at <code>2023-05-17 01:58:43,619</code>.  Does that ring a bell at all?</p>",
        "id": 1570850,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684293504
    },
    {
        "content": "<p>It also shows that indeed supervisord is not running after the upgrade, but frustratingly doesn't show anything in the logs about why.</p>",
        "id": 1570851,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684293675
    },
    {
        "content": "<p>Oh, I wonder if the ulimit max-socket changes are maybe relevant?</p>",
        "id": 1570852,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684293696
    },
    {
        "content": "<p>Looking at the <code>supervisorctl</code> logs more closely, the ordering and timing looks like a <code>./scripts/stop-server</code> at <code>2023-05-16 21:56:26,381</code> followed by a <code>supervisorctl stop all</code> at <code>2023-05-16 22:01:21,105</code> for the remaining services.</p>",
        "id": 1570853,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684293846
    },
    {
        "content": "<p>It's my fault! Before taking a snapshot, I did execute /home/zulip/development/current/scripts/stop-server. I want to preserve integrity of the database. It's much easier to rollback to an healthy installation, if Zulip was doing nothing. And I dont want any users to post message while I try to mess with my installation.</p>",
        "id": 1570881,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684296271
    },
    {
        "content": "<p>I could retry, but this time :</p>\n<ol>\n<li>stop Zulip</li>\n<li>take a snapshot of the container</li>\n<li>block client access to Zulip container</li>\n<li>start Zulip</li>\n<li>try to upgrade again and then check supervisord.log</li>\n</ol>",
        "id": 1570885,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684296435
    },
    {
        "content": "<p>No, that's fine!</p>",
        "id": 1570886,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684296442
    },
    {
        "content": "<p>It was just an oddity I was trying to understand -- it's totally fine to have the system quiesced before running the upgrade.  I think we've pinned this down to \"supervisor doesn't come back up after puppet is run\"</p>",
        "id": 1570887,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684296507
    },
    {
        "content": "<p>Did you also do a <code>supervisorctl stop all</code> at <code>2023-05-16 22:01:21,105</code>?</p>",
        "id": 1570888,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684296578
    },
    {
        "content": "<p>No I did not execute that specific command... not knowingly</p>",
        "id": 1570890,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684296674
    },
    {
        "content": "<p>Hm, OK.  Something stopped the rest of the services then.  I think there must be timezone shenanigans here, for the two logs to line up</p>",
        "id": 1570891,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684296752
    },
    {
        "content": "<p>Yeah, <code>2023-05-16 22:01:21,105</code> lines up with when the puppet run happened, if there's a <code>-0400</code> time zone offset to the upgrade running puppet at around <code>2023-05-17 02:01:04,937</code> and taking 77 seconds.</p>",
        "id": 1570892,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684296852
    },
    {
        "content": "<p>upgrade.log is not  GMT-4, but supervisorctl is.</p>",
        "id": 1570893,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684296854
    },
    {
        "content": "<p>OK, so <code>supervisord</code> isn't starting cleanly after the puppet apply.  If you're down for <em>one more outage</em>, trying to use <code>journalctl</code> and/or <code>/usr/bin/supervisord -n -c /etc/supervisor/supervisord.conf</code> to determine why it's not coming up cleanly, after the failed upgrade runs puppet, should point us toward what we need.</p>",
        "id": 1570894,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684296934
    },
    {
        "content": "<p>Before that, actually, can you show <code>ulimit -a</code>?</p>",
        "id": 1570895,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684296945
    },
    {
        "content": "<p><code>ulimit -a</code> without any other actions? like live right now?</p>",
        "id": 1570896,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684296997
    },
    {
        "content": "<p>Yeah, inside the container.  That just shows limits, it doesn't change anything.</p>",
        "id": 1570897,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684297013
    },
    {
        "content": "<p>root@ct-zulip:~# ulimit -a<br>\nreal-time non-blocking time  (microseconds, -R) unlimited<br>\ncore file size              (blocks, -c) 0<br>\ndata seg size               (kbytes, -d) unlimited<br>\nscheduling priority                 (-e) 0<br>\nfile size                   (blocks, -f) unlimited<br>\npending signals                     (-i) 243873<br>\nmax locked memory           (kbytes, -l) 64<br>\nmax memory size             (kbytes, -m) unlimited<br>\nopen files                          (-n) 40000<br>\npipe size                (512 bytes, -p) 8<br>\nPOSIX message queues         (bytes, -q) 819200<br>\nreal-time priority                  (-r) 0<br>\nstack size                  (kbytes, -s) 8192<br>\ncpu time                   (seconds, -t) unlimited<br>\nmax user processes                  (-u) 243873<br>\nvirtual memory              (kbytes, -v) unlimited<br>\nfile locks                          (-x) unlimited</p>",
        "id": 1570898,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684297034
    },
    {
        "content": "<ol>\n<li>stop Zulip</li>\n<li>snapshot</li>\n<li>upgrade attempt (git main)</li>\n<li>when it fails : <code>journalctl</code> and  <code>/usr/bin/supervisord -n -c /etc/supervisor/supervisord.conf</code></li>\n</ol>",
        "id": 1570899,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684297190
    },
    {
        "content": "<p>Let me get you the right journalctl incant first.</p>",
        "id": 1570900,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684297208
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>journalctl -eu supervisor.service\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code>/usr/bin/supervisord -n -c /etc/supervisor/supervisord.conf\n</code></pre></div>",
        "id": 1570903,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684297404
    },
    {
        "content": "<p>immediatly after a failed upgrade attempt</p>",
        "id": 1570904,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684297468
    },
    {
        "content": "<p>My strong belief at this point is that something in the container is interfering with out attempt to set a very-high filedescriptor ulimit, which is unnecessary for most instances, and we can set a config to default that lower and let folks scale it up.  But I'd like to know what the limit is being forced to for you, and which limit is being the problem -- it's <em>probably</em> the <code>minfds</code> setting but it could be the <code>/etc/systemd/system.conf.d/limits.conf</code> change.</p>",
        "id": 1570906,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684297536
    },
    {
        "content": "<p>Yes.</p>",
        "id": 1570907,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684297545
    },
    {
        "content": "<p>I expect the latter will bomb out with the same logs that the former shows, but I'm including both in case I'm wrong, so we get the most information we can from the attempt.</p>",
        "id": 1570908,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684297581
    },
    {
        "content": "<p>ok. attempt in progress</p>",
        "id": 1570909,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684297604
    },
    {
        "content": "<p>If the latter starts up cleanly by some miracle, you'll need to ^C out of it, and then you can do your restore from snapshot.</p>",
        "id": 1570910,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684297620
    },
    {
        "content": "<p>Well, I guess it doesn't matter what you do or don't do before throwing everything away to restore from the snapshot. <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 1570911,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684297654
    },
    {
        "content": "<p>It seems that you are in the same timezone as I...</p>",
        "id": 1570912,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684297783
    },
    {
        "content": "<p>near point of failure</p>",
        "id": 1570914,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684297887
    },
    {
        "content": "<p>May 17 00:26:18 ct-zulip supervisord[157]: 2023-05-17 00:26:18,267 INFO waiting for process-fts-updates to stop<br>\nMay 17 00:26:18 ct-zulip supervisord[157]: 2023-05-17 00:26:18,271 INFO stopped: process-fts-updates (terminated by SIGTERM)<br>\nMay 17 00:26:18 ct-zulip supervisord[157]: 2023-05-17 00:26:18,271 INFO waiting for zulip-django to stop<br>\nMay 17 00:26:19 ct-zulip supervisord[157]: 2023-05-17 00:26:19,284 INFO stopped: zulip-django (exit status 0)<br>\nMay 17 00:26:19 ct-zulip supervisord[157]: 2023-05-17 00:26:19,285 INFO waiting for zulip-tornado to stop<br>\nMay 17 00:26:19 ct-zulip supervisord[157]: 2023-05-17 00:26:19,540 INFO stopped: zulip-tornado (exit status 0)<br>\nMay 17 00:26:19 ct-zulip supervisord[157]: 2023-05-17 00:26:19,541 INFO waiting for zulip_events_deferred_work to stop<br>\nMay 17 00:26:19 ct-zulip supervisord[157]: 2023-05-17 00:26:19,781 INFO stopped: zulip_events_deferred_work (exit status 0)<br>\nMay 17 00:26:19 ct-zulip supervisord[157]: 2023-05-17 00:26:19,782 INFO waiting for zulip_events_digest_emails to stop<br>\nMay 17 00:26:20 ct-zulip supervisord[157]: 2023-05-17 00:26:20,022 INFO stopped: zulip_events_digest_emails (exit status 0)<br>\nMay 17 00:26:20 ct-zulip supervisord[157]: 2023-05-17 00:26:20,023 INFO waiting for zulip_events_email_mirror to stop<br>\nMay 17 00:26:20 ct-zulip supervisord[157]: 2023-05-17 00:26:20,263 INFO stopped: zulip_events_email_mirror (exit status 0)<br>\nMay 17 00:26:20 ct-zulip supervisord[157]: 2023-05-17 00:26:20,264 INFO waiting for zulip_events_email_senders to stop<br>\nMay 17 00:26:20 ct-zulip supervisord[157]: 2023-05-17 00:26:20,508 INFO stopped: zulip_events_email_senders (exit status 0)<br>\nMay 17 00:26:20 ct-zulip supervisord[157]: 2023-05-17 00:26:20,509 INFO waiting for zulip_events_embed_links to stop<br>\nMay 17 00:26:20 ct-zulip supervisord[157]: 2023-05-17 00:26:20,751 INFO stopped: zulip_events_embed_links (exit status 0)<br>\nMay 17 00:26:20 ct-zulip supervisord[157]: 2023-05-17 00:26:20,752 INFO waiting for zulip_events_embedded_bots to stop<br>\nMay 17 00:26:20 ct-zulip supervisord[157]: 2023-05-17 00:26:20,993 INFO stopped: zulip_events_embedded_bots (exit status 0)<br>\nMay 17 00:26:20 ct-zulip supervisord[157]: 2023-05-17 00:26:20,994 INFO waiting for zulip_events_error_reports to stop<br>\nMay 17 00:26:21 ct-zulip supervisord[157]: 2023-05-17 00:26:21,234 INFO stopped: zulip_events_error_reports (exit status 0)<br>\nMay 17 00:26:21 ct-zulip supervisord[157]: 2023-05-17 00:26:21,236 INFO waiting for zulip_events_invites to stop<br>\nMay 17 00:26:21 ct-zulip supervisord[157]: 2023-05-17 00:26:21,477 INFO stopped: zulip_events_invites (exit status 0)<br>\nMay 17 00:26:21 ct-zulip supervisord[157]: 2023-05-17 00:26:21,478 INFO waiting for zulip_events_missedmessage_emails to stop<br>\nMay 17 00:26:21 ct-zulip supervisord[157]: 2023-05-17 00:26:21,719 INFO stopped: zulip_events_missedmessage_emails (exit status 0)<br>\nMay 17 00:26:21 ct-zulip supervisord[157]: 2023-05-17 00:26:21,720 INFO waiting for zulip_events_missedmessage_mobile_notifications to stop<br>\nMay 17 00:26:21 ct-zulip supervisord[157]: 2023-05-17 00:26:21,964 INFO stopped: zulip_events_missedmessage_mobile_notifications (exit status 0)<br>\nMay 17 00:26:21 ct-zulip supervisord[157]: 2023-05-17 00:26:21,965 INFO waiting for zulip_events_outgoing_webhooks to stop<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,206 INFO stopped: zulip_events_outgoing_webhooks (exit status 0)<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,207 INFO waiting for zulip_events_user_activity to stop<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,444 INFO stopped: zulip_events_user_activity (exit status 0)<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,445 INFO waiting for zulip_events_user_activity_interval to stop<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,685 INFO stopped: zulip_events_user_activity_interval (exit status 0)<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,686 INFO waiting for zulip_events_user_presence to stop<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,932 INFO stopped: zulip_events_user_presence (exit status 0)<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,933 INFO waiting for zulip_deliver_scheduled_emails to stop<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,940 INFO stopped: zulip_deliver_scheduled_emails (terminated by SIGTERM)<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,941 INFO waiting for zulip_deliver_scheduled_messages to stop<br>\nMay 17 00:26:22 ct-zulip supervisord[157]: 2023-05-17 00:26:22,948 INFO stopped: zulip_deliver_scheduled_messages (terminated by SIGTERM)<br>\nMay 17 00:31:12 ct-zulip supervisord[157]: 2023-05-17 00:31:12,413 INFO waiting for go-camo to stop<br>\nMay 17 00:31:13 ct-zulip supervisord[157]: 2023-05-17 00:31:13,416 INFO stopped: go-camo (exit status 0)<br>\nMay 17 00:31:13 ct-zulip supervisord[157]: 2023-05-17 00:31:13,419 INFO spawned: 'go-camo' with pid 15013<br>\nMay 17 00:31:13 ct-zulip supervisord[157]: 2023-05-17 00:31:13,420 INFO waiting for smokescreen to stop<br>\nMay 17 00:31:14 ct-zulip supervisord[157]: 2023-05-17 00:31:14,131 INFO stopped: smokescreen (exit status 0)<br>\nMay 17 00:31:14 ct-zulip supervisord[157]: 2023-05-17 00:31:14,133 INFO spawned: 'smokescreen' with pid 15021<br>\nMay 17 00:31:14 ct-zulip supervisord[157]: 2023-05-17 00:31:14,253 INFO waiting for go-camo to stop<br>\nMay 17 00:31:14 ct-zulip supervisord[157]: 2023-05-17 00:31:14,253 INFO waiting for smokescreen to stop<br>\nMay 17 00:31:14 ct-zulip supervisord[157]: 2023-05-17 00:31:14,254 INFO stopped: go-camo (exit status 0)<br>\nMay 17 00:31:16 ct-zulip supervisord[157]: 2023-05-17 00:31:16,810 INFO waiting for smokescreen to stop<br>\nMay 17 00:31:18 ct-zulip supervisord[157]: 2023-05-17 00:31:18,814 INFO waiting for smokescreen to stop<br>\nMay 17 00:31:20 ct-zulip supervisord[157]: 2023-05-17 00:31:20,818 INFO waiting for smokescreen to stop<br>\nMay 17 00:31:22 ct-zulip supervisord[157]: 2023-05-17 00:31:22,822 INFO waiting for smokescreen to stop<br>\nMay 17 00:31:24 ct-zulip supervisord[157]: 2023-05-17 00:31:24,825 WARN killing 'smokescreen' (15021) with SIGKILL<br>\nMay 17 00:31:24 ct-zulip supervisord[157]: 2023-05-17 00:31:24,826 INFO waiting for smokescreen to stop<br>\nMay 17 00:31:24 ct-zulip supervisord[157]: 2023-05-17 00:31:24,827 INFO stopped: smokescreen (terminated by SIGKILL)<br>\nMay 17 00:31:25 ct-zulip systemd[1]: Stopping Supervisor process control system for UNIX...<br>\nMay 17 00:31:25 ct-zulip supervisorctl[15039]: Shut down<br>\nMay 17 00:31:25 ct-zulip systemd[1]: supervisor.service: Succeeded.<br>\nMay 17 00:31:25 ct-zulip systemd[1]: Stopped Supervisor process control system for UNIX.<br>\nMay 17 00:31:25 ct-zulip systemd[1]: supervisor.service: Consumed 1min 2.796s CPU time.<br>\nMay 17 00:31:25 ct-zulip systemd[1]: Started Supervisor process control system for UNIX.<br>\nMay 17 00:31:26 ct-zulip supervisord[15040]: Error: The minimum number of file descriptors required to run this process is 1000000 as per the \"minfds\" command-line argument or config file setting. The current &gt;<br>\nMay 17 00:31:26 ct-zulip supervisord[15040]: For help, use /usr/bin/supervisord -h<br>\nMay 17 00:31:26 ct-zulip systemd[1]: supervisor.service: Main process exited, code=exited, status=2/INVALIDARGUMENT<br>\nMay 17 00:31:26 ct-zulip systemd[1]: supervisor.service: Failed with result 'exit-code'.<br>\nMay 17 00:32:16 ct-zulip systemd[1]: supervisor.service: Scheduled restart job, restart counter is at 1.<br>\nMay 17 00:32:16 ct-zulip systemd[1]: Stopped Supervisor process control system for UNIX.<br>\nMay 17 00:32:16 ct-zulip systemd[1]: Started Supervisor process control system for UNIX.<br>\nMay 17 00:32:16 ct-zulip supervisord[15117]: Error: The minimum number of file descriptors required to run this process is 1000000 as per the \"minfds\" command-line argument or config file setting. The current &gt;<br>\nMay 17 00:32:16 ct-zulip supervisord[15117]: For help, use /usr/bin/supervisord -h<br>\nMay 17 00:32:16 ct-zulip systemd[1]: supervisor.service: Main process exited, code=exited, status=2/INVALIDARGUMENT<br>\nMay 17 00:32:16 ct-zulip systemd[1]: supervisor.service: Failed with result 'exit-code'.</p>",
        "id": 1570915,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684298008
    },
    {
        "content": "<p><code>ulimit -a</code>?</p>",
        "id": 1570916,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684298025
    },
    {
        "content": "<p>Also seeing the end of the line that starts:</p>\n<div class=\"codehilite\"><pre><span></span><code>May 17 00:32:16 ct-zulip supervisord[15117]: Error: The minimum number of file descriptors required to run this process is 1000000 as per the &quot;minfds&quot; command-line argument or config file setting. The current &gt;\n</code></pre></div>",
        "id": 1570918,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684298062
    },
    {
        "content": "<p>root@ct-zulip:~# ulimit -a<br>\nreal-time non-blocking time  (microseconds, -R) unlimited<br>\ncore file size              (blocks, -c) 0<br>\ndata seg size               (kbytes, -d) unlimited<br>\nscheduling priority                 (-e) 0<br>\nfile size                   (blocks, -f) unlimited<br>\npending signals                     (-i) 243873<br>\nmax locked memory           (kbytes, -l) 64<br>\nmax memory size             (kbytes, -m) unlimited<br>\nopen files                          (-n) 40000<br>\npipe size                (512 bytes, -p) 8<br>\nPOSIX message queues         (bytes, -q) 819200<br>\nreal-time priority                  (-r) 0<br>\nstack size                  (kbytes, -s) 8192<br>\ncpu time                   (seconds, -t) unlimited<br>\nmax user processes                  (-u) 243873<br>\nvirtual memory              (kbytes, -v) unlimited<br>\nfile locks                          (-x) unlimited</p>",
        "id": 1570924,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684298111
    },
    {
        "content": "<p>root@ct-zulip:~# /usr/bin/supervisord -n -c /etc/supervisor/supervisord.conf<br>\nError: The minimum number of file descriptors required to run this process is 1000000 as per the \"minfds\" command-line argument or config file setting. The current environment will only allow you to open 1000000 file descriptors.  Either raise the number of usable file descriptors in your environment (see README.rst) or lower the minfds setting in the config file to allow the process to start.<br>\nFor help, use /usr/bin/supervisord -h</p>",
        "id": 1570927,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684298140
    },
    {
        "content": "<p>root@ct-zulip:~# cat /etc/systemd/system.conf.d/limits.conf<br>\n[Manager]<br>\nDefaultLimitNOFILE=1000000</p>",
        "id": 1570928,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684298243
    },
    {
        "content": "<p>OK, that's what we got; you can go ahead and restore from snapshot.</p>",
        "id": 1570929,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684298321
    },
    {
        "content": "<p>Error: The minimum number of file descriptors required to run this process is 1000000 as per the \"minfds\" command-line argument or config file setting. The current environment will only allow you to open 1000000 file descriptors.  Either raise the number of usable file descriptors in your environment (see README.rst) or lower the minfds setting in the config file to allow the process to start</p>",
        "id": 1570930,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684298489
    },
    {
        "content": "<p>journalctl was not wraping lines</p>",
        "id": 1570932,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684298517
    },
    {
        "content": "<p>good. I'm back on Zulip Server 6.1</p>",
        "id": 1570933,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684298629
    },
    {
        "content": "<p>The error of \"you can't set it to <em>x</em>, the max is <em>x</em>\" vaguely rings a bell, but I'll poke more tomorrow.</p>\n<p>I suspect we'll tune down the minfds, since moving that up is really only relevant for very very large deploys.</p>",
        "id": 1570935,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684298762
    },
    {
        "content": "<p>Thank you again for being so willing to try this repeatedly!</p>",
        "id": 1570936,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684298777
    },
    {
        "content": "<p>Oho: <a href=\"#narrow/stream/9-issues/topic/Unable.20to.20start.20zulip/near/779097\">https://chat.zulip.org/#narrow/stream/9-issues/topic/Unable.20to.20start.20zulip/near/779097</a></p>",
        "id": 1570938,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684298939
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/Upgrades.20in.20unprivileged.20LXC.20container/near/1570936\">said</a>:</p>\n<blockquote>\n<p>Thank you again for being so willing to try this repeatedly!</p>\n</blockquote>\n<p>My pleasure... but I do have to gain something in helping you... The ability to easily upgrade to 7.0 when released.</p>",
        "id": 1570946,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684299401
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/Upgrades.20in.20unprivileged.20LXC.20container/near/1570938\">said</a>:</p>\n<blockquote>\n<p>Oho: <a href=\"#narrow/stream/9-issues/topic/Unable.20to.20start.20zulip/near/779097\">https://chat.zulip.org/#narrow/stream/9-issues/topic/Unable.20to.20start.20zulip/near/779097</a></p>\n</blockquote>\n<p>3 years ago? Want me to try that <code>lxc.prlimit.nofile: 50000</code> ?</p>",
        "id": 1570951,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684299541
    },
    {
        "content": "<p>It'd need to be <code>1000000</code> for the latest bump</p>",
        "id": 1570953,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684299609
    },
    {
        "content": "<p>root@ct-zulip:~# prlimit -p 1<br>\nRESOURCE   DESCRIPTION                             SOFT      HARD UNITS<br>\nAS         address space limit                unlimited unlimited bytes<br>\nCORE       max core file size                         0 unlimited bytes<br>\nCPU        CPU time                           unlimited unlimited seconds<br>\nDATA       max data size                      unlimited unlimited bytes<br>\nFSIZE      max file size                      unlimited unlimited bytes<br>\nLOCKS      max number of file locks held      unlimited unlimited locks<br>\nMEMLOCK    max locked-in-memory address space     65536     65536 bytes<br>\nMSGQUEUE   max bytes in POSIX mqueues            819200    819200 bytes<br>\nNICE       max nice prio allowed to raise             0         0<br>\nNOFILE     max number of open files              524288    524288 files<br>\nNPROC      max number of processes               243873    243873 processes<br>\nRSS        max resident set size              unlimited unlimited bytes<br>\nRTPRIO     max real-time priority                     0         0<br>\nRTTIME     timeout for real-time tasks        unlimited unlimited microsecs<br>\nSIGPENDING max number of pending signals         243873    243873 signals<br>\nSTACK      max stack size                       8388608 unlimited bytes</p>",
        "id": 1570958,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684299976
    },
    {
        "content": "<p>with the  instruction in my container .conf :</p>\n<p>root@ct-zulip:~# prlimit -p 1<br>\nRESOURCE   DESCRIPTION                             SOFT      HARD UNITS<br>\nAS         address space limit                unlimited unlimited bytes<br>\nCORE       max core file size                         0 unlimited bytes<br>\nCPU        CPU time                           unlimited unlimited seconds<br>\nDATA       max data size                      unlimited unlimited bytes<br>\nFSIZE      max file size                      unlimited unlimited bytes<br>\nLOCKS      max number of file locks held      unlimited unlimited locks<br>\nMEMLOCK    max locked-in-memory address space     65536     65536 bytes<br>\nMSGQUEUE   max bytes in POSIX mqueues            819200    819200 bytes<br>\nNICE       max nice prio allowed to raise             0         0<br>\nNOFILE     max number of open files             1000000   1000000 files<br>\nNPROC      max number of processes               243873    243873 processes<br>\nRSS        max resident set size              unlimited unlimited bytes<br>\nRTPRIO     max real-time priority                     0         0<br>\nRTTIME     timeout for real-time tasks        unlimited unlimited microsecs<br>\nSIGPENDING max number of pending signals         243873    243873 signals<br>\nSTACK      max stack size                       8388608 unlimited bytes</p>",
        "id": 1570960,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684300209
    },
    {
        "content": "<p>I need to retire for the night, but that looks hopeful for a successful upgrade to me.</p>",
        "id": 1570961,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684300488
    },
    {
        "content": "<p>almost there...</p>",
        "id": 1570964,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684300514
    },
    {
        "content": "<p>I'm past point of failure... still upgrading</p>",
        "id": 1570966,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684300566
    },
    {
        "content": "<p>Deployments cleaned successfully...<br>\nCleaning orphaned/unused caches...<br>\nCleaning unused venv caches...<br>\nCleaning unused node modules caches...<br>\nCleaning unused emoji caches...<br>\nDone!</p>",
        "id": 1570967,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684300598
    },
    {
        "content": "<p>Did I do a downgrade?<br>\n<a href=\"/user_uploads/2/88/sE0viRcRGTlfV-n3p5N3qGrv/image.png\">image.png</a><br>\n6.0 isnt lower than 6.1?</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/88/sE0viRcRGTlfV-n3p5N3qGrv/image.png\" title=\"image.png\"><img src=\"/user_uploads/2/88/sE0viRcRGTlfV-n3p5N3qGrv/image.png\"></a></div>",
        "id": 1570968,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684300708
    },
    {
        "content": "<p>You can answer tomorrow... damned... we already are tomorrow</p>",
        "id": 1570969,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684300798
    },
    {
        "content": "<p>You did not downgrade - that's <code>main</code>.  It doesn't report as 7.0 because we haven't tagged that release yet.  So it reports as 2340 commits after 6.0.</p>",
        "id": 1571072,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684324869
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/Upgrades.20in.20unprivileged.20LXC.20container/near/1571072\">said</a>:</p>\n<blockquote>\n<p>You did not downgrade - that's <code>main</code>.  It doesn't report as 7.0 because we haven't tagged that release yet.  So it reports as 2340 commits after 6.0.</p>\n</blockquote>\n<p>Seriously, thank you for all your effort, trying to figure out what was not working in my attempts to upgrade to 7.0betas.</p>",
        "id": 1571089,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684327326
    },
    {
        "content": "<p>Could you explain why minfds or in my case lxc.prlimit.nofile is so important to be set to 1000000? Per supervisord documentation, the default value for minfds is 1024...  So why almost a thousand times bigger?</p>",
        "id": 1571090,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684327701
    },
    {
        "content": "<p>And, don't know if it's a placebo effect, but with 1000000 I think it was quicker to get where it usually broke.</p>",
        "id": 1571091,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684327828
    },
    {
        "content": "<p>Every connection requires a file descriptor for it -- in some cases, two (for example, nginx has one for each connection from a client, and another for the connection it then makes to django or tornado in the backend on the client's behalf, to proxy the request).  The default of 1024 means that deployments start getting connection failures if they have more than 400-500 clients at the same time.</p>\n<p>There's very little <em>cost</em> to increasing the maximum number of file descriptors -- the limit is there mostly because services want to be able to control if they turn away clients (due to lack of available file descriptors) at high load, or attempt to service them and perhaps time out due to lack of CPU time to service the many connections.</p>\n<p>We opted to just increase the limits across the board, in that it gives a large buffer for truly huge deployments (think Zulip Cloud) and makes the failure modes of small ones a little easier to reason about (admins are more likely to observe and understand CPU saturation, rather than random connection refused messages reported by clients).</p>\n<p>However, the limits we set in <a href=\"https://github.com/zulip/zulip/commit/1c76036c61d86d4fc6a574b734e1d04ad0ab43a3\">1c76036c61d86d4fc6a574b734e1d04ad0ab43a3</a> are pretty absurdly huge; we had to roll back one of the changes in <a href=\"https://github.com/zulip/zulip/commit/262b19346e4eb55e945bb627c28a5421a3d5d5b1\">262b19346e4eb55e945bb627c28a5421a3d5d5b1</a> because it <em>did</em> have a cost.  I think the right fix is to also back out more of <a href=\"https://github.com/zulip/zulip/commit/1c76036c61d86d4fc6a574b734e1d04ad0ab43a3\">1c76036c61d86d4fc6a574b734e1d04ad0ab43a3</a> and put it behind a config variable, since the existing (6.0 and before) values are already quite large.</p>",
        "id": 1571153,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684334947
    },
    {
        "content": "<p>And yes, any performance changes are purely placebo.  <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 1571154,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684334966
    },
    {
        "content": "<p>Pushed <a href=\"https://github.com/zulip/zulip/pull/25641\">#25641</a>.</p>",
        "id": 1571161,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684336847
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/Upgrades.20in.20unprivileged.20LXC.20container/near/1571161\">said</a>:</p>\n<blockquote>\n<p>Pushed <a href=\"https://github.com/zulip/zulip/pull/25641\">#25641</a>.</p>\n</blockquote>\n<p>If this is merged, my fix by setting <code>lxc.prlimit.nofile</code> to 1000000 won't be necessary anymore?</p>",
        "id": 1571229,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684342811
    },
    {
        "content": "<p>Correct.</p>",
        "id": 1571230,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1684342827
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/Upgrades.20in.20unprivileged.20LXC.20container/near/1571153\">said</a>:</p>\n<blockquote>\n<p>Every connection requires a file descriptor for it -- in some cases, two (for example, nginx has one for each connection from a client, and another for the connection it then makes to django or tornado in the backend on the client's behalf, to proxy the request).  The default of 1024 means that deployments start getting connection failures if they have more than 400-500 clients at the same time.<br>\n.....</p>\n</blockquote>\n<p>Thank you for this answer. Again thank you for your troubleshooting!</p>",
        "id": 1571233,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684342968
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/Upgrades.20in.20unprivileged.20LXC.20container/near/1571161\">said</a>:</p>\n<blockquote>\n<p>Pushed <a href=\"https://github.com/zulip/zulip/pull/25641\">#25641</a>.</p>\n</blockquote>\n<p>Hi! the PR was merged on May 18th. So, it's safe to assume I could set <code>lxc.prlimit.nofile</code> back to it's default value? Earlier this evening I did another upgrade with 1M nofile from git main. I'm now running 6.0.2492.</p>",
        "id": 1576388,
        "sender_full_name": "Louis-Marie Gendron",
        "timestamp": 1684983155
    },
    {
        "content": "<p>Hello there,<br>\nI following this guide to set up my development environment<br>\n<a href=\"https://github.com/zulip/docker-zulip\">https://github.com/zulip/docker-zulip</a></p>\n<p>when I write docker-compose up </p>\n<p>I get this never ending log</p>\n<p><a href=\"/user_uploads/2/c7/bgnBj5Lo4d-Q4xjFHDGePimi/zulip_error.png\">zulip_error.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/c7/bgnBj5Lo4d-Q4xjFHDGePimi/zulip_error.png\" title=\"zulip_error.png\"><img src=\"/user_uploads/2/c7/bgnBj5Lo4d-Q4xjFHDGePimi/zulip_error.png\"></a></div><p>could someone tell me what I am doing wrong.</p>",
        "id": 1576538,
        "sender_full_name": "Shahboz",
        "timestamp": 1685010376
    },
    {
        "content": "<p>I can share the content of my yml file as well</p>",
        "id": 1576539,
        "sender_full_name": "Shahboz",
        "timestamp": 1685010470
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"26472\">@Louis-Marie Gendron</span> we've not backported the change to the 6.x branch, I think, but if you've upgraded to <code>main</code>, you should have the fix.</p>",
        "id": 1576788,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1685042127
    }
]