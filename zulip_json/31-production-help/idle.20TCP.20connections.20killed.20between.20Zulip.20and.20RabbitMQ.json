[
    {
        "content": "<p>Hi,</p>\n<p>I migrated an old instance of Zulip (2.1.4-0) deployed with Docker-compose to Kubernetes with the HelmChart.<br>\nI upgraded Zulip until 6.1-0 + Psql 10 to 14 and i use the versions of dependencies specified in the HelmChart :</p>\n<ul>\n<li>memcached 6.0.16</li>\n<li>rabbitmq 8.32.0</li>\n<li>redis 16.8.7</li>\n</ul>\n<p>After a period of inactivity, the first request generates errors :</p>\n<p><strong>errors.log :</strong></p>\n<div class=\"codehilite\"><pre><span></span><code>[...]\ndjango.db.utils.OperationalError: server closed the connection unexpectedly\n    This probably means the server terminated abnormally\n    before or while processing the request.\n[...]\n</code></pre></div>\n<p><strong>workers.log :</strong></p>\n<div class=\"codehilite\"><pre><span></span><code>2023-02-21 15:21:19.691 WARN [] *** Sending self SIGUSR1 to trigger a restart.\n2023-02-21 15:21:19.692 INFO [] Processing traceback with type server for None\n2023-02-21 15:21:19.692 INFO [process_queue] Worker 0 disconnecting from queue missedmessage_emails\n2023-02-21 15:21:22.612 INFO [process_queue] Worker 0 connecting to queue missedmessage_emails\n</code></pre></div>\n<p><strong>events_missedmessage_emails.log (for example) :</strong></p>\n<div class=\"codehilite\"><pre><span></span><code>2023-02-21 15:21:19.666 ERR  [] Problem handling data on queue missedmessage_emails\n</code></pre></div>\n<p>I have this issue with multiples workers :</p>\n<ul>\n<li>zulip-workers:zulip_events_missedmessage_emails</li>\n<li>zulip-workers:zulip_events_user_activity</li>\n<li>zulip-workers:zulip_events_user_activity_interval</li>\n<li>zulip-workers:zulip_events_user_presence</li>\n</ul>\n<p>It's look like the idle TCP connections are killed after few minutes, Zulip try to connect at RabbitMQ with this connection without success and recreate a new connection.</p>\n<p>I found this old issue which describe the problem (with memcached, not with rmq but its very similar) : <a href=\"https://github.com/zulip/zulip/issues/14455\">https://github.com/zulip/zulip/issues/14455</a></p>\n<p>Do you have an idea for fix that ?<br>\nThanks you.</p>",
        "id": 1511260,
        "sender_full_name": "Anthony B.",
        "timestamp": 1677057496
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"3\" href=\"/#narrow/stream/3-backend/topic/idle.20TCP.20connections.20killed.20between.20Zulip.20and.20RabbitMQ\">#backend &gt; idle TCP connections killed between Zulip and RabbitMQ</a> by <span class=\"user-mention silent\" data-user-id=\"21836\">Lauryn Menard</span>.</p>",
        "id": 1511264,
        "sender_full_name": "Notification Bot",
        "timestamp": 1677058359
    },
    {
        "content": "<p>Hey <span class=\"user-mention\" data-user-id=\"26114\">@Anthony B.</span> - I moved your message to the <a class=\"stream\" data-stream-id=\"31\" href=\"/#narrow/stream/31-production-help\">#production help</a> stream so that folks who'll be able to help with your issue will see it sooner.</p>\n<p>Also, when posting logs, tracebacks, etc., it's helpful to use <a href=\"https://zulip.com/help/code-blocks\">code blocks</a>.</p>",
        "id": 1511272,
        "sender_full_name": "Lauryn Menard",
        "timestamp": 1677061070
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"26114\">@Anthony B.</span>: Are you using Docker Swarm?</p>\n<p>How long is the period of inactivity?</p>",
        "id": 1511384,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1677089568
    },
    {
        "content": "<p>I also don't see anything in there which points to rabbitmq.  <code>django.db.utils.OperationalError</code> implies it's a database error, so PostgreSQL, not RabbitMQ.  You'll need to show more of the tracebacks to show any of the actual tracebacks or error messages.</p>",
        "id": 1511394,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1677089807
    },
    {
        "content": "<p><a href=\"https://github.com/zulip/zulip/commit/b312001fd92dc36233e5a9f57cd9fada890880c4\">b312001fd92dc36233e5a9f57cd9fada890880c4</a> addressed this for RabbitMQ, so I'd guess that this may be a problem with memcached/postgresql but <em>not</em> RabbitMQ.</p>",
        "id": 1511395,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1677089871
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> </p>\n<p>No, Kubernetes (1.25).<br>\nFor the period of inactivity, i don't know exactly, i think at least 20 min for trigger this error.</p>\n<p>In the /var/log/zulip/events_user_activity_interval.log (also in /var/log/zulip/errors.log) :</p>\n<div class=\"codehilite\"><pre><span></span><code> 2023-02-23 08:17:03.978 ERR  [] Problem handling data on queue user_activity_interval\nTraceback (most recent call last):\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/backends/utils.py&quot;, line 89, in _execute\n    return self.cursor.execute(sql, params)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/lib/db.py&quot;, line 34, in execute\n    wrapper_execute(self, super().execute, query, vars)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/lib/db.py&quot;, line 19, in wrapper_execute\n    action(sql, params)\npsycopg2.OperationalError: server closed the connection unexpectedly\n    This probably means the server terminated abnormally\n    before or while processing the request.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/worker/queue_processors.py&quot;, line 317, in do_consume\n    consume_func(events)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/worker/queue_processors.py&quot;, line 357, in &lt;lambda&gt;\n    consume_func = lambda events: self.consume(events[0])\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/worker/queue_processors.py&quot;, line 553, in consume\n    user_profile = get_user_profile_by_id(event[&quot;user_profile_id&quot;])\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/lib/cache.py&quot;, line 171, in func_with_caching\n    val = func(*args, **kwargs)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/models.py&quot;, line 3698, in get_user_profile_by_id\n    return UserProfile.objects.select_related().get(id=user_profile_id)\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/models/query.py&quot;, line 646, in get\n    num = len(clone)\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/models/query.py&quot;, line 376, in __len__\n    self._fetch_all()\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/models/query.py&quot;, line 1866, in _fetch_all\n    self._result_cache = list(self._iterable_class(self))\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/models/query.py&quot;, line 87, in __iter__\n    results = compiler.execute_sql(\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/models/sql/compiler.py&quot;, line 1398, in execute_sql\n    cursor.execute(sql, params)\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/backends/utils.py&quot;, line 67, in execute\n    return self._execute_with_wrappers(\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/backends/utils.py&quot;, line 80, in _execute_with_wrappers\n    return executor(sql, params, many, context)\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/backends/utils.py&quot;, line 89, in _execute\n    return self.cursor.execute(sql, params)\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/utils.py&quot;, line 91, in __exit__\n    raise dj_exc_value.with_traceback(traceback) from exc_value\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/backends/utils.py&quot;, line 89, in _execute\n    return self.cursor.execute(sql, params)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/lib/db.py&quot;, line 34, in execute\n    wrapper_execute(self, super().execute, query, vars)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/lib/db.py&quot;, line 19, in wrapper_execute\n    action(sql, params)\ndjango.db.utils.OperationalError: server closed the connection unexpectedly\n    This probably means the server terminated abnormally\n    before or while processing the request.\nStack (most recent call last):\n  File &quot;/home/zulip/deployments/current/manage.py&quot;, line 151, in &lt;module&gt;\n    execute_from_command_line(sys.argv)\n  File &quot;/home/zulip/deployments/current/manage.py&quot;, line 116, in execute_from_command_line\n    utility.execute()\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/core/management/__init__.py&quot;, line 440, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/core/management/base.py&quot;, line 402, in run_from_argv\n    self.execute(*args, **cmd_options)\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/core/management/base.py&quot;, line 448, in execute\n    output = self.handle(*args, **options)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/management/commands/process_queue.py&quot;, line 111, in handle\n    worker.start()\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/worker/queue_processors.py&quot;, line 406, in start\n    self.q.start_json_consumer(\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/lib/queue.py&quot;, line 213, in start_json_consumer\n    self.ensure_queue(queue_name, do_consume)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/lib/queue.py&quot;, line 169, in ensure_queue\n    callback(self.channel)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/lib/queue.py&quot;, line 203, in do_consume\n    callback(events)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/worker/queue_processors.py&quot;, line 408, in &lt;lambda&gt;\n    lambda events: self.consume_single_event(events[0]),\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/worker/queue_processors.py&quot;, line 358, in consume_single_event\n    self.do_consume(consume_func, [event])\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/worker/queue_processors.py&quot;, line 327, in do_consume\n    self._handle_consume_exception(events, e)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/worker/queue_processors.py&quot;, line 384, in _handle_consume_exception\n    logging.exception(\n2023-02-23 08:17:04.001 WARN [] *** Sending self SIGUSR1 to trigger a restart.\n2023-02-23 08:17:04.002 INFO [process_queue] Worker 0 disconnecting from queue user_activity_interval\n2023-02-23 08:17:06.733 INFO [process_queue] Worker 0 connecting to queue user_activity_interval\n</code></pre></div>\n<p>At same time in logs of RabbitMQ pod :</p>\n<div class=\"codehilite\"><pre><span></span><code>2023-02-23 08:17:03.998541+00:00 [info] &lt;0.10513.16&gt; accepting AMQP connection &lt;0.10513.16&gt; (192.168.98.39:49344 -&gt; 192.168.98.47:5672)\n2023-02-23 08:17:03.999951+00:00 [info] &lt;0.10513.16&gt; connection &lt;0.10513.16&gt; (192.168.98.39:49344 -&gt; 192.168.98.47:5672): user &#39;zulip&#39; authenticated and granted access to vhost &#39;/&#39;\n2023-02-23 08:17:04.110198+00:00 [warn] &lt;0.8620.16&gt; closing AMQP connection &lt;0.8620.16&gt; (192.168.98.39:44636 -&gt; 192.168.98.47:5672, vhost: &#39;/&#39;, user: &#39;zulip&#39;):\n2023-02-23 08:17:04.110198+00:00 [warn] &lt;0.8620.16&gt; client unexpectedly closed TCP connection\n2023-02-23 08:17:04.299169+00:00 [warn] &lt;0.10513.16&gt; closing AMQP connection &lt;0.10513.16&gt; (192.168.98.39:49344 -&gt; 192.168.98.47:5672, vhost: &#39;/&#39;, user: &#39;zulip&#39;):\n2023-02-23 08:17:04.299169+00:00 [warn] &lt;0.10513.16&gt; client unexpectedly closed TCP connection\n2023-02-23 08:17:06.736031+00:00 [info] &lt;0.10533.16&gt; accepting AMQP connection &lt;0.10533.16&gt; (192.168.98.39:56568 -&gt; 192.168.98.47:5672)\n2023-02-23 08:17:06.737465+00:00 [info] &lt;0.10533.16&gt; connection &lt;0.10533.16&gt; (192.168.98.39:56568 -&gt; 192.168.98.47:5672): user &#39;zulip&#39; authenticated and granted access to vhost &#39;/&#39;\n</code></pre></div>\n<p>/var/log/zulip/queue_error/user_activity_interval.errors :</p>\n<div class=\"codehilite\"><pre><span></span><code>Wed Feb 22 12:13:31 2023    [{&quot;user_profile_id&quot;:757,&quot;time&quot;:1677068011}]\nWed Feb 22 14:00:06 2023    [{&quot;user_profile_id&quot;:772,&quot;time&quot;:1677074406}]\nWed Feb 22 14:54:17 2023    [{&quot;user_profile_id&quot;:772,&quot;time&quot;:1677077657}]\nWed Feb 22 15:40:05 2023    [{&quot;user_profile_id&quot;:757,&quot;time&quot;:1677080405}]\nWed Feb 22 16:59:54 2023    [{&quot;user_profile_id&quot;:757,&quot;time&quot;:1677085194}]\nWed Feb 22 17:41:18 2023    [{&quot;user_profile_id&quot;:430,&quot;time&quot;:1677087678}]\nThu Feb 23 07:46:38 2023    [{&quot;user_profile_id&quot;:430,&quot;time&quot;:1677138398}]\nThu Feb 23 08:17:04 2023    [{&quot;user_profile_id&quot;:430,&quot;time&quot;:1677140223}]\n</code></pre></div>\n<p>The worker is restarted each time : </p>\n<div class=\"codehilite\"><pre><span></span><code>zulip-workers:zulip_events_user_activity                        RUNNING   pid 231, uptime 22:40:28\nzulip-workers:zulip_events_user_activity_interval               RUNNING   pid 1208, uptime 1:40:00\nzulip-workers:zulip_events_user_presence                        RUNNING   pid 233, uptime 22:40:28\n</code></pre></div>",
        "id": 1512080,
        "sender_full_name": "Anthony B.",
        "timestamp": 1677146033
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>2023-02-23 08:17:03.999951+00:00 [info] &lt;0.10513.16&gt; connection &lt;0.10513.16&gt; (192.168.98.39:49344 -&gt; 192.168.98.47:5672): user &#39;zulip&#39; authenticated and granted access to vhost &#39;/&#39;\n2023-02-23 08:17:04.299169+00:00 [warn] &lt;0.10513.16&gt; closing AMQP connection &lt;0.10513.16&gt; (192.168.98.39:49344 -&gt; 192.168.98.47:5672, vhost: &#39;/&#39;, user: &#39;zulip&#39;):\n2023-02-23 08:17:04.299169+00:00 [warn] &lt;0.10513.16&gt; client unexpectedly closed TCP connection\n</code></pre></div>\n<p>That's like 0.3s before the connection got closed.  That's ... not a lot.</p>",
        "id": 1512144,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1677167994
    },
    {
        "content": "<p>This is a bit outside my expertise; I don't know if <span class=\"user-mention\" data-user-id=\"25085\">@Josh Klar</span> has any thoughts on k8s configuration which might be causing this sort of thing.</p>\n<p>One potential workaround would be to try telling PostgreSQL to have a short keepalive ping:</p>\n<div class=\"codehilite\" data-code-language=\"Diff\"><pre><span></span><code><span class=\"gh\">diff --git zproject/computed_settings.py zproject/computed_settings.py</span>\n<span class=\"gh\">index ba3f369308..2a5d57c4ed 100644</span>\n<span class=\"gd\">--- zproject/computed_settings.py</span>\n<span class=\"gi\">+++ zproject/computed_settings.py</span>\n<span class=\"gu\">@@ -283,6 +283,8 @@ DATABASES: Dict[str, Dict[str, Any]] = {</span>\n<span class=\"w\"> </span>        \"OPTIONS\": {\n<span class=\"w\"> </span>            \"connection_factory\": TimeTrackingConnection,\n<span class=\"w\"> </span>            \"cursor_factory\": TimeTrackingCursor,\n<span class=\"gi\">+            \"keepalives\": 1,</span>\n<span class=\"gi\">+            \"keepalives_idle\": 60,</span>\n<span class=\"w\"> </span>        },\n<span class=\"w\"> </span>    }\n<span class=\"w\"> </span>}\n</code></pre></div>\n<p>...but if you're seeing connection drops after <em>less than a second</em>, then no amount of keepalive is going to save you; you'll need to adress the underlying networking issue that's terminating the connections.</p>",
        "id": 1512147,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1677168371
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/idle.20TCP.20connections.20killed.20between.20Zulip.20and.20RabbitMQ/near/1512147\">said</a>:</p>\n<blockquote>\n<p>This is a bit outside my expertise; I don't know if <span class=\"user-mention silent\" data-user-id=\"25085\">Josh Klar</span> has any thoughts on k8s configuration which might be causing this sort of thing.</p>\n<p>One potential workaround would be to try telling PostgreSQL to have a short keepalive ping:</p>\n<div class=\"codehilite\" data-code-language=\"Diff\"><pre><span></span><code><span class=\"gh\">diff --git zproject/computed_settings.py zproject/computed_settings.py</span>\n<span class=\"gh\">index ba3f369308..2a5d57c4ed 100644</span>\n<span class=\"gd\">--- zproject/computed_settings.py</span>\n<span class=\"gi\">+++ zproject/computed_settings.py</span>\n<span class=\"gu\">@@ -283,6 +283,8 @@ DATABASES: Dict[str, Dict[str, Any]] = {</span>\n<span class=\"w\"> </span>        \"OPTIONS\": {\n<span class=\"w\"> </span>            \"connection_factory\": TimeTrackingConnection,\n<span class=\"w\"> </span>            \"cursor_factory\": TimeTrackingCursor,\n<span class=\"gi\">+            \"keepalives\": 1,</span>\n<span class=\"gi\">+            \"keepalives_idle\": 60,</span>\n<span class=\"w\"> </span>        },\n<span class=\"w\"> </span>    }\n<span class=\"w\"> </span>}\n</code></pre></div>\n<p>...but if you're seeing connection drops after <em>less than a second</em>, then no amount of keepalive is going to save you; you'll need to adress the underlying networking issue that's terminating the connections.</p>\n</blockquote>\n<p>Thanks you for ur help.</p>\n<p>Why the keepalive of PostgreSQL would have an impact with communications between Zulip and RabbitMQ ? After reading the logs, the connections are closed between Zulip and RabbitMQ. <br>\nI missed something i think.</p>",
        "id": 1512155,
        "sender_full_name": "Anthony B.",
        "timestamp": 1677170161
    },
    {
        "content": "<p><em>Both</em> RabbitMQ and PostgreSQL are having their connections dropped.  Here's RabbitMQ's connection dropping:</p>\n<div class=\"codehilite\"><pre><span></span><code>2023-02-23 08:17:03.999951+00:00 [info] &lt;0.10513.16&gt; connection &lt;0.10513.16&gt; (192.168.98.39:49344 -&gt; 192.168.98.47:5672): user &#39;zulip&#39; authenticated and granted access to vhost &#39;/&#39;\n2023-02-23 08:17:04.299169+00:00 [warn] &lt;0.10513.16&gt; closing AMQP connection &lt;0.10513.16&gt; (192.168.98.39:49344 -&gt; 192.168.98.47:5672, vhost: &#39;/&#39;, user: &#39;zulip&#39;):\n2023-02-23 08:17:04.299169+00:00 [warn] &lt;0.10513.16&gt; client unexpectedly closed TCP connection\n</code></pre></div>\n<p>Here's PostgreSQL's dropping (<code>psycopg2</code> is the low-level PostgreSQL driver):</p>\n<div class=\"codehilite\"><pre><span></span><code>Traceback (most recent call last):\n  File &quot;/srv/zulip-venv-cache/b2cdd20dd55ad18cd1e512082a3ec530e6b6ece9/zulip-py3-venv/lib/python3.8/site-packages/django/db/backends/utils.py&quot;, line 89, in _execute\n    return self.cursor.execute(sql, params)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/lib/db.py&quot;, line 34, in execute\n    wrapper_execute(self, super().execute, query, vars)\n  File &quot;/home/zulip/deployments/2023-01-23-18-51-59/zerver/lib/db.py&quot;, line 19, in wrapper_execute\n    action(sql, params)\npsycopg2.OperationalError: server closed the connection unexpectedly\n    This probably means the server terminated abnormally\n    before or while processing the request.\n</code></pre></div>",
        "id": 1512157,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1677170402
    },
    {
        "content": "<p>I suspect memcache is also having its connection severed, but we just don't have as good logging for that.</p>",
        "id": 1512159,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1677170446
    },
    {
        "content": "<p>(I've seen this and will spend my walk to go pick up breakfast in town giving this some thought. Re-marking as unread so I get back to this when at my desk, thanks for the ping)</p>",
        "id": 1512160,
        "sender_full_name": "Josh Klar",
        "timestamp": 1677170571
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/idle.20TCP.20connections.20killed.20between.20Zulip.20and.20RabbitMQ/near/1512159\">said</a>:</p>\n<blockquote>\n<p>Both RabbitMQ and PostgreSQL are having their connections dropped</p>\n</blockquote>\n<p>Effectively, i see a lot of connections resets in the log of PostgreSQL pod :</p>\n<div class=\"codehilite\"><pre><span></span><code>2023-02-23 09:54:58.495 UTC [107340] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 09:57:09.567 UTC [107413] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 09:59:49.311 UTC [108098] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.311 UTC [112183] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.311 UTC [112152] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.311 UTC [112167] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.311 UTC [112151] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.311 UTC [112136] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.311 UTC [112168] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.311 UTC [111988] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.312 UTC [112184] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.312 UTC [112135] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.312 UTC [112120] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 10:42:29.312 UTC [112021] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 11:43:39.326 UTC [117330] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 11:52:23.615 UTC [110031] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 12:31:42.911 UTC [121302] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 12:32:02.808 UTC [127867] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 14:34:02.943 UTC [132055] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 14:34:02.943 UTC [132152] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 14:34:02.943 UTC [132023] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 14:34:02.943 UTC [132022] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 14:38:25.087 UTC [132219] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 14:38:25.087 UTC [132218] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 15:12:38.914 UTC [138416] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 15:17:44.383 UTC [135826] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 15:20:28.222 UTC [133856] LOG:  could not receive data from client: Connection reset by peer\n2023-02-23 16:26:33.150 UTC [138878] LOG:  could not receive data from client: Connection reset by peer\n</code></pre></div>\n<p>It's weird, however, the timestamps does not match with the previous errors. <br>\nI will continue to search the problem. For the moment, I don't see any impact.</p>",
        "id": 1512164,
        "sender_full_name": "Anthony B.",
        "timestamp": 1677171253
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"26114\">@Anthony B.</span> is this a local k8s cluster of some sort, or is this a cloud PaaS? (GKE, EKS, AKS, etc.)</p>\n<p>If local/on-prem, can you describe the topology a bit (whatever you can share that isn't some sort of trade secret or other proprietary info would be helpful), including any Docker driver versions, kernel versions, which distro is the host, etc? Just to kinda narrow down the haystacks to find a needle in.</p>",
        "id": 1512239,
        "sender_full_name": "Josh Klar",
        "timestamp": 1677176692
    },
    {
        "content": "<p>Bonus if you can get me the output of <code>iptables -t nat -L KUBE-SERVICES</code> on the node in question, though I understand if that one's a no-go since that's going to describe your firewalling rules.</p>\n<p>I'm somewhat looking to see if there's duplicate entries or something else that's tripping up the k8s routing engine here, or perhaps otherwise causing it to cycle connections (or perhaps pods/services entirely) for some strange reason.</p>\n<p>A ways back, K8S had problems with this type of network dropping, but a lot of the tools used then aren't used now, at least in the mainline k8s distribution. Less certain about minikube, k3s, etc., but again: more info should help here.</p>",
        "id": 1512243,
        "sender_full_name": "Josh Klar",
        "timestamp": 1677176982
    },
    {
        "content": "<p>Lastly, to see if it's outright pods getting cycled out, <code>kubectl get events -n &lt;your_app_namespace&gt; --sort-by='.metadata.creationTimestamp'</code> would be helpful: it won't tell us <em>why</em> something got torn down, but it would tell us <em>if</em>, which is helpful here (and then we can dig further).</p>",
        "id": 1512246,
        "sender_full_name": "Josh Klar",
        "timestamp": 1677177206
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"25085\">@Josh Klar</span> <br>\nIt's a local k8s cluster with 3 control-plane and 19 workers.</p>\n<div class=\"codehilite\"><pre><span></span><code>K8s : 1.24.8\nContainerd : 1.6.9\nOS : Rocky Linux 8.6\nKernel : 4.18.0-372.26.1.el8_6.x86_64\nCNI : Weave 2.8.1\n</code></pre></div>\n<p>The pods running on the same node, except memcached.</p>\n<p>Rules iptables :</p>\n<div class=\"codehilite\"><pre><span></span><code>Chain KUBE-SERVICES (2 references)\ntarget     prot opt source               destination\nKUBE-MARK-MASQ  all  -- !192.168.64.0/18      anywhere             /* Kubernetes service cluster ip + port for masquerade purpose */ match-set KUBE-CLUSTER-IP dst,dst\nKUBE-NODE-PORT  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL\nACCEPT     all  --  anywhere             anywhere             match-set KUBE-CLUSTER-IP dst,dst\n# Warning: iptables-legacy tables present, use iptables-legacy to see them\n</code></pre></div>\n<p>Events pods : </p>\n<div class=\"codehilite\"><pre><span></span><code>LAST SEEN   TYPE      REASON             OBJECT                                      MESSAGE\n4m3s        Warning   DNSConfigForming   pod/test-zulip-memcached-5f9f8b88dc-nsl6q   Search Line limits were exceeded, some search paths have been omitted, the applied search line is: infra-zulip.svc.cluster.local svc.cluster.local cluster.local ****** ***** *******\n3m21s       Warning   DNSConfigForming   pod/test-zulip-redis-master-0               Search Line limits were exceeded, some search paths have been omitted, the applied search line is: infra-zulip.svc.cluster.local svc.cluster.local cluster.local ****** ***** *******\n2m58s       Warning   DNSConfigForming   pod/test-zulip-rabbitmq-0                   Search Line limits were exceeded, some search paths have been omitted, the applied search line is: infra-zulip.svc.cluster.local svc.cluster.local cluster.local ****** ***** *******\n4m24s       Warning   DNSConfigForming   pod/test-zulip-postgresql-0                 Search Line limits were exceeded, some search paths have been omitted, the applied search line is: infra-zulip.svc.cluster.local svc.cluster.local cluster.local ****** ***** *******\n2m33s       Warning   DNSConfigForming   pod/test-zulip-0                            Search Line limits were exceeded, some search paths have been omitted, the applied search line is: infra-zulip.svc.cluster.local svc.cluster.local cluster.local ****** ***** *******\n</code></pre></div>\n<p>i will search on the network level to find logs or something that can help.<br>\nTell me if u want more informations, i'm not very familiar with K8s for the moment.</p>",
        "id": 1513186,
        "sender_full_name": "Anthony B.",
        "timestamp": 1677236516
    },
    {
        "content": "<p>Interesting to try: <a href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#session-affinity\">https://kubernetes.io/docs/reference/networking/virtual-ips/#session-affinity</a></p>",
        "id": 1513368,
        "sender_full_name": "Anthony B.",
        "timestamp": 1677257500
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"26114\">Anthony B.</span> <a href=\"#narrow/stream/31-production-help/topic/idle.20TCP.20connections.20killed.20between.20Zulip.20and.20RabbitMQ/near/1512155\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/idle.20TCP.20connections.20killed.20between.20Zulip.20and.20RabbitMQ/near/1512147\">said</a>:</p>\n<blockquote>\n<p>This is a bit outside my expertise; I don't know if <span class=\"user-mention silent\" data-user-id=\"25085\">Josh Klar</span> has any thoughts on k8s configuration which might be causing this sort of thing.</p>\n<p>One potential workaround would be to try telling PostgreSQL to have a short keepalive ping:</p>\n<div class=\"codehilite\" data-code-language=\"Diff\"><pre><span></span><code><span class=\"gh\">diff --git zproject/computed_settings.py zproject/computed_settings.py</span>\n<span class=\"gh\">index ba3f369308..2a5d57c4ed 100644</span>\n<span class=\"gd\">--- zproject/computed_settings.py</span>\n<span class=\"gi\">+++ zproject/computed_settings.py</span>\n<span class=\"gu\">@@ -283,6 +283,8 @@ DATABASES: Dict[str, Dict[str, Any]] = {</span>\n<span class=\"w\"> </span>        \"OPTIONS\": {\n<span class=\"w\"> </span>            \"connection_factory\": TimeTrackingConnection,\n<span class=\"w\"> </span>            \"cursor_factory\": TimeTrackingCursor,\n<span class=\"gi\">+            \"keepalives\": 1,</span>\n<span class=\"gi\">+            \"keepalives_idle\": 60,</span>\n<span class=\"w\"> </span>        },\n<span class=\"w\"> </span>    }\n<span class=\"w\"> </span>}\n</code></pre></div>\n<p>...but if you're seeing connection drops after <em>less than a second</em>, then no amount of keepalive is going to save you; you'll need to adress the underlying networking issue that's terminating the connections.</p>\n</blockquote>\n<p>Thanks you for ur help.</p>\n<p>Why the keepalive of PostgreSQL would have an impact with communications between Zulip and RabbitMQ ? After reading the logs, the connections are closed between Zulip and RabbitMQ. <br>\nI missed something i think.</p>\n</blockquote>\n<p>This workaround seems to work well for the moment. Thanks you <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 1515376,
        "sender_full_name": "Anthony B.",
        "timestamp": 1677589311
    },
    {
        "content": "<p>Are those options we should consider just adding unconditionally?</p>",
        "id": 1516174,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1677700341
    },
    {
        "content": "<p>I don't see a potential side-effect, those options had fully fixed my issues with Zulip on K8s.</p>",
        "id": 1516812,
        "sender_full_name": "Anthony B.",
        "timestamp": 1677761116
    },
    {
        "content": "<p>This feels like papering over the problem -- for instance, the memcached connection has no heartbeat and is likely getting killed.  Just we power along without errors when we don't have a memcached connection, so it's a silent performance-killer rather than stopping you dead.</p>",
        "id": 1517048,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1677788601
    },
    {
        "content": "<p>(The potential side effect would be a bunch of idle network traffic wasting resources.)</p>",
        "id": 1518088,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1677891749
    },
    {
        "content": "<p>I feel like the main benefit of merging that sort of change, like the one we did with RabbitMQ, is our not needing time to debug this class of issue introduced by container hosting systems that decide to be clever with how they limit network connections -- they feel pretty unpleasant for users and don't feel super easy to investigate or to provide a clear error message with details on what the user should consider at the right time.</p>",
        "id": 1518089,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1677891850
    },
    {
        "content": "<p>But I suppose the safer option would be to add a setting that we turn on by default in some docker environments (or at least recommend considering somewhere in our Docker docs?).</p>",
        "id": 1518090,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1677891900
    }
]