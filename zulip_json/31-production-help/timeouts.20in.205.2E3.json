[
    {
        "content": "<p>Hi Team, Once we upgraded from Zulip Server 5.2 to 5.3 the client connections are not stable showing internal server error randomly for the users</p>",
        "id": 1398409,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656480914
    },
    {
        "content": "<p>2022/06/29 11:11:47 [error] 1342#1342: *5126 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.20.12.31, server: , request: \"GET / HTTP/2.0\", upstream: \"uwsgi://unix:/home/zulip/deployments/uwsgi-socket\"</p>",
        "id": 1398410,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656481399
    },
    {
        "content": "<p>Getting the above error from nginix error log</p>",
        "id": 1398411,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656481434
    },
    {
        "content": "<p>Is there anything notable in <code>/var/log/zulip/django.log</code> when those timeouts happen?</p>\n<p>Is it always when logging in (on requests to <code>/</code>)?</p>",
        "id": 1398433,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656485130
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span></p>",
        "id": 1398434,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656485414
    },
    {
        "content": "<p>2022-06-29 06:49:21.956 INFO [zr] 10.31.13.147    GET     200 11.9s (mem: 8.1s/166) (md: 366ms/2) (db: 627ms/5q) (+start: 905ms) /json/messages (4151@root via Mozilla)<br>\n2022-06-29 06:49:21.958 INFO [zr] 10.31.7.214     GET     200 19.0s (mem: 13.6s/209) (md: 490ms/2) (db: 258ms/6q) (+start: 391ms) /json/messages (412@root via Mozilla)</p>",
        "id": 1398435,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656485428
    },
    {
        "content": "<p>this is the log im getting from /var/log/zulip/django.log</p>",
        "id": 1398436,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656485445
    },
    {
        "content": "<p>Yes it is happening when we loggedin</p>",
        "id": 1398446,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656487102
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"23436\">@Visvanathan</span> : are there any <code>ERROR</code> lines in <code>/var/log/zulip/django.log</code> around when the problem happens?</p>",
        "id": 1398485,
        "sender_full_name": "Matt",
        "timestamp": 1656504045
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"24078\">@Matt Keller</span> WARN [urllib3.connectionpool] Connection pool is full, discarding connection: 127.0.0.1. Connection pool size: 10</p>",
        "id": 1398489,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656504635
    },
    {
        "content": "<p>I'm getting this warning</p>",
        "id": 1398490,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656504651
    },
    {
        "content": "<p>in django.log</p>",
        "id": 1398491,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656504664
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> can u pls help us on this.</p>",
        "id": 1398497,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1656509318
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"24078\">@Matt Keller</span> Pls do help us <span class=\"user-mention\" data-user-id=\"23436\">@Visvanathan</span></p>",
        "id": 1398498,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1656509371
    },
    {
        "content": "<p>If you run <code>supervisorctl status</code> it should show <code>RUNNING</code> for all services, and the uptimes should not be too low</p>",
        "id": 1398501,
        "sender_full_name": "Matt",
        "timestamp": 1656510384
    },
    {
        "content": "<p>Feel free to paste the output here if you would like</p>",
        "id": 1398502,
        "sender_full_name": "Matt",
        "timestamp": 1656510407
    },
    {
        "content": "<p>If that looks ok, I'd next try restarting django with <code>supervisorctl restart zulip-django</code> ...</p>",
        "id": 1398503,
        "sender_full_name": "Matt",
        "timestamp": 1656510531
    },
    {
        "content": "<p>I spent a while looking into the urllib3.connectionpool warning earlier, and I don't <em>think</em> that's related, as it's for outgoing requests from the django app, not the incoming requests we are looking at.</p>",
        "id": 1398507,
        "sender_full_name": "Matt",
        "timestamp": 1656510647
    },
    {
        "content": "<p>zulip-django service is in running state only</p>",
        "id": 1398508,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656510736
    },
    {
        "content": "<p>the uptimes is not low</p>",
        "id": 1398509,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656510760
    },
    {
        "content": "<p>it is running fine</p>",
        "id": 1398510,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656510771
    },
    {
        "content": "<p>2022/06/29 19:20:53 [error] 1199#1199: *3646 upstream prematurely closed connection while reading response header from upstream, client: 10.50.15.246, server: , request: \"DELETE /json/messages/7098016 HTTP/2.0\", upstream: \"uwsgi://unix:/home/zulip/deployments/uwsgi-socket:\", host: \"<a href=\"http://inchat.episource.com\">inchat.episource.com</a>\", referrer: \"<a href=\"https://inchat.episource.com/\">https://inchat.episource.com/</a>\"</p>",
        "id": 1398511,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656510881
    },
    {
        "content": "<p>this is the error we are getting. i suspect the upstream django closing the connection.</p>",
        "id": 1398512,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656510934
    },
    {
        "content": "<p>2022/06/29 19:38:08 [error] 1199#1199: *148741 upstream prematurely closed connection while reading response header from upstream, client: 172.16.2.237, server: , request: \"POST /json/users/me/presence HTTP/2.0\", upstream: \"uwsgi://unix:/home/zulip/deployments/uwsgi-socket:\", host: \"<a href=\"http://inchat.episource.com\">inchat.episource.com</a>\", referrer: \"<a href=\"https://inchat.episource.com/\">https://inchat.episource.com/</a>\"<br>\n2022/06/29 19:38:08 [error] 1199#1199: *148741 upstream prematurely closed connection while reading response header from upstream, client: 172.16.2.237, server: , request: \"POST /json/messages/flags HTTP/2.0\", upstream: \"uwsgi://unix:/home/zulip/deployments/uwsgi-socket:\", host: \"<a href=\"http://inchat.episource.com\">inchat.episource.com</a>\", referrer: \"<a href=\"https://inchat.episource.com/\">https://inchat.episource.com/</a>\"<br>\n2022/06/29 19:38:11 [error] 1199#1199: *55180 upstream prematurely closed connection while reading response header from upstream, client: 172.16.38.75, server: , request: \"POST /json/users/me/presence HTTP/2.0\", upstream: \"uwsgi://unix:/home/zulip/deployments/uwsgi-socket:\", host: \"<a href=\"http://inchat.episource.com\">inchat.episource.com</a>\", referrer: \"<a href=\"https://inchat.episource.com/\">https://inchat.episource.com/</a>\"<br>\n2022/06/29 19:38:11 [error] 1199#1199: *847 upstream prematurely closed connection while reading response header from upstream, client: 10.50.14.176, server: , request: \"POST /json/typing HTTP/2.0\", upstream: \"uwsgi://unix:/home/zulip/deployments/uwsgi-socket:\", host: \"<a href=\"http://inchat.episource.com\">inchat.episource.com</a>\", referrer: \"<a href=\"https://inchat.episource.com/\">https://inchat.episource.com/</a>\"</p>",
        "id": 1398517,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656511899
    },
    {
        "content": "<p>2022-06-29 19:49:05.312 [warning] &lt;0.8875.15&gt; closing AMQP connection &lt;0.8875.15&gt; (127.0.0.1:41626 -&gt; 127.0.0.1:5672, vhost: '/', user: 'zulip'):<br>\nclient unexpectedly closed TCP connection<br>\n2022-06-29 19:49:05.312 [warning] &lt;0.8744.15&gt; closing AMQP connection &lt;0.8744.15&gt; (127.0.0.1:41510 -&gt; 127.0.0.1:5672, vhost: '/', user: 'zulip'):<br>\nclient unexpectedly closed TCP connection<br>\n2022-06-29 19:49:05.312 [warning] &lt;0.8735.15&gt; closing AMQP connection &lt;0.8735.15&gt; (127.0.0.1:41504 -&gt; 127.0.0.1:5672, vhost: '/', user: 'zulip'):<br>\nclient unexpectedly closed TCP connection<br>\n2022-06-29 19:49:05.313 [warning] &lt;0.8970.15&gt; closing AMQP connection &lt;0.8970.15&gt; (127.0.0.1:41710 -&gt; 127.0.0.1:5672, vhost: '/', user: 'zulip'):<br>\nclient unexpectedly closed TCP connection<br>\n2022-06-29 19:49:05.314 [warning] &lt;0.8955.15&gt; closing AMQP connection &lt;0.8955.15&gt; (127.0.0.1:41702 -&gt; 127.0.0.1:5672, vhost: '/', user: 'zulip'):<br>\nclient unexpectedly closed TCP connection<br>\n2022-06-29 19:49:05.314 [warning] &lt;0.9710.15&gt; closing AMQP connection &lt;0.9710.15&gt; (127.0.0.1:42216 -&gt; 127.0.0.1:5672, vhost: '/', user: 'zulip'):<br>\nclient unexpectedly closed TCP connection</p>",
        "id": 1398522,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656512535
    },
    {
        "content": "<p>i'm getting this warning log from rabbitmq</p>",
        "id": 1398523,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656512557
    },
    {
        "content": "<p>That is interesting. Did you restart django previously as I advised?</p>",
        "id": 1398524,
        "sender_full_name": "Matt",
        "timestamp": 1656512861
    },
    {
        "content": "<p>yes I did</p>",
        "id": 1398525,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656512879
    },
    {
        "content": "<p>are you still seeing the <code>WARN [urllib3.connectionpool] Connection pool is full</code> messages in the django log?</p>",
        "id": 1398526,
        "sender_full_name": "Matt",
        "timestamp": 1656512959
    },
    {
        "content": "<p>now it is not showing in the django log</p>",
        "id": 1398527,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656513007
    },
    {
        "content": "<p>Good, that was annoying me, but I was pretty certain it wasn't the problem <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 1398528,
        "sender_full_name": "Matt",
        "timestamp": 1656513069
    },
    {
        "content": "<p>what would be the issue <span class=\"user-mention\" data-user-id=\"24078\">@Matt Keller</span></p>",
        "id": 1398529,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656513139
    },
    {
        "content": "<p>i keep on getting this upstream prematurely closed connections</p>",
        "id": 1398530,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656513183
    },
    {
        "content": "<p>for that users we are getting \"Internal server error\"</p>",
        "id": 1398531,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656513229
    },
    {
        "content": "<p>Nginx is saying Django is closing connections<br>\nRabbitMQ is saying something - Either Django or Tornado - is closing connections.<br>\nDjango is not saying anything bad...</p>\n<p>I assume you've looked at memory and other system things, and you're not running out of anything?</p>",
        "id": 1398532,
        "sender_full_name": "Matt",
        "timestamp": 1656513553
    },
    {
        "content": "<p>i have checked the server processor and memory utilization it is normal only</p>",
        "id": 1398533,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656513648
    },
    {
        "content": "<p>there is no high utilization at all.</p>",
        "id": 1398534,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656513663
    },
    {
        "content": "<p>Alex and some others will be around in about an hour, and hopefully something here is obvious to them, as I have run out of things to look into.</p>",
        "id": 1398535,
        "sender_full_name": "Matt",
        "timestamp": 1656513894
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"24078\">@Matt Keller</span> if someone help us it will be really great.</p>",
        "id": 1398536,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1656514010
    },
    {
        "content": "<p>can someone help/guide us pls</p>",
        "id": 1398572,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1656523494
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"23436\">@Visvanathan</span> / <span class=\"user-mention\" data-user-id=\"23446\">@Jaganath Srinivasan</span>: You reported something similar in <a href=\"#narrow/stream/31-production-help/topic/upstream.20timed.20out.20.28110.20connection.20timed.20out.29.20while.20reading/near/1345975\">https://chat.zulip.org/#narrow/stream/31-production-help/topic/upstream.20timed.20out.20.28110.20connection.20timed.20out.29.20while.20reading/near/1345975</a></p>\n<p>I don't know of anything related to 5.3 which would have affected this codepath.  Were you seeing this problem before the upgrade, as well?</p>",
        "id": 1398598,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656525899
    },
    {
        "content": "<p>Do <em>all</em> users have this problem on sign-in, or just some of them?  Is sign-in the only time it happens?  How many users do you have?</p>",
        "id": 1398600,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656525968
    },
    {
        "content": "<p>Before upgrade we had no issues at all. After upgrade only we have these issues. All users are affected @Alex</p>",
        "id": 1398714,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1656538416
    },
    {
        "content": "<p>Is these issues are due to upgrade</p>",
        "id": 1398717,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1656538541
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span></p>",
        "id": 1398718,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1656538603
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"12178\">Alex Vandiver</span> <a href=\"#narrow/stream/31-production-help/topic/timeouts.20in.205.2E3/near/1398600\">said</a>:</p>\n<blockquote>\n<p>Do <em>all</em> users have this problem on sign-in, or just some of them?  Is sign-in the only time it happens?  How many users do you have?</p>\n</blockquote>\n<p>Yes, all users are having this problem on sign-in randomly. It is happening only to the sign-in users. 5K users we have.</p>",
        "id": 1398853,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656564439
    },
    {
        "content": "<p>How's the load on the server?  The users who don't have trouble logging in, how long does it take to load the page when they log in?</p>",
        "id": 1398855,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656564685
    },
    {
        "content": "<p><a href=\"/user_uploads/2/25/qnLX3mLVO_jo3H40_z5kCMWw/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/25/qnLX3mLVO_jo3H40_z5kCMWw/image.png\" title=\"image.png\"><img src=\"/user_uploads/2/25/qnLX3mLVO_jo3H40_z5kCMWw/image.png\"></a></div>",
        "id": 1398856,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656564827
    },
    {
        "content": "<p>once i give sign in i got the internal server error page</p>",
        "id": 1398858,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656564912
    },
    {
        "content": "<p>That looks like a failed login, which took 25s to fail.  Zulip kills requests that take longer than 20s to run.</p>",
        "id": 1398859,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656565023
    },
    {
        "content": "<p>What is the load on your server?</p>",
        "id": 1398860,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656565031
    },
    {
        "content": "<p><a href=\"/user_uploads/2/36/qfpQ-IBqeF94NwcvYAIJAQb_/MicrosoftTeams-image-1.png\">MicrosoftTeams-image-1.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/36/qfpQ-IBqeF94NwcvYAIJAQb_/MicrosoftTeams-image-1.png\" title=\"MicrosoftTeams-image-1.png\"><img src=\"/user_uploads/2/36/qfpQ-IBqeF94NwcvYAIJAQb_/MicrosoftTeams-image-1.png\"></a></div>",
        "id": 1398862,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656565328
    },
    {
        "content": "<p>this is my current load</p>",
        "id": 1398863,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656565337
    },
    {
        "content": "<p>That looks like moderately heavy load.  It's possible there's some performance regression in 5.3 that's affecting you but that we've not seen in other places -- or it's possible that you've crossed over into a differnt performance domain for unrelated reasons.</p>\n<p>There are no schema changes between 5.3 and 5.2.  You can try downgrading to 5.2 and see if that resolves anything.  Note that this is a minor security regression, because of CVE-2022-31017 -- you should decide if you care about that before downgrading to 5.2.</p>\n<p>If downgrading to 5.2 fixes things, then we'll want to follow up and see if we can help track down what, specifically, the issue is.</p>",
        "id": 1398868,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656565999
    },
    {
        "content": "<p>Okay <span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> let me try to downgrade and check.</p>",
        "id": 1398875,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656566602
    },
    {
        "content": "<p>Thanks for your time. I will get back once the downgrade is completed.</p>",
        "id": 1398876,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656566652
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> Could you please share the downgrade procedure document or link.</p>",
        "id": 1398878,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656566813
    },
    {
        "content": "<p><a href=\"https://zulip.readthedocs.io/en/latest/production/upgrade-or-modify.html#rolling-back-to-a-prior-version\">https://zulip.readthedocs.io/en/latest/production/upgrade-or-modify.html#rolling-back-to-a-prior-version</a></p>",
        "id": 1398879,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656566843
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span></p>",
        "id": 1398880,
        "sender_full_name": "Visvanathan",
        "timestamp": 1656566869
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"23436\">@Visvanathan</span> / <span class=\"user-mention\" data-user-id=\"23446\">@Jaganath Srinivasan</span>: Did reverting to 5.2 address the issue?</p>",
        "id": 1399998,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1657042829
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> yes we have reverted to 5.2 the issue got resolved but still we are getting this upstream prematurely closed connection error is triggering in /var/log/nginix/error.log</p>",
        "id": 1400373,
        "sender_full_name": "Visvanathan",
        "timestamp": 1657082035
    },
    {
        "content": "<p>Hi, I'm getting this error in zulip/errors.log</p>",
        "id": 1431315,
        "sender_full_name": "Visvanathan",
        "timestamp": 1662548440
    },
    {
        "content": "<p>WARN [urllib3.connectionpool] Connection pool is full, discarding connection: 127.0.0.1. Connection pool size: 10</p>",
        "id": 1431316,
        "sender_full_name": "Visvanathan",
        "timestamp": 1662548682
    },
    {
        "content": "<p>WARN [urllib3.connectionpool] Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f405a086f40&gt;: Failed to establish a new connection: [Errno 110] Connection timed out')': /api/v1/events/internal</p>",
        "id": 1431317,
        "sender_full_name": "Visvanathan",
        "timestamp": 1662548717
    },
    {
        "content": "<p>Due to this issue users are getting internal server error page</p>",
        "id": 1431318,
        "sender_full_name": "Visvanathan",
        "timestamp": 1662548945
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> need your support on this.</p>",
        "id": 1431333,
        "sender_full_name": "Visvanathan",
        "timestamp": 1662551038
    },
    {
        "content": "<p>Hello, I recalled our <a href=\"#narrow/stream/31-production-help/topic/timeouts.20in.205.2E3/near/1398409\">previous conversation</a> about this. Have you restarted django, are there any ERROR in the django.log?</p>",
        "id": 1431338,
        "sender_full_name": "Matt",
        "timestamp": 1662551831
    },
    {
        "content": "<p>The problem last time was fairly high load, as Zulip automatically kills requests that take too long.</p>",
        "id": 1431339,
        "sender_full_name": "Matt",
        "timestamp": 1662551892
    },
    {
        "content": "<p>Hi Matt, there is no error in django.log</p>",
        "id": 1431342,
        "sender_full_name": "Visvanathan",
        "timestamp": 1662552502
    },
    {
        "content": "<p>Something is wrong with your tornado instance - check <code>supervisorctl status</code> to make sure it's running, and <code>tornado.log</code> for errrors.</p>\n<p>Unfortunately, more than that I can't really say right now - I'm on vacation.</p>",
        "id": 1433689,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1662863443
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> Need your help on the issue mentioned by <span class=\"user-mention\" data-user-id=\"23436\">@Visvanathan</span></p>",
        "id": 1433999,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1662971850
    },
    {
        "content": "<p>tornado is running.</p>",
        "id": 1434088,
        "sender_full_name": "Visvanathan",
        "timestamp": 1662995837
    },
    {
        "content": "<p>Errors in tornado.log?</p>",
        "id": 1434099,
        "sender_full_name": "Matt",
        "timestamp": 1662998215
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"24078\">@Matt Keller</span> there is no error logs only Info log is there.</p>",
        "id": 1434101,
        "sender_full_name": "Visvanathan",
        "timestamp": 1662998567
    },
    {
        "content": "<p>We have problem with all users . We upgraded to 5.5.</p>",
        "id": 1434549,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1663061929
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"24078\">@Matt Keller</span>  Can u pls help us on this. We are really struggling</p>",
        "id": 1434605,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1663074642
    },
    {
        "content": "<p>We are unable to downgrade as well</p>",
        "id": 1434606,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1663074665
    },
    {
        "content": "<p>Re-reading our <a href=\"#narrow/stream/31-production-help/topic/timeouts.20in.205.2E3/near/1398862\">previous conversation</a> from June, you're running on a busy server that is probably doing more than just Zulip. Are you running Zulip in a Virtual Machine? Container? Have you tried restarting it?</p>",
        "id": 1434613,
        "sender_full_name": "Matt",
        "timestamp": 1663075587
    },
    {
        "content": "<p>We are running on VM Environment. One Host is full dedicated to Zulip. We restarted Zulip VM. Not the Complete Host.</p>",
        "id": 1434614,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1663075647
    },
    {
        "content": "<p>I mean no other VM running in Same machine</p>",
        "id": 1434615,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1663075665
    },
    {
        "content": "<p>I recommend doing through the <a href=\"https://zulip.readthedocs.io/en/latest/production/troubleshooting.html\">Troubleshooting Guide</a> and pay particular attention to services and checking their log files. It's difficult for me to imagine a scenario where you are getting Internal Server Errors without any errors being logged anywhere.</p>",
        "id": 1434618,
        "sender_full_name": "Matt",
        "timestamp": 1663075885
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span> can u pls help us on this issues</p>",
        "id": 1435128,
        "sender_full_name": "Jaganath Srinivasan",
        "timestamp": 1663133160
    }
]