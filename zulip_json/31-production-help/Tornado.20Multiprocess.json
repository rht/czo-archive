[
    {
        "content": "<p>Is tornado multiprocessing available in 1.9.1?<br>\nIt seems to add the tornado_processes value to the application_server section of the zulip.conf configuration file and then run the puppet-apply script.<br>\nBut I did not find clear information in the guide document. So ask if it is actually available.</p>",
        "id": 683280,
        "sender_full_name": "YI Jeong",
        "timestamp": 1547665931
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"3667\">@YI Jeong</span> it is, though not documented because it requires a bit of a fork to use and it's not useful if you only have one organization, since the current setup shards by realm.</p>",
        "id": 683284,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1547666259
    },
    {
        "content": "<p>Okay.. Each process is responsible for each realm.<br>\nSo, unfortunately, it will not help us. thank you.</p>",
        "id": 683287,
        "sender_full_name": "YI Jeong",
        "timestamp": 1547666680
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"3667\">@YI Jeong</span> how large is your organization?</p>",
        "id": 683289,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1547666767
    },
    {
        "content": "<p>We have done considerable work towards the single-realm version of that, but it's not in a state where one can use it without collaboration from the core team (available via a support contract)</p>",
        "id": 683290,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1547666796
    },
    {
        "content": "<p>But if your organization is not huge, you may not need it.</p>",
        "id": 683291,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1547666805
    },
    {
        "content": "<p>Our organization is not large.<br>\nSo I think that it will be possible to solve it simply (some setting change, tuning), but it is not easy. <span class=\"emoji emoji-1f644\" title=\"rolling eyes\">:rolling_eyes:</span> <br>\n<a href=\"user_uploads/2/71/OhqY21kNWghUiAbWPekNlhMG/pasted_image.png\" target=\"_blank\" title=\"user_uploads/2/71/OhqY21kNWghUiAbWPekNlhMG/pasted_image.png\">pasted image</a><br>\n<a href=\"user_uploads/2/70/LB8O-jBYwd9q-su3coQcnKVg/pasted_image.png\" target=\"_blank\" title=\"user_uploads/2/70/LB8O-jBYwd9q-su3coQcnKVg/pasted_image.png\">pasted image</a></p>\n<div class=\"message_inline_image\"><a href=\"user_uploads/2/71/OhqY21kNWghUiAbWPekNlhMG/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"user_uploads/2/71/OhqY21kNWghUiAbWPekNlhMG/pasted_image.png\"></a></div><div class=\"message_inline_image\"><a href=\"user_uploads/2/70/LB8O-jBYwd9q-su3coQcnKVg/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"user_uploads/2/70/LB8O-jBYwd9q-su3coQcnKVg/pasted_image.png\"></a></div>",
        "id": 683297,
        "sender_full_name": "YI Jeong",
        "timestamp": 1547667138
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"3667\">@YI Jeong</span> are you having a Tornado performance problem?  I would not expect one with that many users.</p>",
        "id": 683436,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1547675894
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span> <br>\nThe tornado process used a lot of resources, and the logs looked like it might need to load-balance.</p>\n<div class=\"codehilite\"><pre><span></span>2019-01-17 05:16:57.665 INFO [] Tornado  65.9% busy over the past 43.2 seconds\n2019-01-17 05:17:53.161 INFO [] Tornado  96.7% busy over the past 55.5 seconds\n2019-01-17 05:17:58.187 INFO [] Tornado  90.9% busy over the past 59.6 seconds\n</pre></div>",
        "id": 683521,
        "sender_full_name": "YI Jeong",
        "timestamp": 1547702469
    },
    {
        "content": "<p>What does 'lp' mean in the log below?</p>\n<div class=\"codehilite\"><pre><span></span>2019-01-17 05:16:15.658 INFO [zr] 123.123.123.123   GET     200   6ms (lp: 5.7s) (db: 1ms/1q) /json/events [1547669980:469/1/message] (ID@domain via ZulipElectron)\n</pre></div>",
        "id": 683522,
        "sender_full_name": "YI Jeong",
        "timestamp": 1547702623
    },
    {
        "content": "<p>The problem seems to be simple.<br>\nIn the idle state, Zulip is no problem at all, but when there are more users (about 300 users), message transmission is delayed. However, the CPU, memory, and IO resources of the server are in a sufficient state. (Except Tornado?)</p>\n<p>I think I just need to find where this delay occurs.<br>\nFirst, I will migrate the data after a clean install and monitor performance once again.</p>\n<p>top</p>\n<div class=\"codehilite\"><pre><span></span>Tasks: 200 total,   3 running, 197 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 28.0 us,  2.9 sy,  0.0 ni, 67.6 id,  0.3 wa,  0.0 hi,  1.3 si,  0.0 st\nKiB Mem : 16421148 total,  1455076 free,  3949104 used, 11016968 buff/cache\nKiB Swap:  4189180 total,  4094396 free,    94784 used. 11964092 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n43750 zulip     20   0  496392 263224  21304 R  93.0  1.6 182:33.04 python3 /home/zulip/deployments/current/manage.py runtornado 127.0.0.1:9993\n43768 zulip     20   0  337780 105252  21432 S  59.3  0.6  87:14.66 python3 /home/zulip/deployments/current/manage.py process_queue --queue_name=user_activity\n43891 zulip     20   0  498480 151156  18820 R  21.5  0.9  50:43.77 /home/zulip/deployments/current/zulip-current-venv/bin/uwsgi --ini /etc/zulip/uwsgi.ini\n 1175 rabbitmq  20   0 4480968 121528   5240 S  15.9  0.7 229:59.57 /usr/lib/erlang/erts-7.3/bin/beam.smp -W w -A 64 -P 1048576 -K true -B i -- -root /usr/lib/erlang -progname erl -- -home /var/lib/rabbitmq -- -pa /usr/lib/rabbitmq/li+\n  854 root      20   0   61240  13712   7024 S   8.9  0.1  88:29.59 /usr/bin/python /usr/bin/supervisord -n -c /etc/supervisor/supervisord.conf\n43890 zulip     20   0  502140 155064  19452 S   8.9  0.9  31:19.92 /home/zulip/deployments/current/zulip-current-venv/bin/uwsgi --ini /etc/zulip/uwsgi.ini\n43759 zulip     20   0  337656 105160  21688 S   7.0  0.6  15:46.27 python3 /home/zulip/deployments/current/manage.py process_queue --queue_name=user_presence\n43889 zulip     20   0  495636 148672  19452 S   7.0  0.9  20:37.46 /home/zulip/deployments/current/zulip-current-venv/bin/uwsgi --ini /etc/zulip/uwsgi.ini\n 1041 redis     20   0   57800  22712   2124 S   6.6  0.1  88:44.13 /usr/bin/redis-server 127.0.0.1:6379\n43888 zulip     20   0  507652 159044  17940 S   6.6  1.0  15:10.69 /home/zulip/deployments/current/zulip-current-venv/bin/uwsgi --ini /etc/zulip/uwsgi.ini\n  862 nobody    20   0  748976 304996   2128 S   4.6  1.9  55:45.23 /usr/bin/memcached -m 996 -p 11211 -u nobody -l 127.0.0.1 -o slab_reassign,slab_automove,lru_crawler,lru_maintainer,maxconns_fast,hash_algorithm=murmur3\n43886 zulip     20   0  506044 157104  17612 S   4.0  1.0  10:23.54 /home/zulip/deployments/current/zulip-current-venv/bin/uwsgi --ini /etc/zulip/uwsgi.ini\n43887 zulip     20   0  509656 161036  17632 S   3.6  1.0  12:01.58 /home/zulip/deployments/current/zulip-current-venv/bin/uwsgi --ini /etc/zulip/uwsgi.ini\n 1151 zulip     20   0  162876  27636   4724 S   1.7  0.2  11:42.81 nginx: worker process\n 1153 zulip     20   0  161860  26692   4692 S   1.7  0.2  11:14.08 nginx: worker process\n 1148 zulip     20   0  162024  26920   4724 S   1.3  0.2  11:15.86 nginx: worker process\n 1149 zulip     20   0  162668  27448   4708 S   1.3  0.2  12:06.62 nginx: worker process\n 1150 zulip     20   0  162660  27464   4716 S   1.0  0.2  11:47.04 nginx: worker process\n 1146 zulip     20   0  162592  27632   4720 S   0.7  0.2  11:17.58 nginx: worker process\n 1147 zulip     20   0  162532  27604   4716 S   0.7  0.2  11:59.25 nginx: worker process\n 1152 zulip     20   0  162664  27580   4724 S   0.7  0.2  11:21.92 nginx: worker process\n43765 zulip     20   0  337644 105096  21656 S   0.7  0.6   1:40.15 python3 /home/zulip/deployments/current/manage.py process_queue --queue_name=user_activity_interval\n    7 root      20   0       0      0      0 S   0.3  0.0   8:22.70 [rcu_sched]\n 1130 snmp      20   0   67828   4444   2588 S   0.3  0.0   3:25.79 /usr/sbin/snmpd -Lsd -Lf /dev/null -u snmp -g snmp -I -smux mteTrigger mteTriggerConf -p /run/snmpd.pid\n 1393 root      20   0 1287476  19708   4360 S   0.3  0.1  24:35.61 SMSAGENT\n27069 infra     20   0   96940   4300   3328 S   0.3  0.0   0:00.89 sshd: infra@pts/1\n43758 zulip     20   0  336920 103524  20836 S   0.3  0.6   0:06.50 python3 /home/zulip/deployments/current/manage.py process_queue --queue_name=slow_queries\n</pre></div>\n\n\n<p>server.log</p>\n<div class=\"codehilite\"><pre><span></span>2019-01-17 05:42:03.192 INFO [zr] 123.124.222.100   SOCKET  200   5ms (db: 1ms/1q) /socket/auth [transport=websocket] (usera@domain via ?)\n2019-01-17 05:42:03.195 INFO [zr] 123.123.201.23    GET     200   7ms (lp: 40ms) (db: 1ms/1q) /json/events [1547669980:4/1/message] (userb@domain via ZulipElectron)\n2019-01-17 05:42:03.204 INFO [zr] 123.124.222.100   POST    200   7ms /json/report/send_times [46860ms/9803ms/(unknown)ms/echo:True/diff:False] (usera@domain via website)\n2019-01-17 05:42:03.205 INFO [zr] 123.124.222.161   GET     200   6ms (lp: 40ms) (db: 1ms/1q) /json/events [1547669980:1471/1/message] (userc@domain via ZulipElectron)\n2019-01-17 05:42:03.207 INFO [zr] 123.123.201.23    SOCKET  200   1ms (lp: 120ms) (db: 1ms/1q) /socket/service_request [transport=websocket, queue_delay: 63ms/19ms, service_time: 37ms] (userb@domain via ?)\n2019-01-17 05:42:03.208 INFO [zr] 123.123.201.23    SOCKET  200   0ms (db: 1ms/1q) /socket/close [transport=websocket] (userb@domain via ?)\n2019-01-17 05:42:03.220 INFO [zr] 123.124.226.46    SOCKET  200  12ms (db: 2ms/2q) /socket/auth [transport=websocket] (userd@domain via ?)\n2019-01-17 05:42:03.232 INFO [] Tornado  99.8% busy over the past 54.5 seconds\n2019-01-17 05:42:03.243 INFO [zr] 123.124.226.46    POST    200   7ms /json/report/send_times [42606ms/15268ms/(unknown)ms/echo:True/diff:False] (userd@domain via ZulipElectron)\n2019-01-17 05:42:03.272 INFO [zr] 123.124.222.24    GET     200   9ms (lp: 1.6s) /json/events [1547669980:2326/1/message] (usere@domain via website)\n2019-01-17 05:42:03.272 INFO [zr] 123.124.226.76    POST    200   7ms /json/report/narrow_times [51ms/297ms/297ms] (userf@domain via website)\n2019-01-17 05:42:03.274 INFO [zr] 123.123.100.28    GET     200   6ms (lp: 240ms) /api/v1/events [1547669980:2784/1/message] (chat-bot@domain via ZulipPython)\n2019-01-17 05:42:03.276 INFO [zr] 123.124.222.23    GET     200   6ms (lp: 1.2s) /json/events [1547669980:31/1/message] (userg@domain via website)\n2019-01-17 05:42:03.279 INFO [zr] 123.124.222.18    GET     200   7ms (lp: 165ms) /json/events [1547669980:99/1/message] (userh@domain via website)\n2019-01-17 05:42:03.280 INFO [zr] 123.124.222.101   GET     200   8ms (lp: 608ms) /json/events [1547669980:53/1/message] (useri@domain via ZulipElectron)\n2019-01-17 05:42:03.283 INFO [zr] 123.124.222.17    GET     200   7ms (lp: 1.0s) /json/events [1547669980:1230/1/message] (userj@domain via ZulipElectron)\n2019-01-17 05:42:03.289 INFO [zr] 123.124.222.102   GET     200   6ms (lp: 5.2s) /json/events [1547669980:87/1/message] (userk@domain via ZulipElectron)\n2019-01-17 05:42:03.293 INFO [zr] 123.124.222.100   GET     200  10ms (lp: 169ms) /json/events [1547669980:2482/1/message] (usera@domain via website)\n2019-01-17 05:42:03.298 INFO [zr] 123.124.222.40    GET     200   6ms (lp: 1.0s) /json/events [1547669980:1923/1/message] (userl@domain via ZulipElectron)\n2019-01-17 05:42:03.301 INFO [zr] 123.124.222.100   SOCKET  200   1ms (lp: 78ms) /socket/service_request [transport=websocket, queue_delay: 5ms/36ms, service_time: 36ms] (usera@domain via ?)\n2019-01-17 05:42:03.316 INFO [zr] 123.124.222.100   SOCKET  200   1ms (lp: 91ms) /socket/service_request [transport=websocket, queue_delay: 4ms/41ms, service_time: 45ms] (usera@domain via ?)\n2019-01-17 05:42:03.319 INFO [zr] 123.124.222.100   SOCKET  200   6ms (lp: 93ms) /socket/service_request [transport=websocket, queue_delay: 6ms/22ms, service_time: 60ms] (usera@domain via ?)\n2019-01-17 05:42:03.328 INFO [zr] 127.0.0.1       POST    200   3ms /api/v1/events/internal [1547669980:2785/0] (chat-bot@domain via internal)\n2019-01-17 05:42:03.341 INFO [zr] 123.124.222.23    GET     200   6ms /json/events [1547669980:31/2] (userg@domain via website)\n2019-01-17 05:42:03.353 INFO [zr] 123.124.222.17    GET     200   6ms /json/events [1547669980:1230/2] (userj@domain via ZulipElectron)\n2019-01-17 05:42:03.355 INFO [zr] 123.124.222.100   POST    200  14ms /json/report/send_times [39464ms/39423ms/(unknown)ms/echo:True/diff:False] (usera@domain via website)\n2019-01-17 05:42:03.363 INFO [zr] 127.0.0.1       POST    200   3ms /api/v1/events/internal [1547669980:2785/0] (chat-bot@domain via internal)\n</pre></div>",
        "id": 683524,
        "sender_full_name": "YI Jeong",
        "timestamp": 1547704228
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"3667\">@YI Jeong</span> OK, thanks for sharing that data.  It does seem you are having Tornado load issues.  I'm a bit puzzled why the average request processing times are so long.  I'm about to head out on a trip but I can do some investigation next week.</p>",
        "id": 683766,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1547753422
    }
]