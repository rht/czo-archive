[
    {
        "content": "<p>Hi, we are test deploying zulip in our org. The pilot team that's trying it is only about 20 members.</p>\n<p>We are experiencing constant 80-90 &amp; RAM usage over the course of a week. </p>\n<p>According to <a href=\"https://zulip.readthedocs.io/en/stable/production/requirements.html#hardware-specifications\">zulip doc on hardware</a> 2 CPUs and 4GB RAM should be plenty for 20 users. </p>\n<p>We are using <a href=\"https://cloud.google.com/blog/products/compute/google-compute-engine-gets-new-e2-vm-machine-types\">GCP E2 machine type</a>, so not sure if that would cause performance issues.</p>\n<p>What's the machine spec and number of users that others use?</p>",
        "id": 1076952,
        "sender_full_name": "Sean Yuan",
        "timestamp": 1607415543
    },
    {
        "content": "<p><a href=\"/user_uploads/2/38/4uUdnv5sW7ZU20H5tfb7nsUs/Screenshot-from-2020-12-08-16-16-36.png\">Screenshot-from-2020-12-08-16-16-36.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/38/4uUdnv5sW7ZU20H5tfb7nsUs/Screenshot-from-2020-12-08-16-16-36.png\" title=\"Screenshot-from-2020-12-08-16-16-36.png\"><img src=\"/user_uploads/2/38/4uUdnv5sW7ZU20H5tfb7nsUs/Screenshot-from-2020-12-08-16-16-36.png\"></a></div>",
        "id": 1076953,
        "sender_full_name": "Sean Yuan",
        "timestamp": 1607415644
    },
    {
        "content": "<p>Follow up:</p>\n<p>After upgrading our machine to <br>\n2 CPU<br>\n8 GB RAM</p>\n<p>The CPU spikes were gone, and RAM usage hovered around 40%. <br>\nMaybe the official documentation need to recommend more than 4 GB of RAM?</p>",
        "id": 1077602,
        "sender_full_name": "Sean Yuan",
        "timestamp": 1607483673
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"17540\">@Sean Yuan</span> based on that screenshot, it looks like RabbitMQ was consuming all the CPU; I'm guessing you had some sort of configuration problem with that service that was ultimately responsible for the performance issues you saw, and the upgrade resolved that issue.</p>",
        "id": 1082161,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1608162360
    },
    {
        "content": "<p>(<code>beam.smp</code> is the main process using CPU in that screenshot, at least)</p>",
        "id": 1082162,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1608162377
    },
    {
        "content": "<p>We used the default installer on a single host, so did not configure any settings for rabbitMq</p>",
        "id": 1082280,
        "sender_full_name": "Sean Yuan",
        "timestamp": 1608171501
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"17540\">@Sean Yuan</span> hmm, can you try looking at <code>dmesg</code> to see if you have OOM kills or any other obvious logging problems before increasing resources?</p>",
        "id": 1083863,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1608345020
    },
    {
        "content": "<p>I’ll take a look and see if I can find them</p>",
        "id": 1083929,
        "sender_full_name": "Sean Yuan",
        "timestamp": 1608353806
    },
    {
        "content": "<p>I did a search for \"OOM\" and \"kill\" in the server logs, did not find anything relevant</p>",
        "id": 1084820,
        "sender_full_name": "Sean Yuan",
        "timestamp": 1608531899
    },
    {
        "content": "<p><a href=\"/user_uploads/2/70/To2Is0E31V20l6FSSlhn-D3x/Screen-Shot-2020-12-21-at-2.27.55-PM.png\">Screen-Shot-2020-12-21-at-2.27.55-PM.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/70/To2Is0E31V20l6FSSlhn-D3x/Screen-Shot-2020-12-21-at-2.27.55-PM.png\" title=\"Screen-Shot-2020-12-21-at-2.27.55-PM.png\"><img src=\"/user_uploads/2/70/To2Is0E31V20l6FSSlhn-D3x/Screen-Shot-2020-12-21-at-2.27.55-PM.png\"></a></div>",
        "id": 1084821,
        "sender_full_name": "Sean Yuan",
        "timestamp": 1608532090
    },
    {
        "content": "<p><code>errors.log </code> did have mentions of losing connection to tornado</p>",
        "id": 1084822,
        "sender_full_name": "Sean Yuan",
        "timestamp": 1608532117
    },
    {
        "content": "<p>tornado.log</p>\n<div class=\"codehilite\"><pre><span></span><code>199288:2020-12-08 13:20:01.648 INFO [:9993] Tornado 9993   0.2% busy over the past 44.5 seconds\n199289:2020-12-08 13:20:01.649 INFO [:9993] Client disconnected for queue 1607234662:194 (17 via ZulipElectron)\n199290:2020-12-08 13:20:01.649 INFO [:9993] Client disconnected for queue 1607234662:263 (14 via ZulipElectron)\n199291:2020-12-08 13:20:01.649 INFO [:9993] Client disconnected for queue 1607234662:226 (24 via ZulipElectron)\n199292:2020-12-08 13:20:01.649 INFO [:9993] Client disconnected for queue 1607234662:150 (18 via ZulipElectron)\n199293:2020-12-08 13:20:01.654 INFO [:9993] Client disconnected for queue 1607234662:201 (10 via ZulipElectron)\n199294:2020-12-08 13:20:01.655 INFO [:9993] Client disconnected for queue 1607234662:266 (9 via ZulipElectron)\n199295:2020-12-08 13:20:01.668 INFO [:9993] Client disconnected for queue 1607234662:281 (8 via website)\n199296:2020-12-08 13:20:01.669 INFO [:9993] Client disconnected for queue 1607234662:261 (16 via website)\n199297:2020-12-08 13:20:06.343 WARN [pika.adapters.utils.selector_ioloop_adapter:9993] WRITE indicated on fd=19, but writer callback is None; events=0b11001\n199298:2020-12-08 13:20:06.349 WARN [zulip.queue:9993] TornadoQueueClient lost connection to RabbitMQ, reconnecting in 2 secs...\n199299:2020-12-08 13:20:08.356 INFO [:9993] Tornado 9993   0.2% busy over the past 51.2 seconds\n199300:2020-12-08 13:20:08.357 WARN [zulip.queue:9993] TornadoQueueClient attempting to reconnect to RabbitMQ\n199301:2020-12-08 13:20:08.361 ERR  [pika.adapters.utils.io_services_utils:9993] Socket failed to connect: &lt;socket.socket fd=19, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 42210)&gt;; error=111 (Connection refused)\n199302:2020-12-08 13:20:08.364 ERR  [pika.adapters.utils.connection_workflow:9993] TCP Connection attempt failed: ConnectionRefusedError(111, &#39;Connection refused&#39;); dest=(&lt;AddressFamily.AF_INET: 2&gt;, &lt;SocketKind.SOCK_STREAM: 1&gt;, 6, &#39;&#39;, (&#39;127.0.0.1&#39;, 5672))\n199303:2020-12-08 13:20:08.365 ERR  [pika.adapters.utils.connection_workflow:9993] AMQPConnector - reporting failure: AMQPConnectorSocketConnectError: ConnectionRefusedError(111, &#39;Connection refused&#39;)\n199304:2020-12-08 13:20:08.366 ERR  [pika.adapters.utils.connection_workflow:9993] AMQP connection workflow failed: AMQPConnectionWorkflowFailed: 1 exceptions in all; last exception - AMQPConnectorSocketConnectError: ConnectionRefusedError(111, &#39;Connection refused&#39;); first exception - None.\n199305:2020-12-08 13:20:08.367 ERR  [pika.adapters.utils.connection_workflow:9993] AMQPConnectionWorkflow - reporting failure: AMQPConnectionWorkflowFailed: 1 exceptions in all; last exception - AMQPConnectorSocketConnectError: ConnectionRefusedError(111, &#39;Connection refused&#39;); first exception - None\n199306:2020-12-08 13:20:08.367 ERR  [pika.adapters.base_connection:9993] Full-stack connection workflow failed: AMQPConnectionWorkflowFailed: 1 exceptions in all; last exception - AMQPConnectorSocketConnectError: ConnectionRefusedError(111, &#39;Connection refused&#39;); first exception - None\n199307:2020-12-08 13:20:08.367 ERR  [pika.adapters.base_connection:9993] Self-initiated stack bring-up failed: AMQPConnectionError: (AMQPConnectionWorkflowFailed: 1 exceptions in all; last exception - AMQPConnectorSocketConnectError: ConnectionRefusedError(111, &#39;Connection refused&#39;); first exception - None,)\n199308:2020-12-08 13:20:08.367 WARN [zulip.queue:9993] TornadoQueueClient couldn&#39;t connect to RabbitMQ, retrying in 2 secs...\n199309:2020-12-08 13:20:10.218 ERR  [pika.connection:9993] Illegal close(200, &#39;Normal shutdown&#39;) request on &lt;ExceptionFreeTornadoConnection CLOSED transport=None params=&lt;ConnectionParameters host=127.0.0.1 port=5672 virtual_host=/ ssl=False&gt;&gt; because it was called while connection state=CLOSED.\n199319:2020-12-08 13:20:10.237 INFO [:9993] Tornado 9993 dumped 8 event queues in 0.004s\n</code></pre></div>",
        "id": 1084824,
        "sender_full_name": "Sean Yuan",
        "timestamp": 1608532490
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"17540\">@Sean Yuan</span> those logs look to be normal around the time a server is restarted.</p>",
        "id": 1085162,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1608583462
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"17540\">@Sean Yuan</span> An OOM kill would show up not in the server logs, but in the kernel log (<code>dmesg</code>). Here’s an <a href=\"https://unix.stackexchange.com/questions/103792/how-do-i-read-the-output-of-dmesg-to-determine-how-much-memory-a-process-is-us\">example</a> if what that would look like.</p>",
        "id": 1085335,
        "sender_full_name": "Anders Kaseorg",
        "timestamp": 1608590713
    },
    {
        "content": "<p>I'm afraid the machine had been restarted after the upgrade, so there was no sign of OOM kill in the log. <br>\nThank you for the debugging efforts though</p>",
        "id": 1085392,
        "sender_full_name": "Sean Yuan",
        "timestamp": 1608602437
    }
]