[
    {
        "content": "<p>Hi, i just give it a next try to import Rocketchat into Zulip. Unfortunally i run in the following failure while convertig the data:</p>\n<p>~/deployments/current$ ./manage.py convert_rocketchat_data /home/zulip/rocketchat --output /home/zulip/converted_rocketchat_data<br>\nConverting Data ...<br>\n2022-06-03 14:28:46.344 INFO [] Starting to process custom emoji<br>\n2022-06-03 14:28:46.350 INFO [] Done processing emoji<br>\nTraceback (most recent call last):<br>\n  File \"./manage.py\", line 157, in &lt;module&gt;<br>\n    execute_from_command_line(sys.argv)<br>\n  File \"./manage.py\", line 122, in execute_from_command_line<br>\n    utility.execute()<br>\n  File \"/srv/zulip-venv-cache/726c7d1d35d9cd61bb4b70e98167a1db7b0c524d/zulip-py3-venv/lib/python3.8/site-packages/django/core/management/__init__.py\", line 413, in execute<br>\n    self.fetch_command(subcommand).run_from_argv(self.argv)<br>\n  File \"/srv/zulip-venv-cache/726c7d1d35d9cd61bb4b70e98167a1db7b0c524d/zulip-py3-venv/lib/python3.8/site-packages/django/core/management/base.py\", line 354, in run_from_argv<br>\n    self.execute(*args, **cmd_options)<br>\n  File \"/srv/zulip-venv-cache/726c7d1d35d9cd61bb4b70e98167a1db7b0c524d/zulip-py3-venv/lib/python3.8/site-packages/django/core/management/base.py\", line 398, in execute<br>\n    output = self.handle(*args, **options)<br>\n  File \"/home/zulip/deployments/2022-06-03-15-16-06/zerver/management/commands/convert_rocketchat_data.py\", line 45, in handle<br>\n    do_convert_data(rocketchat_data_dir=data_dir, output_dir=output_dir)<br>\n  File \"/home/zulip/deployments/2022-06-03-15-16-06/zerver/data_import/rocketchat.py\", line 1072, in do_convert_data<br>\n    upload_id_to_upload_data_map = map_upload_id_to_upload_data(rocketchat_data[\"upload\"])<br>\n  File \"/home/zulip/deployments/2022-06-03-15-16-06/zerver/data_import/rocketchat.py\", line 768, in map_upload_id_to_upload_data<br>\n    upload_id_to_upload_data_map[chunk[\"files_id\"]][\"chunk\"].append(chunk[\"data\"])<br>\nKeyError: 'HkdjnryAAL6FmHXza'</p>\n<p>Any help would be welcome!</p>",
        "id": 1388070,
        "sender_full_name": "felix mendoline",
        "timestamp": 1654267780
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"9\" href=\"/#narrow/stream/9-issues/topic/Import.20Rocketchat.20data\">#issues &gt; Import Rocketchat data</a> by <span class=\"user-mention silent\" data-user-id=\"19257\">Alya Abbott</span>.</p>",
        "id": 1388249,
        "sender_full_name": "Notification Bot",
        "timestamp": 1654284679
    },
    {
        "content": "<p>That says something is unexpected with the Rocketchat's export of its file uploads.  Specifically, it got a \"chunk\" of a file which it didn't provide metadata for.</p>",
        "id": 1388270,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1654286034
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"22138\">@felix mendoline</span>: Is there anything odd about the export in terms of file uploads?</p>\n<p>Failing that, we'd need to see the export data, or a reproducer, to debug further.</p>",
        "id": 1388271,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1654286087
    },
    {
        "content": "<p>Hi, file size is expected a restore in Rocketchat works...so I guess it is fine. I will take a new Backup and try again. Thanks for your help!</p>",
        "id": 1388540,
        "sender_full_name": "felix mendoline",
        "timestamp": 1654362944
    },
    {
        "content": "<p>ok, i now tried several times again and make sure files are not corrupted, but i always get the same error:</p>\n<p>Converting Data ...<br>\n2022-06-15 13:48:55.661 INFO [] Starting to process custom emoji<br>\n2022-06-15 13:48:55.794 INFO [] Done processing emoji<br>\nTraceback (most recent call last):<br>\n  File \"./manage.py\", line 157, in &lt;module&gt;<br>\n    execute_from_command_line(sys.argv)<br>\n  File \"./manage.py\", line 122, in execute_from_command_line<br>\n    utility.execute()<br>\n  File \"/srv/zulip-venv-cache/726c7d1d35d9cd61bb4b70e98167a1db7b0c524d/zulip-py3-venv/lib/python3.8/site-packages/django/core/management/__init__.py\", line 413, in execute<br>\n    self.fetch_command(subcommand).run_from_argv(self.argv)<br>\n  File \"/srv/zulip-venv-cache/726c7d1d35d9cd61bb4b70e98167a1db7b0c524d/zulip-py3-venv/lib/python3.8/site-packages/django/core/management/base.py\", line 354, in run_from_argv<br>\n    self.execute(*args, **cmd_options)<br>\n  File \"/srv/zulip-venv-cache/726c7d1d35d9cd61bb4b70e98167a1db7b0c524d/zulip-py3-venv/lib/python3.8/site-packages/django/core/management/base.py\", line 398, in execute<br>\n    output = self.handle(*args, **options)<br>\n  File \"/home/zulip/deployments/2022-06-03-15-16-06/zerver/management/commands/convert_rocketchat_data.py\", line 45, in handle<br>\n    do_convert_data(rocketchat_data_dir=data_dir, output_dir=output_dir)<br>\n  File \"/home/zulip/deployments/2022-06-03-15-16-06/zerver/data_import/rocketchat.py\", line 1072, in do_convert_data<br>\n    upload_id_to_upload_data_map = map_upload_id_to_upload_data(rocketchat_data[\"upload\"])<br>\n  File \"/home/zulip/deployments/2022-06-03-15-16-06/zerver/data_import/rocketchat.py\", line 768, in map_upload_id_to_upload_data<br>\n    upload_id_to_upload_data_map[chunk[\"files_id\"]][\"chunk\"].append(chunk[\"data\"])<br>\nKeyError: 'HkdjnryAAL6FmHXza'</p>\n<p>Any idea?</p>",
        "id": 1392734,
        "sender_full_name": "felix mendoline",
        "timestamp": 1655301062
    },
    {
        "content": "<p>Hi, i still do not get it to work and run out of ideas. If some one knows a developer / supporter who could debug the problem in detail and offer pyed help....i would be glad for any contact. Thanks!</p>",
        "id": 1393996,
        "sender_full_name": "felix mendoline",
        "timestamp": 1655749278
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"22138\">@felix mendoline</span>: I think we're happy to help get it fixed, but as I noted above, we need to have the example Rocketchat data to examine, so we can understand how it expects us to handle it.</p>\n<p>If you're amenable, you can email me a copy, and I'll see if I can distill what the issue is so we can make progress on fixing it.</p>",
        "id": 1395625,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656008225
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> thanks a lot. My problem is, that the Rocketchat data is about 8GB compressed. So email is maybe not a easy way. I could provide you a server (dedicated testsystem) where the data is located and you could use the server to debug it. Would this be a option?</p>",
        "id": 1396288,
        "sender_full_name": "felix mendoline",
        "timestamp": 1656105788
    },
    {
        "content": "<p>Directly logging into other folks' production servers is a little further than we're willing to go, unfortunately.  Do you have some non-email way (Dropbox, etc) to share a copy of the data?</p>",
        "id": 1396290,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656106064
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"12178\">@Alex Vandiver</span> As i say it is not a production system it would be dedicated only for you...which you could destroy in anyway :) But if this do not work i ll can arrange a FTP server to provide the data.</p>",
        "id": 1396296,
        "sender_full_name": "felix mendoline",
        "timestamp": 1656106603
    },
    {
        "content": "<p>Test server or not, I think logging in is further than we're willing to go.</p>\n<p>I'll send you a link you can use to upload the data.</p>",
        "id": 1396364,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1656110132
    },
    {
        "content": "<p>Cool, thanks!</p>",
        "id": 1398959,
        "sender_full_name": "felix mendoline",
        "timestamp": 1656591140
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"22138\">@felix mendoline</span>: There were multiple odd data hiccups in that database.  My suspicion is this is mongodb being ... special, and not a misunderstanding of the underlying schema, but I've adjusted the importer to be more forgiving of really weird data.</p>\n<p>Pushed <a href=\"https://github.com/zulip/zulip/pull/22486\">#22486</a>.</p>",
        "id": 1405127,
        "sender_full_name": "Alex Vandiver",
        "timestamp": 1657845512
    },
    {
        "content": "<p>hi. I'm also currently working on importing our production rocketchat data and I've run into a few more and mostly different issues than you. a few of my commits are likely good to merge so if anyone wants to take a look, feel free: <a href=\"https://github.com/zulip/zulip/pull/22546\">https://github.com/zulip/zulip/pull/22546</a></p>",
        "id": 1408106,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1658417223
    },
    {
        "content": "<p>I'm also seeing some issues that I haven't fully analysed yet, but maybe one of you guys has some input:</p>\n<ul>\n<li>discussions in rocket chat are basically their own rooms, but they are attached to a parent room. the converter imports them as threads in a stream which means that if the discussion had a set of members that differed form the parent room, those members seem to be ignored by the converter and thus they will no longer be able to read their old messages. similarly, rocketchat threads, which would usually map to threads in zulip, are all mapped to a single thread in the parent room/stream. I'd love to get these RC discussion imported as dedicated streams instead of threads and RC threads as threads in that stream, but I don't yet know how difficult that is.</li>\n<li>if a message has been edited in RC, it is imported multiple times in zulip and all of them show at once. I'd like to either show the messages with their edit history, or better (since RC didn't have such a visible history) only import the latest version of the message. again, no idea how difficult that would be yet</li>\n</ul>",
        "id": 1408108,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1658417674
    },
    {
        "content": "<p>on another note, the conversion of 10 million messages (20gb mongodb dump) requires around 60GB of memory and takes about an hour. the import of the result with a semi-tuned, but resource constrained, database takes around 48 hours, most of which (like 30-40 hours)  is spent at the very end of the import script without any status output. I'm not sure if there is anything to be done about that, but it does make the whole thing quite slow to test/adjust/fix. I haven't monitored resource usage during the import yet, so it might also just be some bottleneck somewhere.</p>",
        "id": 1408114,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1658418892
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"23931\">@Florian Pritz</span> hey, thanks for working on this!</p>",
        "id": 1408239,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658432882
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"23931\">Florian Pritz</span> <a href=\"#narrow/stream/31-production-help/topic/Import.20Rocketchat.20data/near/1408114\">said</a>:</p>\n<blockquote>\n<p>On another note, the conversion of 10 million messages (20gb mongodb dump) requires around 60GB of memory and takes about an hour.</p>\n</blockquote>\n<p>For this part, I'm pretty sure the issue is that the Python MongoDB library we're using to access the dumps, <code>bson</code>, loads the entire MongoDB dump into memory. This is <a href=\"https://github.com/zulip/zulip/pull/20295\">#20295</a> in the Zulip issue tracker. I wouldn't be surprised if there's ways we can work around this structure, but the core issue is that Rocket.Chat's export format doesn't make this part easy.</p>",
        "id": 1408240,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658433210
    },
    {
        "content": "<p>I'm surprised by the report of the import tool taking 30-40 hours to run without output. Can you give the log lines before/after the long period? That will be helpful for debugging. I have a theory that the issue might be that we're not making use of the <code>long_term_idle</code> feature in the Rocket.Chat export process, like we do in Zulip and Slack exports.</p>",
        "id": 1408241,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658433302
    },
    {
        "content": "<p>BTW, <span class=\"user-mention\" data-user-id=\"14114\">@Priyansh Garg (garg3133)</span> FYI as the original author of the tool.</p>",
        "id": 1408243,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658433357
    },
    {
        "content": "<p>The issues with how Rocket.Chat threads and conversations are mapped seem like things that could be readily mapped differently -- so we'd be very happy to work with you on putting together a revised plan. </p>\n<p>For having a good edit/refresh testing cycle, I'd consider making a new Rocket.Chat instance and just putting a few \"threads\" and \"conversations\" in it to work with; I'm pretty sure that approach is how we tested the logic for the Rocket.Chat importer.</p>",
        "id": 1408246,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658433480
    },
    {
        "content": "<p>Posted an initial review on your PR; I really appreciate the clean commit series!</p>",
        "id": 1408266,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658434382
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"7\">Tim Abbott</span> <a href=\"#narrow/stream/31-production-help/topic/Import.20Rocketchat.20data/near/1408241\">said</a>:</p>\n<blockquote>\n<p>I'm surprised by the report of the import tool taking 30-40 hours to run without output. Can you give the log lines before/after the long period? That will be helpful for debugging. I have a theory that the issue might be that we're not making use of the <code>long_term_idle</code> feature in the Rocket.Chat export process, like we do in Zulip and Slack exports.</p>\n</blockquote>\n<p>All I see right now in an old log is this, which does not show any output for about 37 hours after the messages were imported.</p>\n<div class=\"codehilite\"><pre><span></span><code>2022-07-05 15:59:10.013 INFO [] Successfully imported &lt;class &#39;zerver.models.Message&#39;&gt; from zerver_message.\n2022-07-05 15:59:10.080 INFO [] Successfully imported &lt;class &#39;zerver.models.UserMessage&#39;&gt; from zerver_usermessage[9872].\n2022-07-05 16:00:25.141 INFO [] Successfully imported &lt;class &#39;zerver.models.Reaction&#39;&gt; from zerver_reaction.\n2022-07-07 05:21:05.362 INFO [] Importing attachment data from /data/converted-rocketchat-data-PROD-v5/attachment.json\n2022-07-07 05:21:10.068 INFO [] Successfully imported &lt;class &#39;zerver.models.Attachment&#39;&gt; from zerver_attachment.\n2022-07-07 05:21:26.163 INFO [] Successfully imported M2M table zerver_attachment_messages\nChecking the system bots.\nFinished importing system bots.\n</code></pre></div>\n<p>The log ends after this last line and the import was completed.</p>",
        "id": 1408732,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1658490341
    },
    {
        "content": "<p>As for testing with a new RC instance: Yeah, I'll do that. I just gotta set up another Zulip instance to test on since I don't want to wipe the one that currently contains the imported prod data. Can't yet say when I'll have time to do that</p>",
        "id": 1408733,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1658490543
    },
    {
        "content": "<p>Cool, yeah, so it's between the reaction import and the attachment data step.</p>",
        "id": 1408866,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658511941
    },
    {
        "content": "<p>So, the only block of code in between those two points is this:</p>\n<div class=\"codehilite\"><pre><span></span><code>    # Similarly, we need to recalculate the first_message_id for stream objects.\n    for stream in Stream.objects.filter(realm=realm):\n        recipient = Recipient.objects.get(type=Recipient.STREAM, type_id=stream.id)\n        first_message = Message.objects.filter(recipient=recipient).first()\n        if first_message is None:\n            stream.first_message_id = None\n        else:\n            stream.first_message_id = first_message.id\n        stream.save(update_fields=[&quot;first_message_id&quot;])\n</code></pre></div>",
        "id": 1408867,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658512027
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"23931\">@Florian Pritz</span> I think you can try running one iteration of that loop on your instance and see if it takes a long time, if so, grab the postgres query from <code>/var/log/postgresql</code> (all queries taking &gt;500ms I think get logged there) and do an <code>EXPLAIN</code> on it.</p>",
        "id": 1408868,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658512088
    },
    {
        "content": "<p>I wonder if this is something really silly, like <code>Message.objects.filter(recipient=recipient).first()</code> should be <code>Message.objects.filter(recipient=recipient).order_by(\"id\").first()</code> in order to ensure it uses an index.</p>",
        "id": 1408871,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658512113
    },
    {
        "content": "<p>I don't have any theory for how that could be slow other than it is doing a table scan of the message table for every stream.</p>",
        "id": 1408872,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658512200
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span> Thanks for the pointer. That line actually runs for about 50 seconds for a randomly selected recipient, but using <code>order_by</code> does not make a difference. What might also be relevant here is that my deployment is running in kubernetes, so each component has dedicated cpu/memory limits. My database had 1GB of memory, but I guess that might be too small for this amount of data even though regular zulip usage worked fine. I could reduce the time to around 1.6 seconds by increasing the memory of the postgres container and changing the postgres <code>shared_buffers</code> setting. 1.6 seconds for a lookup of indexed data still feels a bit excessive, but much better than before at least.  Just in case, here's an explain analyze of the query:</p>\n<div class=\"codehilite\"><pre><span></span><code>zulip=# explain analyze SELECT &quot;zerver_message&quot;.&quot;id&quot;, &quot;zerver_message&quot;.&quot;sender_id&quot;, &quot;zerver_message&quot;.&quot;recipient_id&quot;, &quot;zerver_message&quot;.&quot;subject&quot;, &quot;zerver_message&quot;.&quot;content&quot;, &quot;zerver_message&quot;.&quot;rendered_content&quot;, &quot;zerver_message&quot;.&quot;rendered_content_version&quot;, &quot;zerver_message&quot;.&quot;date_sent&quot;, &quot;zerver_message&quot;.&quot;sending_client_id&quot;, &quot;zerver_message&quot;.&quot;last_edit_time&quot;, &quot;zerver_message&quot;.&quot;edit_history&quot;, &quot;zerver_message&quot;.&quot;has_attachment&quot;, &quot;zerver_message&quot;.&quot;has_image&quot;, &quot;zerver_message&quot;.&quot;has_link&quot;, &quot;zerver_message&quot;.&quot;search_tsvector&quot; FROM &quot;zerver_message&quot; WHERE &quot;zerver_message&quot;.&quot;recipient_id&quot; = 5360 ORDER BY &quot;zerver_message&quot;.&quot;id&quot; ASC LIMIT 1;\n                                                                        QUERY PLAN\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\n Limit  (cost=0.43..644.56 rows=1 width=381) (actual time=1602.084..1602.086 rows=1 loops=1)\n   -&gt;  Index Scan using zerver_message_pkey on zerver_message  (cost=0.43..2351703.18 rows=3651 width=381) (actual time=1602.082..1602.082 rows=1 loops=1)\n         Filter: (recipient_id = 5360)\n         Rows Removed by Filter: 3528281\n Planning Time: 0.684 ms\n Execution Time: 1602.151 ms\n(6 rows)\n</code></pre></div>\n<p>Removing the <code>recipient_id=5360</code> filter from the query makes it complete in 0.1ms and also uses the same <code>zerver_message_pkey</code> index which only contains the <code>id</code> column. I'm guessing that postgres thus starts with the lowest <code>id</code> and then tries to find one where the recipient matches. Since the stream in question was probably created quite late, it has to scan 3.5 million rows from other streams before it finds the right one. I added another index <code>create index zerver_message_test_idx on zerver_message (id, recipient_id);</code> and that brings the time down to 60 to 100ms. Am I right in assuming that this operation only happens during an import and not during normal operation? If so, would it make sense to add such an index only during an import and remove it again after? Then again I only have around 1600 streams, so this might not be such a huge time saver now that the database has enough memory. Still 1600 * 2 second is almost one hour. Guess I'll give the index a try on my next big import.</p>",
        "id": 1409921,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1658757258
    },
    {
        "content": "<p>In case it's relevant, I'm using postgres 11.6. Will upgrade in the future, but not right now.</p>",
        "id": 1409952,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1658766180
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"23931\">@Florian Pritz</span> interesting! We have the following indexes on the recipient table:</p>\n<div class=\"codehilite\"><pre><span></span><code>    &quot;zerver_message_recipient_id_5a7b6f03&quot; btree (recipient_id)\n    &quot;zerver_message_recipient_subject&quot; btree (recipient_id, subject, id DESC NULLS LAST)\n</code></pre></div>",
        "id": 1410145,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658782827
    },
    {
        "content": "<p>I'd expect that an index on <code>(recipient_id, id)</code> would make this particular query immediate; but it's not a query that I'd expect to happen often in a normal Zulip server, since generally one's interested in \"oldest message in a topic\", not \"oldest message in a stream\".</p>",
        "id": 1410146,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658782896
    },
    {
        "content": "<p>Still, I'm a bit surprised that the query planner isn't choosing to use the <code>recipient_id</code> index; I wonder if an <code>UPDATE STATISTICS</code> operation before that step would cause it to choose a better query plan without an additional index.</p>",
        "id": 1410147,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1658782937
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span> I haven't yet had time to continue working on all this, but I just figured I'd try a generalized query. In a very quick test, <code>SELECT min(id), recipient_id from zerver_message group by recipient_id;</code> returns the same as the existing query for my test ID (well, except for all the unused fields and the ORM magic) and it also completes in about 30 seconds for the entire table. If I have some more time and nobody else beats me to it, I'll check if the result matches with the imported data, but I think it should. They query is simple enough after all.</p>",
        "id": 1416943,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1659976475
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"23931\">@Florian Pritz</span> I think <code>group by</code> isn't the right query, though -- you want <code>where recipient_id=12</code> or whatever.</p>",
        "id": 1417142,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1659996708
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span> I'm actually trying to do all the work, that the current loop does per stream, in a single DB call for all data at once instead. The current way means, that postgres has to evaluate the first few messages again and again, each time we ask for the first message for a particular recipient since it can't remember that it had just looked at that data a moment ago. I think it makes much more sense to say \"give me all the first messages of all the streams\" and then postgres can just do a single table/index scan and keep track of what is the first message it sees for each stream.</p>\n<p>I've come up with a complete query now that mimics what is already recorded in my database (populated from the import).</p>\n<p>The current data would be: <code>select id, first_message_id from zerver_stream order by id;</code></p>\n<p>My query which calculates the same data in a way similar to what the current import code does: <code>SELECT r.type_id, min(m.id) from zerver_message m JOIN zerver_recipient r ON r.id = m.recipient_id WHERE r.type = 2 group by r.type_id, recipient_id order by r.type_id;</code> </p>\n<p>They both return the same message IDs for all my streams. All that is really needed now, is to put that query into the import code and replace the loop you quoted above with one that uses the result of this query and only performs the <code>stream.first_message_id = query_result.min_id; stream.save(update_fields=[\"first_message_id\"])</code> portion. You guys are probably much better than me at writing that code though.</p>",
        "id": 1417325,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1660032010
    },
    {
        "content": "<p>Yeah, I see, yeah 30 seconds for getting the bulk answer for all streams in a single query is pretty reasonable; I thought you were doing that on a per-stream basis.</p>",
        "id": 1417467,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1660065555
    },
    {
        "content": "<p>It seems totally possible that we can even just do it as a single raw SQL <code>UPDATE zerver_stream ...</code> query, without going to Django at all; though that likely has minimal practical advantage over just fetching the data and then using a Django <code>.bulk_update()</code> operation, since it's just one integer per stream.</p>",
        "id": 1417468,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1660065627
    },
    {
        "content": "<p>I'd be happy to merge such a single SQL query approach; probably don't have time to fiddle with that myself this week, but I expect sure someone else watching this stream does. (<a href=\"#narrow/stream/31-production-help/topic/Import.20Rocketchat.20data/near/1408867\">https://chat.zulip.org/#narrow/stream/31-production-help/topic/Import.20Rocketchat.20data/near/1408867</a> is the Python algorithm)</p>",
        "id": 1417469,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1660065784
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"23931\">@Florian Pritz</span> meanwhile, if you can respond to the feedback in your PR, I can try merging the commits that are finished.</p>",
        "id": 1417470,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1660065815
    },
    {
        "content": "<p>I guess to simplify your work, I've done the finishing work on <a href=\"https://github.com/zulip/zulip/pull/22486\">#22486</a>, which you've got a version of each of the commits (confirming those anomalies are not unique to your system), and tagged it to merge once CI finishes, so that should help you shrink your branch.</p>",
        "id": 1417475,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1660066424
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"7\">@Tim Abbott</span> I've adjusted the PR according to your feedback and rebased it on <code>main</code>. Thank you for that and sorry for taking this long.  I've also added type hints for a new function I added and expanded the commit message for some commits with more background info.</p>",
        "id": 1417922,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1660125208
    },
    {
        "content": "<p>Cool, I've added this to my review queue.</p>",
        "id": 1418134,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1660175331
    },
    {
        "content": "<p>If someone stumbles upon this in the future, the  SQL improvement for <code>first_message_id</code> now has a PR: <a href=\"https://github.com/zulip/zulip/pull/23096\">https://github.com/zulip/zulip/pull/23096</a></p>",
        "id": 1442612,
        "sender_full_name": "Florian Pritz",
        "timestamp": 1664452663
    },
    {
        "content": "<p>Thanks for doing this! Posted a comment with some questions, but I'm excited to integrate this.</p>",
        "id": 1442759,
        "sender_full_name": "Tim Abbott",
        "timestamp": 1664474234
    }
]